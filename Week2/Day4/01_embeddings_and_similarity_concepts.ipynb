{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Understanding Embeddings & Similarity Concepts\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand embeddings conceptually and practically\n",
    "- Explore different similarity/distance metrics and when to use them\n",
    "- Generate embeddings using OpenAI's API\n",
    "- Compare similarity approaches without getting lost in implementation details\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: What Are Embeddings? \n",
    "\n",
    "### The GPS Coordinates Analogy\n",
    "\n",
    "Imagine you're trying to find restaurants in a city. Instead of describing each restaurant with words like \"cozy\", \"expensive\", \"Italian\", you could place each restaurant on a map using GPS coordinates (latitude, longitude).\n",
    "\n",
    "**Embeddings work the same way for text:**\n",
    "- Instead of describing words with other words, we represent them as coordinates in \"meaning space\"\n",
    "- Similar meanings end up close together in this space\n",
    "- Different meanings are far apart\n",
    "\n",
    "Let's explore this concept with financial terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# OpenAI API Key Setup\n",
    "# Option 1: Set as environment variable (RECOMMENDED)\n",
    "# export OPENAI_API_KEY=\"your-api-key-here\"\n",
    "\n",
    "# Option 2: Load from .env file (for development)\n",
    "# Create a .env file in your project directory with: OPENAI_API_KEY=your-api-key-here\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"✅ Loaded environment variables from .env file\")\n",
    "except ImportError:\n",
    "    print(\"📋 Install python-dotenv if you want to use .env files: pip install python-dotenv\")\n",
    "\n",
    "# Option 3: Set directly in code (NOT RECOMMENDED for production)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Verify API key is available\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"❌ OPENAI_API_KEY not found!\")\n",
    "    print(\"Please set your OpenAI API key using one of these methods:\")\n",
    "    print(\"1. Environment variable: export OPENAI_API_KEY='your-key'\")\n",
    "    print(\"2. Create .env file with: OPENAI_API_KEY=your-key\")\n",
    "    print(\"3. Get your key from: https://platform.openai.com/api-keys\")\n",
    "else:\n",
    "    print(f\"✅ OpenAI API key found (ends with: ...{api_key[-4:]})\")\n",
    "\n",
    "# Set up OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "print(\"🚀 Setup complete! Ready to generate embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Financial Concepts in 2D Space\n",
    "\n",
    "Let's start with some financial terms and see how they cluster together when we represent them as embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample financial terms - organized by categories\n",
    "financial_terms = {\n",
    "    \"Profitability\": [\"profit\", \"revenue\", \"earnings\", \"income\", \"margin\"],\n",
    "    \"Losses\": [\"loss\", \"deficit\", \"debt\", \"bankruptcy\", \"default\"],\n",
    "    \"Investment\": [\"portfolio\", \"stocks\", \"bonds\", \"mutual funds\", \"dividend\"],\n",
    "    \"Analysis\": [\"valuation\", \"analysis\", \"forecast\", \"projection\", \"estimate\"]\n",
    "}\n",
    "\n",
    "# Flatten the terms for processing\n",
    "all_terms = []\n",
    "term_categories = []\n",
    "for category, terms in financial_terms.items():\n",
    "    all_terms.extend(terms)\n",
    "    term_categories.extend([category] * len(terms))\n",
    "\n",
    "print(f\"We'll analyze {len(all_terms)} financial terms across {len(financial_terms)} categories\")\n",
    "print(f\"Terms: {all_terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**💡 Key Insight Preview:**\n",
    "\n",
    "Before we generate embeddings, let's make a prediction:\n",
    "- Which terms do you think will be closest to each other?\n",
    "- Which terms will be furthest apart?\n",
    "- How might \"profit\" and \"loss\" relate to each other?\n",
    "\n",
    "Keep these predictions in mind as we explore the actual embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Creating Embeddings with APIs (25 minutes)\n",
    "\n",
    "Now let's generate actual embeddings using OpenAI's API and see how our predictions match reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-3-small\") -> List[float]:\n",
    "    \"\"\"\n",
    "    Get embedding for a single text using OpenAI API\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "        model: OpenAI embedding model to use\n",
    "    \n",
    "    Returns:\n",
    "        List of floats representing the embedding vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding for '{text}': {e}\")\n",
    "        return None\n",
    "\n",
    "def get_embeddings_batch(texts: List[str], model: str = \"text-embedding-3-small\") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Get embeddings for multiple texts efficiently\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to embed\n",
    "        model: OpenAI embedding model to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping text to embedding vector\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    \n",
    "    try:\n",
    "        # OpenAI allows batch processing up to 2048 inputs\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=texts\n",
    "        )\n",
    "        \n",
    "        for i, embedding_data in enumerate(response.data):\n",
    "            embeddings[texts[i]] = embedding_data.embedding\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch embedding: {e}\")\n",
    "        # Fallback to individual requests\n",
    "        for text in texts:\n",
    "            embedding = get_embedding(text, model)\n",
    "            if embedding:\n",
    "                embeddings[text] = embedding\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for our financial terms\n",
    "print(\"Generating embeddings for financial terms...\")\n",
    "term_embeddings = get_embeddings_batch(all_terms)\n",
    "\n",
    "print(f\"\\nSuccessfully generated embeddings for {len(term_embeddings)} terms\")\n",
    "\n",
    "# Let's examine the properties of these embeddings\n",
    "if term_embeddings:\n",
    "    sample_term = list(term_embeddings.keys())[0]\n",
    "    sample_embedding = term_embeddings[sample_term]\n",
    "    \n",
    "    print(f\"\\nEmbedding Properties:\")\n",
    "    print(f\"- Dimensions: {len(sample_embedding)}\")\n",
    "    print(f\"- Sample values: {sample_embedding[:5]}...\")\n",
    "    print(f\"- Value range: {min(sample_embedding):.4f} to {max(sample_embedding):.4f}\")\n",
    "    print(f\"- Vector magnitude: {np.linalg.norm(sample_embedding):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Embedding Properties\n",
    "\n",
    "**What do these numbers mean?**\n",
    "\n",
    "- **Dimensions (1536):** Each word becomes a point in 1536-dimensional space\n",
    "- **Values (-1 to 1):** Each dimension captures a different aspect of meaning\n",
    "- **Vector Magnitude:** The \"length\" of the vector (OpenAI embeddings are normalized)\n",
    "\n",
    "**Key Insight:** We can't interpret individual dimensions, but we can compare entire vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize these high-dimensional embeddings in 2D\n",
    "# We'll use PCA (Principal Component Analysis) to reduce dimensions\n",
    "\n",
    "def visualize_embeddings_2d(embeddings: Dict[str, List[float]], categories: List[str]):\n",
    "    \"\"\"\n",
    "    Visualize high-dimensional embeddings in 2D using PCA\n",
    "    \"\"\"\n",
    "    # Convert to numpy array\n",
    "    terms = list(embeddings.keys())\n",
    "    vectors = np.array([embeddings[term] for term in terms])\n",
    "    \n",
    "    # Reduce to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    vectors_2d = pca.fit_transform(vectors)\n",
    "    \n",
    "    # Create color map for categories\n",
    "    unique_categories = list(set(categories))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_categories)))\n",
    "    category_colors = {cat: colors[i] for i, cat in enumerate(unique_categories)}\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i, (term, category) in enumerate(zip(terms, categories)):\n",
    "        x, y = vectors_2d[i]\n",
    "        plt.scatter(x, y, c=[category_colors[category]], s=100, alpha=0.7)\n",
    "        plt.annotate(term, (x, y), xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=10, ha='left')\n",
    "    \n",
    "    # Add legend\n",
    "    for category, color in category_colors.items():\n",
    "        plt.scatter([], [], c=[color], s=100, label=category, alpha=0.7)\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title('Financial Terms in 2D Embedding Space', fontsize=14)\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print explained variance\n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Total variance captured in 2D: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Visualize our financial terms\n",
    "if term_embeddings:\n",
    "    visualize_embeddings_2d(term_embeddings, term_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Observation Questions\n",
    "\n",
    "Look at the visualization above and consider:\n",
    "\n",
    "1. **Clustering:** Do similar financial concepts cluster together?\n",
    "2. **Separation:** Are different categories well-separated?\n",
    "3. **Surprises:** Any unexpected relationships or groupings?\n",
    "4. **Relationships:** How do opposites like \"profit\" and \"loss\" relate?\n",
    "\n",
    "**Key Insight:** Even though we reduced 1536 dimensions to just 2, we can still see meaningful patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost & Performance Considerations\n",
    "\n",
    "Let's understand the practical aspects of using embedding APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate embedding costs and performance\n",
    "def analyze_embedding_costs(texts: List[str], model: str = \"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Analyze costs and performance of embedding generation\n",
    "    \"\"\"\n",
    "    # OpenAI pricing (as of 2024)\n",
    "    pricing = {\n",
    "        \"text-embedding-3-small\": 0.00002,  # per 1K tokens\n",
    "        \"text-embedding-3-large\": 0.00013,  # per 1K tokens\n",
    "    }\n",
    "    \n",
    "    # Estimate tokens (rough approximation: 1 token ≈ 4 characters)\n",
    "    total_chars = sum(len(text) for text in texts)\n",
    "    estimated_tokens = total_chars / 4\n",
    "    \n",
    "    cost_per_1k = pricing.get(model, 0.00002)\n",
    "    estimated_cost = (estimated_tokens / 1000) * cost_per_1k\n",
    "    \n",
    "    print(f\"Embedding Analysis for {len(texts)} texts:\")\n",
    "    print(f\"- Model: {model}\")\n",
    "    print(f\"- Total characters: {total_chars:,}\")\n",
    "    print(f\"- Estimated tokens: {estimated_tokens:,.0f}\")\n",
    "    print(f\"- Estimated cost: ${estimated_cost:.6f}\")\n",
    "    print(f\"- Cost per text: ${estimated_cost/len(texts):.6f}\")\n",
    "\n",
    "analyze_embedding_costs(all_terms)\n",
    "\n",
    "# Compare models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model Comparison:\")\n",
    "print(\"text-embedding-3-small: 1536 dimensions, faster, cheaper\")\n",
    "print(\"text-embedding-3-large: 3072 dimensions, more accurate, more expensive\")\n",
    "print(\"\\nFor most applications, 'small' model is sufficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Distance Metrics Deep Dive\n",
    "\n",
    "Now that we have embeddings, how do we measure how similar they are? There are several approaches, each with different strengths and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cosine Similarity (15 minutes)\n",
    "\n",
    "**Concept:** Measures the angle between two vectors (direction, not magnitude)\n",
    "\n",
    "**When to Use:** Text similarity, normalized data, when you care about meaning direction but not intensity\n",
    "\n",
    "**Range:** -1 (opposite) to 1 (identical)\n",
    "\n",
    "**Visual Intuition:** Think of two arrows pointing in space - cosine similarity measures how much they point in the same direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors\n",
    "    \n",
    "    Formula: cos(θ) = (A · B) / (||A|| * ||B||)\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    a = np.array(vec1)\n",
    "    b = np.array(vec2)\n",
    "    \n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(a, b)\n",
    "    \n",
    "    # Calculate magnitudes\n",
    "    magnitude_a = np.linalg.norm(a)\n",
    "    magnitude_b = np.linalg.norm(b)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if magnitude_a == 0 or magnitude_b == 0:\n",
    "        return 0\n",
    "    \n",
    "    return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "# Demonstrate with simple 2D vectors first\n",
    "print(\"Simple 2D Vector Examples:\")\n",
    "print(\"Vector A: [1, 0] (pointing right)\")\n",
    "print(\"Vector B: [0, 1] (pointing up)\")\n",
    "print(f\"Cosine similarity: {cosine_similarity([1, 0], [0, 1]):.3f}\")\n",
    "print(\"-> Perpendicular vectors have similarity of 0\")\n",
    "\n",
    "print(\"\\nVector A: [1, 1] (pointing northeast)\")\n",
    "print(\"Vector B: [2, 2] (pointing northeast, but longer)\")\n",
    "print(f\"Cosine similarity: {cosine_similarity([1, 1], [2, 2]):.3f}\")\n",
    "print(\"-> Same direction = perfect similarity, regardless of length\")\n",
    "\n",
    "print(\"\\nVector A: [1, 0] (pointing right)\")\n",
    "print(\"Vector B: [-1, 0] (pointing left)\")\n",
    "print(f\"Cosine similarity: {cosine_similarity([1, 0], [-1, 0]):.3f}\")\n",
    "print(\"-> Opposite directions = -1 similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's apply cosine similarity to our financial terms\n",
    "def find_most_similar_terms(target_term: str, embeddings: Dict[str, List[float]], top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Find the most similar terms to a target term using cosine similarity\n",
    "    \"\"\"\n",
    "    if target_term not in embeddings:\n",
    "        print(f\"Term '{target_term}' not found in embeddings\")\n",
    "        return\n",
    "    \n",
    "    target_embedding = embeddings[target_term]\n",
    "    similarities = []\n",
    "    \n",
    "    for term, embedding in embeddings.items():\n",
    "        if term != target_term:  # Skip the target term itself\n",
    "            similarity = cosine_similarity(target_embedding, embedding)\n",
    "            similarities.append((term, similarity))\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Most similar terms to '{target_term}':\")\n",
    "    for i, (term, similarity) in enumerate(similarities[:top_k], 1):\n",
    "        print(f\"{i}. {term}: {similarity:.3f}\")\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test with different financial terms\n",
    "if term_embeddings:\n",
    "    print(\"=\" * 50)\n",
    "    find_most_similar_terms(\"profit\", term_embeddings)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    find_most_similar_terms(\"loss\", term_embeddings)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    find_most_similar_terms(\"stocks\", term_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🤔 Analysis Questions:**\n",
    "1. Are the most similar terms what you expected?\n",
    "2. How similar are \"profit\" and \"loss\"? (They're related concepts but opposite meanings)\n",
    "3. What does this tell us about how embeddings capture meaning?\n",
    "\n",
    "**Financial Use Case:** Cosine similarity is perfect for comparing document themes regardless of document length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Euclidean Distance\n",
    "\n",
    "**Concept:** Straight-line distance in vector space (like measuring with a ruler)\n",
    "\n",
    "**When to Use:** When magnitude matters, clustering, when you want absolute differences\n",
    "\n",
    "**Range:** 0 (identical) to ∞ (very different)\n",
    "\n",
    "**Key Difference:** Unlike cosine similarity, Euclidean distance considers both direction AND magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between two vectors\n",
    "    \n",
    "    Formula: sqrt(Σ(ai - bi)²)\n",
    "    \"\"\"\n",
    "    a = np.array(vec1)\n",
    "    b = np.array(vec2)\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "# Demonstrate the difference with simple examples\n",
    "print(\"Comparing Cosine Similarity vs Euclidean Distance:\")\n",
    "print(\"\\nVector A: [1, 1]\")\n",
    "print(\"Vector B: [2, 2] (same direction, double length)\")\n",
    "print(f\"Cosine similarity: {cosine_similarity([1, 1], [2, 2]):.3f} (perfect similarity)\")\n",
    "print(f\"Euclidean distance: {euclidean_distance([1, 1], [2, 2]):.3f} (not zero - considers magnitude)\")\n",
    "\n",
    "print(\"\\nVector A: [1, 0]\")\n",
    "print(\"Vector B: [0, 1] (perpendicular)\")\n",
    "print(f\"Cosine similarity: {cosine_similarity([1, 0], [0, 1]):.3f} (perpendicular)\")\n",
    "print(f\"Euclidean distance: {euclidean_distance([1, 0], [0, 1]):.3f} (distance in space)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two approaches on our financial terms\n",
    "def compare_similarity_methods(term1: str, term2: str, embeddings: Dict[str, List[float]]):\n",
    "    \"\"\"\n",
    "    Compare different similarity methods for two terms\n",
    "    \"\"\"\n",
    "    if term1 not in embeddings or term2 not in embeddings:\n",
    "        print(f\"One or both terms not found in embeddings\")\n",
    "        return\n",
    "    \n",
    "    emb1 = embeddings[term1]\n",
    "    emb2 = embeddings[term2]\n",
    "    \n",
    "    cosine_sim = cosine_similarity(emb1, emb2)\n",
    "    euclidean_dist = euclidean_distance(emb1, emb2)\n",
    "    \n",
    "    print(f\"Comparing '{term1}' and '{term2}':\")\n",
    "    print(f\"  Cosine similarity: {cosine_sim:.4f} (higher = more similar)\")\n",
    "    print(f\"  Euclidean distance: {euclidean_dist:.4f} (lower = more similar)\")\n",
    "    return cosine_sim, euclidean_dist\n",
    "\n",
    "# Test with interesting pairs\n",
    "if term_embeddings:\n",
    "    pairs_to_test = [\n",
    "        (\"profit\", \"revenue\"),\n",
    "        (\"profit\", \"loss\"),\n",
    "        (\"stocks\", \"bonds\"),\n",
    "        (\"analysis\", \"forecast\"),\n",
    "        (\"debt\", \"dividend\")\n",
    "    ]\n",
    "    \n",
    "    for term1, term2 in pairs_to_test:\n",
    "        if term1 in term_embeddings and term2 in term_embeddings:\n",
    "            compare_similarity_methods(term1, term2, term_embeddings)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Manhattan Distance\n",
    "\n",
    "**Concept:** Sum of absolute differences along each dimension (like walking city blocks)\n",
    "\n",
    "**When to Use:** High-dimensional sparse data, when you want to avoid the \"curse of dimensionality\"\n",
    "\n",
    "**Visual Analogy:** In Manhattan, you can't walk diagonally through buildings - you have to go block by block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Manhattan distance between two vectors\n",
    "    \n",
    "    Formula: Σ|ai - bi|\n",
    "    \"\"\"\n",
    "    a = np.array(vec1)\n",
    "    b = np.array(vec2)\n",
    "    return np.sum(np.abs(a - b))\n",
    "\n",
    "# Quick comparison\n",
    "print(\"Distance Comparison for [1, 1] and [3, 3]:\")\n",
    "print(f\"Euclidean distance: {euclidean_distance([1, 1], [3, 3]):.3f} (straight line)\")\n",
    "print(f\"Manhattan distance: {manhattan_distance([1, 1], [3, 3]):.3f} (city blocks)\")\n",
    "\n",
    "# Manhattan distance is often used in high-dimensional spaces\n",
    "# because it's less affected by the \"curse of dimensionality\"\n",
    "print(\"\\nManhattan distance is useful when:\")\n",
    "print(\"- Working with high-dimensional sparse data\")\n",
    "print(\"- You want to avoid distance concentration effects\")\n",
    "print(\"- Computational efficiency is important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dot Product Similarity\n",
    "\n",
    "**Concept:** Direct vector multiplication (considers both direction and magnitude)\n",
    "\n",
    "**When to Use:** When both direction and magnitude matter, when vectors are normalized\n",
    "\n",
    "**Key Insight:** For normalized vectors (like OpenAI embeddings), dot product equals cosine similarity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate dot product similarity between two vectors\n",
    "    \n",
    "    Formula: Σ(ai * bi)\n",
    "    \"\"\"\n",
    "    a = np.array(vec1)\n",
    "    b = np.array(vec2)\n",
    "    return np.dot(a, b)\n",
    "\n",
    "# Compare normalized vs unnormalized vectors\n",
    "print(\"Normalized vs Unnormalized Vectors:\")\n",
    "\n",
    "# Unnormalized vectors\n",
    "vec_a = [3, 4]  # magnitude = 5\n",
    "vec_b = [6, 8]  # magnitude = 10, same direction as A\n",
    "\n",
    "print(f\"\\nUnnormalized vectors: {vec_a} and {vec_b}\")\n",
    "print(f\"Cosine similarity: {cosine_similarity(vec_a, vec_b):.3f}\")\n",
    "print(f\"Dot product: {dot_product_similarity(vec_a, vec_b):.3f}\")\n",
    "print(\"-> Different values because magnitudes differ\")\n",
    "\n",
    "# Normalized versions\n",
    "vec_a_norm = np.array(vec_a) / np.linalg.norm(vec_a)\n",
    "vec_b_norm = np.array(vec_b) / np.linalg.norm(vec_b)\n",
    "\n",
    "print(f\"\\nNormalized vectors: {vec_a_norm} and {vec_b_norm}\")\n",
    "print(f\"Cosine similarity: {cosine_similarity(vec_a_norm, vec_b_norm):.3f}\")\n",
    "print(f\"Dot product: {dot_product_similarity(vec_a_norm, vec_b_norm):.3f}\")\n",
    "print(\"-> Same values because vectors are normalized\")\n",
    "\n",
    "print(\"\\n🔑 Key Insight: OpenAI embeddings are already normalized!\")\n",
    "print(\"So for OpenAI embeddings: dot product = cosine similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify this with our financial term embeddings\n",
    "if term_embeddings and len(term_embeddings) >= 2:\n",
    "    terms = list(term_embeddings.keys())[:2]\n",
    "    emb1 = term_embeddings[terms[0]]\n",
    "    emb2 = term_embeddings[terms[1]]\n",
    "    \n",
    "    cosine_sim = cosine_similarity(emb1, emb2)\n",
    "    dot_product = dot_product_similarity(emb1, emb2)\n",
    "    \n",
    "    print(f\"Verification with OpenAI embeddings for '{terms[0]}' and '{terms[1]}':\")\n",
    "    print(f\"Cosine similarity: {cosine_sim:.6f}\")\n",
    "    print(f\"Dot product: {dot_product:.6f}\")\n",
    "    print(f\"Difference: {abs(cosine_sim - dot_product):.8f}\")\n",
    "    print(\"\\n✅ Confirmed: They're essentially the same for normalized embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Comparison Exercise\n",
    "\n",
    "Let's use the same financial text pairs with all 4 metrics and see when each gives different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_similarity_analysis(embeddings: Dict[str, List[float]]):\n",
    "    \"\"\"\n",
    "    Compare all similarity metrics across financial term pairs\n",
    "    \"\"\"\n",
    "    # Define interesting pairs to analyze\n",
    "    pairs = [\n",
    "        (\"profit\", \"revenue\"),    # Should be very similar\n",
    "        (\"profit\", \"loss\"),       # Related but opposite\n",
    "        (\"stocks\", \"bonds\"),      # Related investment types\n",
    "        (\"debt\", \"dividend\"),     # Unrelated financial terms\n",
    "        (\"analysis\", \"valuation\") # Similar analytical concepts\n",
    "    ]\n",
    "    \n",
    "    print(\"Comprehensive Similarity Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Term Pair':<20} {'Cosine':<10} {'Dot Prod':<10} {'Euclidean':<12} {'Manhattan':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for term1, term2 in pairs:\n",
    "        if term1 in embeddings and term2 in embeddings:\n",
    "            emb1 = embeddings[term1]\n",
    "            emb2 = embeddings[term2]\n",
    "            \n",
    "            cosine_sim = cosine_similarity(emb1, emb2)\n",
    "            dot_prod = dot_product_similarity(emb1, emb2)\n",
    "            euclidean_dist = euclidean_distance(emb1, emb2)\n",
    "            manhattan_dist = manhattan_distance(emb1, emb2)\n",
    "            \n",
    "            pair_name = f\"{term1}-{term2}\"\n",
    "            print(f\"{pair_name:<20} {cosine_sim:<10.4f} {dot_prod:<10.4f} {euclidean_dist:<12.4f} {manhattan_dist:<12.2f}\")\n",
    "    \n",
    "    print(\"\\n📊 Interpretation Guide:\")\n",
    "    print(\"• Cosine/Dot Product: -1 to 1 (higher = more similar)\")\n",
    "    print(\"• Euclidean/Manhattan: 0 to ∞ (lower = more similar)\")\n",
    "    print(\"• For normalized embeddings: Cosine ≈ Dot Product\")\n",
    "\n",
    "if term_embeddings:\n",
    "    comprehensive_similarity_analysis(term_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: When to Use Each Similarity Metric\n",
    "\n",
    "| Metric | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Cosine Similarity** | Text similarity, semantic search | Ignores magnitude, normalized scale | May miss intensity differences |\n",
    "| **Dot Product** | Normalized vectors, fast computation | Simple, efficient | Same as cosine for normalized vectors |\n",
    "| **Euclidean Distance** | Clustering, when magnitude matters | Intuitive, considers all differences | Affected by dimensionality |\n",
    "| **Manhattan Distance** | High-dimensional sparse data | Robust to outliers, efficient | Less intuitive geometrically |\n",
    "\n",
    "### 🎯 For Financial Document Analysis:\n",
    "- **Cosine Similarity**: Perfect for comparing document themes/topics\n",
    "- **Euclidean Distance**: Good for clustering similar documents\n",
    "- **Manhattan Distance**: Useful for large document collections\n",
    "\n",
    "### 🔑 Key Takeaway:\n",
    "**Cosine similarity is the gold standard for text embeddings** because it focuses on meaning direction rather than intensity, making it perfect for semantic search and document comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps: Preparing for Notebook 2\n",
    "\n",
    "In this notebook, we've covered:\n",
    "✅ **Conceptual understanding** of embeddings as \"meaning coordinates\"\n",
    "✅ **Practical experience** generating embeddings with OpenAI's API\n",
    "✅ **Deep dive** into similarity metrics and when to use each\n",
    "✅ **Cost and performance** considerations for real applications\n",
    "\n",
    "**In Notebook 2, we'll apply these concepts to:**\n",
    "- Process and chunk real financial documents\n",
    "- Build efficient similarity search systems\n",
    "- Create a complete document retrieval pipeline\n",
    "- Handle large document collections with optimization strategies\n",
    "\n",
    "**Save your embeddings for reuse:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings for use in Notebook 2\n",
    "if term_embeddings:\n",
    "    with open('financial_term_embeddings.json', 'w') as f:\n",
    "        json.dump(term_embeddings, f)\n",
    "    print(\"✅ Saved embeddings to 'financial_term_embeddings.json'\")\n",
    "    print(\"📋 Ready for Notebook 2: Document Processing & Search Systems\")\n",
    "else:\n",
    "    print(\"⚠️ No embeddings to save. Make sure to run the embedding generation cells above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
