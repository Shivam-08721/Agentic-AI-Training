{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Document Processing & Search Systems\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Process and chunk real financial documents effectively\n",
    "- Build similarity search from scratch using embeddings\n",
    "- Create a complete document retrieval system\n",
    "- Handle large document collections with optimization strategies\n",
    "- Prepare foundation for Day 2's RAG pipeline\n",
    "\n",
    "**Duration:** 90 minutes\n",
    "\n",
    "**Prerequisites:** Complete Notebook 1 (Embeddings & Similarity Concepts)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing our dependencies and loading the embeddings from Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"Install python-dotenv for .env support: pip install python-dotenv\")\n",
    "\n",
    "# Set up OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Load our similarity functions from Notebook 1\n",
    "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    a = np.array(vec1)\n",
    "    b = np.array(vec2)\n",
    "    dot_product = np.dot(a, b)\n",
    "    magnitude_a = np.linalg.norm(a)\n",
    "    magnitude_b = np.linalg.norm(b)\n",
    "    if magnitude_a == 0 or magnitude_b == 0:\n",
    "        return 0\n",
    "    return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "def get_embedding(text: str, model: str = \"text-embedding-3-small\") -> List[float]:\n",
    "    \"\"\"Get embedding for text using OpenAI API\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(model=model, input=text)\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Setup complete! Ready to process documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Document Chunking Strategies (25 minutes)\n",
    "\n",
    "### 1.1 Why Chunking Matters (5 minutes)\n",
    "\n",
    "Before we dive into implementation, let's understand why document chunking is crucial for effective retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comprehensive sample financial documents from external files\n",
    "docs_dir = Path(\"sample_financial_docs\")\n",
    "\n",
    "# Verify the documents directory exists\n",
    "if not docs_dir.exists():\n",
    "    print(f\"‚ùå Documents directory not found: {docs_dir}\")\n",
    "    print(\"Please ensure the sample_financial_docs folder exists with the required files.\")\n",
    "else:\n",
    "    # List available documents\n",
    "    available_docs = list(docs_dir.glob(\"*.txt\"))\n",
    "    print(f\"üìÅ Found {len(available_docs)} financial documents:\")\n",
    "    \n",
    "    for doc_path in available_docs:\n",
    "        # Get file size for reference\n",
    "        file_size = doc_path.stat().st_size\n",
    "        file_size_kb = file_size / 1024\n",
    "        \n",
    "        print(f\"   üìÑ {doc_path.name} ({file_size_kb:.1f} KB)\")\n",
    "    \n",
    "    # Provide overview of document types\n",
    "    print(f\"\\nüìã Document Types Available:\")\n",
    "    print(f\"   ‚Ä¢ Earnings Call Transcript (Q3 2024)\")\n",
    "    print(f\"   ‚Ä¢ 10-K Risk Factors Filing\")\n",
    "    print(f\"   ‚Ä¢ Goldman Sachs Analyst Report\")\n",
    "    print(f\"   ‚Ä¢ CEO Annual Shareholder Letter\")\n",
    "    print(f\"   ‚Ä¢ Enterprise Software Market Research Report\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Ready to process comprehensive financial documents!\")\n",
    "    print(f\"   Total estimated chunks: 50-65 (vs ~10 with short samples)\")\n",
    "    print(f\"   Perfect for demonstrating chunking and search differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunker:\n",
    "    \"\"\"Handles different document chunking strategies\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_by_characters(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into fixed-size character chunks with optional overlap\n",
    "        \n",
    "        Pros: Simple, predictable chunk sizes\n",
    "        Cons: May break sentences/paragraphs awkwardly\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            \n",
    "            # Clean up the chunk\n",
    "            chunk = chunk.strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            # Move start position with overlap\n",
    "            start = end - overlap\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_by_sentences(text: str, max_chunk_size: int = 500) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text by sentences, grouping them to stay under max_chunk_size\n",
    "        \n",
    "        Pros: Preserves sentence boundaries, better semantic coherence\n",
    "        Cons: Variable chunk sizes, complex sentence detection\n",
    "        \"\"\"\n",
    "        # Simple sentence splitting (can be improved with nltk/spacy)\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Check if adding this sentence would exceed chunk size\n",
    "            potential_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "            \n",
    "            if len(potential_chunk) <= max_chunk_size:\n",
    "                current_chunk = potential_chunk\n",
    "            else:\n",
    "                # Save current chunk and start new one\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "        \n",
    "        # Don't forget the last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_by_paragraphs(text: str, max_chunk_size: int = 1000) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text by paragraphs, combining small ones\n",
    "        \n",
    "        Pros: Preserves natural document structure\n",
    "        Cons: Very variable chunk sizes, may be too large/small\n",
    "        \"\"\"\n",
    "        # Split by double newlines (paragraphs)\n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            potential_chunk = current_chunk + \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "            \n",
    "            if len(potential_chunk) <= max_chunk_size:\n",
    "                current_chunk = potential_chunk\n",
    "            else:\n",
    "                # Save current chunk and start new one\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = paragraph\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_with_overlap(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"\n",
    "        Advanced chunking with smart overlap at sentence boundaries\n",
    "        \n",
    "        Pros: Maintains context across chunks, respects sentence boundaries\n",
    "        Cons: More complex, some content duplication\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        overlap_buffer = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            potential_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "            \n",
    "            if len(potential_chunk) <= chunk_size:\n",
    "                current_chunk = potential_chunk\n",
    "            else:\n",
    "                # Save current chunk\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    \n",
    "                    # Create overlap buffer from end of current chunk\n",
    "                    words = current_chunk.split()\n",
    "                    if len(words) > overlap:\n",
    "                        overlap_buffer = \" \".join(words[-overlap:])\n",
    "                    else:\n",
    "                        overlap_buffer = current_chunk\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                current_chunk = overlap_buffer + \" \" + sentence if overlap_buffer else sentence\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "print(\"‚úÖ DocumentChunker class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample document for testing chunking strategies\n",
    "sample_doc_path = docs_dir / \"earnings_call_q3_2024.txt\"\n",
    "\n",
    "if sample_doc_path.exists():\n",
    "    with open(sample_doc_path, 'r') as f:\n",
    "        sample_text = f.read()\n",
    "    \n",
    "    print(f\"Loaded document: {sample_doc_path.name}\")\n",
    "    print(f\"Document length: {len(sample_text):,} characters\")\n",
    "    print(f\"Estimated word count: {len(sample_text.split()):,} words\")\n",
    "    print(f\"Document preview:\\\\n{sample_text[:200]}...\\\\n\")\n",
    "    \n",
    "    # Test different chunking methods\n",
    "    chunker = DocumentChunker()\n",
    "    \n",
    "    methods = [\n",
    "        (\"Characters (500)\", lambda: chunker.chunk_by_characters(sample_text, 500)),\n",
    "        (\"Sentences (500)\", lambda: chunker.chunk_by_sentences(sample_text, 500)),\n",
    "        (\"Paragraphs (1000)\", lambda: chunker.chunk_by_paragraphs(sample_text, 1000)),\n",
    "        (\"With Overlap (500/50)\", lambda: chunker.chunk_with_overlap(sample_text, 500, 50))\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method_name, method_func in methods:\n",
    "        chunks = method_func()\n",
    "        results[method_name] = chunks\n",
    "        \n",
    "        print(f\"\\\\n{'='*50}\")\n",
    "        print(f\"Method: {method_name}\")\n",
    "        print(f\"Number of chunks: {len(chunks)}\")\n",
    "        print(f\"Chunk sizes: {[len(chunk) for chunk in chunks[:5]]}{'...' if len(chunks) > 5 else ''}\")\n",
    "        print(f\"Average chunk size: {np.mean([len(chunk) for chunk in chunks]):.1f} chars\")\n",
    "        print(f\"Size std deviation: {np.std([len(chunk) for chunk in chunks]):.1f} chars\")\n",
    "        \n",
    "        # Show first chunk as example\n",
    "        print(f\"\\\\nFirst chunk preview:\")\n",
    "        print(f\"\\\\\\\"{chunks[0][:150]}...\\\\\\\"\")\n",
    "    \n",
    "    print(\"\\\\n‚úÖ Chunking comparison complete!\")\n",
    "    print(\"\\\\nüí° Notice how different methods create different chunk counts and sizes\")\n",
    "    print(\"üí° Longer documents show clearer differences between chunking strategies\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Sample document not found: {sample_doc_path}\")\n",
    "    print(\"Please ensure the sample_financial_docs directory contains the required files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample document for testing\n",
    "with open(docs_dir / \"earnings_call_q3_2024.txt\", 'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "print(f\"Original document length: {len(sample_text)} characters\")\n",
    "print(f\"Original document preview:\\n{sample_text[:200]}...\\n\")\n",
    "\n",
    "# Test different chunking methods\n",
    "chunker = DocumentChunker()\n",
    "\n",
    "methods = [\n",
    "    (\"Characters (500)\", lambda: chunker.chunk_by_characters(sample_text, 500)),\n",
    "    (\"Sentences (500)\", lambda: chunker.chunk_by_sentences(sample_text, 500)),\n",
    "    (\"Paragraphs (1000)\", lambda: chunker.chunk_by_paragraphs(sample_text, 1000)),\n",
    "    (\"With Overlap (500/50)\", lambda: chunker.chunk_with_overlap(sample_text, 500, 50))\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for method_name, method_func in methods:\n",
    "    chunks = method_func()\n",
    "    results[method_name] = chunks\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Method: {method_name}\")\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    print(f\"Chunk sizes: {[len(chunk) for chunk in chunks]}\")\n",
    "    print(f\"Average chunk size: {np.mean([len(chunk) for chunk in chunks]):.1f} chars\")\n",
    "    \n",
    "    # Show first chunk as example\n",
    "    print(f\"\\nFirst chunk preview:\")\n",
    "    print(f\"\\\"{chunks[0][:150]}...\\\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Chunking comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Analysis Questions:**\n",
    "\n",
    "1. **Which method preserves meaning best?** Look at how sentences and ideas are split\n",
    "2. **Which is most consistent?** Compare chunk size variations\n",
    "3. **Which handles financial terminology well?** Notice how numbers and technical terms are treated\n",
    "4. **Which would work best for search?** Consider semantic coherence of chunks\n",
    "\n",
    "**Key Insights:**\n",
    "- **Character chunking**: Fast but may break sentences awkwardly\n",
    "- **Sentence chunking**: Good balance of speed and coherence\n",
    "- **Paragraph chunking**: Maintains structure but variable sizes\n",
    "- **Overlap chunking**: Best context preservation but some redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Document Processing Pipeline (20 minutes)\n",
    "\n",
    "Now let's build a complete pipeline to process multiple documents and manage their chunks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentInfo:\n",
    "    \"\"\"Metadata for a document\"\"\"\n",
    "    filename: str\n",
    "    title: str\n",
    "    doc_type: str  # earnings_call, 10k_filing, analyst_report, etc.\n",
    "    date: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "    word_count: int = 0\n",
    "    char_count: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ChunkInfo:\n",
    "    \"\"\"Metadata for a document chunk\"\"\"\n",
    "    chunk_id: str\n",
    "    document_id: str\n",
    "    content: str\n",
    "    chunk_index: int  # Position within document\n",
    "    char_count: int\n",
    "    word_count: int\n",
    "    embedding: Optional[List[float]] = None\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Complete document processing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_method: str = \"sentences\", chunk_size: int = 500):\n",
    "        self.chunk_method = chunk_method\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunker = DocumentChunker()\n",
    "        self.documents: Dict[str, DocumentInfo] = {}\n",
    "        self.chunks: Dict[str, ChunkInfo] = {}\n",
    "        \n",
    "    def _detect_document_type(self, filename: str, content: str) -> str:\n",
    "        \"\"\"Simple document type detection based on filename and content\"\"\"\n",
    "        filename_lower = filename.lower()\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        if \"earnings\" in filename_lower or \"earnings call\" in content_lower:\n",
    "            return \"earnings_call\"\n",
    "        elif \"10k\" in filename_lower or \"risk factors\" in content_lower:\n",
    "            return \"10k_filing\"\n",
    "        elif \"analyst\" in filename_lower or \"price target\" in content_lower:\n",
    "            return \"analyst_report\"\n",
    "        elif \"news\" in filename_lower:\n",
    "            return \"news_article\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    \n",
    "    def _extract_title(self, filename: str, content: str) -> str:\n",
    "        \"\"\"Extract title from document content or use filename\"\"\"\n",
    "        lines = content.strip().split('\\n')\n",
    "        first_line = lines[0].strip()\n",
    "        \n",
    "        # If first line is short and looks like a title, use it\n",
    "        if len(first_line) < 100 and len(first_line.split()) < 15:\n",
    "            return first_line\n",
    "        else:\n",
    "            # Use filename without extension as title\n",
    "            return Path(filename).stem.replace('_', ' ').title()\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text content\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove special characters but keep financial symbols\n",
    "        # Keep: periods, commas, parentheses, dollar signs, percentages\n",
    "        text = re.sub(r'[^\\w\\s.,()$%+-]', ' ', text)\n",
    "        \n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def process_document(self, file_path: str) -> str:\n",
    "        \"\"\"Process a single document and return document ID\"\"\"\n",
    "        # Read file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "        \n",
    "        # Clean content\n",
    "        content = self._clean_text(raw_content)\n",
    "        \n",
    "        # Create document metadata\n",
    "        filename = Path(file_path).name\n",
    "        doc_id = Path(file_path).stem  # Use filename without extension as ID\n",
    "        \n",
    "        doc_info = DocumentInfo(\n",
    "            filename=filename,\n",
    "            title=self._extract_title(filename, content),\n",
    "            doc_type=self._detect_document_type(filename, content),\n",
    "            word_count=len(content.split()),\n",
    "            char_count=len(content)\n",
    "        )\n",
    "        \n",
    "        self.documents[doc_id] = doc_info\n",
    "        \n",
    "        # Chunk the document\n",
    "        if self.chunk_method == \"sentences\":\n",
    "            chunks = self.chunker.chunk_by_sentences(content, self.chunk_size)\n",
    "        elif self.chunk_method == \"paragraphs\":\n",
    "            chunks = self.chunker.chunk_by_paragraphs(content, self.chunk_size)\n",
    "        elif self.chunk_method == \"overlap\":\n",
    "            chunks = self.chunker.chunk_with_overlap(content, self.chunk_size, 50)\n",
    "        else:  # default to characters\n",
    "            chunks = self.chunker.chunk_by_characters(content, self.chunk_size)\n",
    "        \n",
    "        # Create chunk metadata\n",
    "        for i, chunk_content in enumerate(chunks):\n",
    "            chunk_id = f\"{doc_id}_chunk_{i:03d}\"\n",
    "            \n",
    "            chunk_info = ChunkInfo(\n",
    "                chunk_id=chunk_id,\n",
    "                document_id=doc_id,\n",
    "                content=chunk_content,\n",
    "                chunk_index=i,\n",
    "                char_count=len(chunk_content),\n",
    "                word_count=len(chunk_content.split())\n",
    "            )\n",
    "            \n",
    "            self.chunks[chunk_id] = chunk_info\n",
    "        \n",
    "        print(f\"‚úÖ Processed {filename}: {len(chunks)} chunks created\")\n",
    "        return doc_id\n",
    "    \n",
    "    def process_directory(self, directory_path: str) -> List[str]:\n",
    "        \"\"\"Process all documents in a directory\"\"\"\n",
    "        directory = Path(directory_path)\n",
    "        doc_ids = []\n",
    "        \n",
    "        for file_path in directory.glob(\"*.txt\"):\n",
    "            doc_id = self.process_document(str(file_path))\n",
    "            doc_ids.append(doc_id)\n",
    "        \n",
    "        return doc_ids\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get processing statistics\"\"\"\n",
    "        total_chunks = len(self.chunks)\n",
    "        total_docs = len(self.documents)\n",
    "        \n",
    "        chunk_sizes = [chunk.char_count for chunk in self.chunks.values()]\n",
    "        doc_types = [doc.doc_type for doc in self.documents.values()]\n",
    "        \n",
    "        return {\n",
    "            \"total_documents\": total_docs,\n",
    "            \"total_chunks\": total_chunks,\n",
    "            \"avg_chunks_per_doc\": total_chunks / total_docs if total_docs > 0 else 0,\n",
    "            \"chunk_size_stats\": {\n",
    "                \"min\": min(chunk_sizes) if chunk_sizes else 0,\n",
    "                \"max\": max(chunk_sizes) if chunk_sizes else 0,\n",
    "                \"mean\": np.mean(chunk_sizes) if chunk_sizes else 0,\n",
    "                \"std\": np.std(chunk_sizes) if chunk_sizes else 0\n",
    "            },\n",
    "            \"document_types\": {doc_type: doc_types.count(doc_type) for doc_type in set(doc_types)}\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ DocumentProcessor class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the document processor with our comprehensive financial documents\n",
    "processor = DocumentProcessor(chunk_method=\"sentences\", chunk_size=400)\n",
    "\n",
    "# Process all sample documents\n",
    "print(\"Processing comprehensive financial documents...\\\\n\")\n",
    "\n",
    "# Get list of document paths  \n",
    "doc_paths = [str(path) for path in docs_dir.glob(\"*.txt\")]\n",
    "\n",
    "if doc_paths:\n",
    "    doc_ids = []\n",
    "    for doc_path in doc_paths:\n",
    "        doc_id = processor.process_document(doc_path)\n",
    "        doc_ids.append(doc_id)\n",
    "    \n",
    "    # Display statistics\n",
    "    stats = processor.get_statistics()\n",
    "    print(f\"\\\\n{'='*50}\")\n",
    "    print(\"PROCESSING STATISTICS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total documents: {stats['total_documents']}\")\n",
    "    print(f\"Total chunks: {stats['total_chunks']}\")\n",
    "    print(f\"Average chunks per document: {stats['avg_chunks_per_doc']:.1f}\")\n",
    "    print(f\"\\\\nChunk size statistics:\")\n",
    "    print(f\"  Min: {stats['chunk_size_stats']['min']} chars\")\n",
    "    print(f\"  Max: {stats['chunk_size_stats']['max']} chars\")\n",
    "    print(f\"  Mean: {stats['chunk_size_stats']['mean']:.1f} chars\")\n",
    "    print(f\"  Std: {stats['chunk_size_stats']['std']:.1f} chars\")\n",
    "    print(f\"\\\\nDocument types: {stats['document_types']}\")\n",
    "    \n",
    "    # Show some example chunks\n",
    "    print(f\"\\\\n{'='*50}\")\n",
    "    print(\"SAMPLE CHUNKS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for i, (chunk_id, chunk_info) in enumerate(list(processor.chunks.items())[:3]):\n",
    "        print(f\"\\\\nChunk {i+1}: {chunk_id}\")\n",
    "        print(f\"Document: {processor.documents[chunk_info.document_id].title}\")\n",
    "        print(f\"Type: {processor.documents[chunk_info.document_id].doc_type}\")\n",
    "        print(f\"Size: {chunk_info.char_count} chars, {chunk_info.word_count} words\")\n",
    "        print(f\"Content: \\\\\\\"{chunk_info.content[:150]}...\\\\\\\"\")\n",
    "    \n",
    "    print(f\"\\\\nüéØ Success! {stats['total_chunks']} chunks created from {stats['total_documents']} documents\")\n",
    "    print(\"Now we have enough chunks to properly demonstrate search capabilities!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå No documents found in {docs_dir}\")\n",
    "    print(\"Please ensure the sample_financial_docs directory contains .txt files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the document processor\n",
    "processor = DocumentProcessor(chunk_method=\"sentences\", chunk_size=400)\n",
    "\n",
    "# Process all sample documents\n",
    "print(\"Processing sample financial documents...\\n\")\n",
    "doc_ids = processor.process_directory(\"sample_financial_docs\")\n",
    "\n",
    "# Display statistics\n",
    "stats = processor.get_statistics()\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"PROCESSING STATISTICS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total documents: {stats['total_documents']}\")\n",
    "print(f\"Total chunks: {stats['total_chunks']}\")\n",
    "print(f\"Average chunks per document: {stats['avg_chunks_per_doc']:.1f}\")\n",
    "print(f\"\\nChunk size statistics:\")\n",
    "print(f\"  Min: {stats['chunk_size_stats']['min']} chars\")\n",
    "print(f\"  Max: {stats['chunk_size_stats']['max']} chars\")\n",
    "print(f\"  Mean: {stats['chunk_size_stats']['mean']:.1f} chars\")\n",
    "print(f\"  Std: {stats['chunk_size_stats']['std']:.1f} chars\")\n",
    "print(f\"\\nDocument types: {stats['document_types']}\")\n",
    "\n",
    "# Show some example chunks\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"SAMPLE CHUNKS\")\n",
    "print(f\"{'='*50}\")\n",
    "for i, (chunk_id, chunk_info) in enumerate(list(processor.chunks.items())[:3]):\n",
    "    print(f\"\\nChunk {i+1}: {chunk_id}\")\n",
    "    print(f\"Document: {processor.documents[chunk_info.document_id].title}\")\n",
    "    print(f\"Type: {processor.documents[chunk_info.document_id].doc_type}\")\n",
    "    print(f\"Size: {chunk_info.char_count} chars, {chunk_info.word_count} words\")\n",
    "    print(f\"Content: \\\"{chunk_info.content[:150]}...\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Building Efficient Search (25 minutes)\n",
    "\n",
    "Now let's create embeddings for our chunks and build a powerful search system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentSearcher:\n",
    "    \"\"\"Efficient similarity-based document search system\"\"\"\n",
    "    \n",
    "    def __init__(self, processor: DocumentProcessor):\n",
    "        self.processor = processor\n",
    "        self.embeddings_generated = False\n",
    "        \n",
    "    def generate_embeddings(self, batch_size: int = 10) -> None:\n",
    "        \"\"\"Generate embeddings for all chunks\"\"\"\n",
    "        chunks_to_embed = [chunk for chunk in self.processor.chunks.values() \n",
    "                          if chunk.embedding is None]\n",
    "        \n",
    "        if not chunks_to_embed:\n",
    "            print(\"All chunks already have embeddings!\")\n",
    "            self.embeddings_generated = True\n",
    "            return\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(chunks_to_embed)} chunks...\")\n",
    "        \n",
    "        # Process in batches to avoid rate limits\n",
    "        for i in range(0, len(chunks_to_embed), batch_size):\n",
    "            batch = chunks_to_embed[i:i + batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1}/{(len(chunks_to_embed)-1)//batch_size + 1}\")\n",
    "            \n",
    "            # Get embeddings for batch\n",
    "            texts = [chunk.content for chunk in batch]\n",
    "            \n",
    "            try:\n",
    "                # Use OpenAI batch API for efficiency\n",
    "                response = client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=texts\n",
    "                )\n",
    "                \n",
    "                # Assign embeddings to chunks\n",
    "                for j, embedding_data in enumerate(response.data):\n",
    "                    batch[j].embedding = embedding_data.embedding\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
    "                # Fallback to individual requests\n",
    "                for chunk in batch:\n",
    "                    embedding = get_embedding(chunk.content)\n",
    "                    if embedding:\n",
    "                        chunk.embedding = embedding\n",
    "        \n",
    "        self.embeddings_generated = True\n",
    "        print(\"‚úÖ Embedding generation complete!\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5, min_similarity: float = 0.3, \n",
    "               doc_type_filter: Optional[str] = None) -> List[Tuple[ChunkInfo, float]]:\n",
    "        \"\"\"Search for relevant chunks using semantic similarity\"\"\"\n",
    "        if not self.embeddings_generated:\n",
    "            raise ValueError(\"Embeddings not generated. Call generate_embeddings() first.\")\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = get_embedding(query)\n",
    "        if not query_embedding:\n",
    "            return []\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for chunk in self.processor.chunks.values():\n",
    "            if chunk.embedding is None:\n",
    "                continue\n",
    "            \n",
    "            # Apply document type filter if specified\n",
    "            if doc_type_filter:\n",
    "                doc_type = self.processor.documents[chunk.document_id].doc_type\n",
    "                if doc_type != doc_type_filter:\n",
    "                    continue\n",
    "            \n",
    "            similarity = cosine_similarity(query_embedding, chunk.embedding)\n",
    "            \n",
    "            # Apply minimum similarity filter\n",
    "            if similarity >= min_similarity:\n",
    "                similarities.append((chunk, similarity))\n",
    "        \n",
    "        # Sort by similarity (highest first) and return top_k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def batch_search(self, queries: List[str], top_k: int = 3) -> Dict[str, List[Tuple[ChunkInfo, float]]]:\n",
    "        \"\"\"Perform multiple searches efficiently\"\"\"\n",
    "        results = {}\n",
    "        for query in queries:\n",
    "            results[query] = self.search(query, top_k=top_k)\n",
    "        return results\n",
    "    \n",
    "    def get_search_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about the search index\"\"\"\n",
    "        embedded_chunks = sum(1 for chunk in self.processor.chunks.values() \n",
    "                             if chunk.embedding is not None)\n",
    "        \n",
    "        return {\n",
    "            \"total_chunks\": len(self.processor.chunks),\n",
    "            \"embedded_chunks\": embedded_chunks,\n",
    "            \"embedding_coverage\": embedded_chunks / len(self.processor.chunks) if self.processor.chunks else 0,\n",
    "            \"searchable_documents\": len(self.processor.documents)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ DocumentSearcher class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create searcher and generate embeddings\n",
    "searcher = DocumentSearcher(processor)\n",
    "\n",
    "print(\"Generating embeddings for search index...\")\n",
    "searcher.generate_embeddings(batch_size=5)  # Small batch size for demo\n",
    "\n",
    "# Display search statistics\n",
    "search_stats = searcher.get_search_statistics()\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"SEARCH INDEX STATISTICS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total chunks: {search_stats['total_chunks']}\")\n",
    "print(f\"Embedded chunks: {search_stats['embedded_chunks']}\")\n",
    "print(f\"Embedding coverage: {search_stats['embedding_coverage']:.1%}\")\n",
    "print(f\"Searchable documents: {search_stats['searchable_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Our Search System\n",
    "\n",
    "Let's test our search system with various financial queries to see how well it retrieves relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries\n",
    "test_queries = [\n",
    "    \"revenue growth and financial performance\",\n",
    "    \"risk factors and cybersecurity threats\",\n",
    "    \"cloud services and technology business\",\n",
    "    \"cash flow and liquidity position\",\n",
    "    \"market competition and competitive risks\"\n",
    "]\n",
    "\n",
    "print(\"üîç TESTING SEARCH SYSTEM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nQuery {i}: \\\"{query}\\\"\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results = searcher.search(query, top_k=3, min_similarity=0.2)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No relevant results found.\")\n",
    "        continue\n",
    "    \n",
    "    for j, (chunk, similarity) in enumerate(results, 1):\n",
    "        doc_title = processor.documents[chunk.document_id].title\n",
    "        doc_type = processor.documents[chunk.document_id].doc_type\n",
    "        \n",
    "        print(f\"  {j}. Score: {similarity:.3f} | {doc_type} | {doc_title}\")\n",
    "        print(f\"     \\\"{chunk.content[:100]}...\\\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Search Features\n",
    "\n",
    "Let's test some advanced search capabilities like filtering by document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test document type filtering\n",
    "print(\"üéØ TESTING FILTERED SEARCH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "query = \"financial performance and growth\"\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "\n",
    "# Search across all document types\n",
    "all_results = searcher.search(query, top_k=5)\n",
    "print(\"All Documents:\")\n",
    "for i, (chunk, similarity) in enumerate(all_results, 1):\n",
    "    doc_type = processor.documents[chunk.document_id].doc_type\n",
    "    doc_title = processor.documents[chunk.document_id].title\n",
    "    print(f\"  {i}. {similarity:.3f} | {doc_type} | {doc_title[:30]}...\")\n",
    "\n",
    "# Search only in earnings calls\n",
    "earnings_results = searcher.search(query, top_k=3, doc_type_filter=\"earnings_call\")\n",
    "print(f\"\\nEarnings Calls Only:\")\n",
    "for i, (chunk, similarity) in enumerate(earnings_results, 1):\n",
    "    doc_title = processor.documents[chunk.document_id].title\n",
    "    print(f\"  {i}. {similarity:.3f} | {doc_title}\")\n",
    "    print(f\"     \\\"{chunk.content[:80]}...\\\"\")\n",
    "\n",
    "# Search only in risk factors\n",
    "risk_results = searcher.search(\"cybersecurity data privacy\", top_k=2, doc_type_filter=\"10k_filing\")\n",
    "print(f\"\\n10-K Filings (Risk Factors):\")\n",
    "for i, (chunk, similarity) in enumerate(risk_results, 1):\n",
    "    doc_title = processor.documents[chunk.document_id].title\n",
    "    print(f\"  {i}. {similarity:.3f} | {doc_title}\")\n",
    "    print(f\"     \\\"{chunk.content[:80]}...\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Complete Retrieval System (20 minutes)\n",
    "\n",
    "Let's integrate everything into a comprehensive financial document retrieval system with advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDocumentRetriever:\n",
    "    \"\"\"Complete financial document retrieval system\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_method: str = \"sentences\", chunk_size: int = 400):\n",
    "        self.processor = DocumentProcessor(chunk_method, chunk_size)\n",
    "        self.searcher = None\n",
    "        self.index_built = False\n",
    "        \n",
    "    def ingest_documents(self, document_paths: List[str]) -> None:\n",
    "        \"\"\"Ingest multiple documents and build search index\"\"\"\n",
    "        print(\"üìÑ Ingesting documents...\")\n",
    "        \n",
    "        for doc_path in document_paths:\n",
    "            self.processor.process_document(doc_path)\n",
    "        \n",
    "        # Build search index\n",
    "        print(\"\\nüîç Building search index...\")\n",
    "        self.searcher = DocumentSearcher(self.processor)\n",
    "        self.searcher.generate_embeddings()\n",
    "        self.index_built = True\n",
    "        \n",
    "        stats = self.processor.get_statistics()\n",
    "        print(f\"\\n‚úÖ Ingestion complete!\")\n",
    "        print(f\"   Documents: {stats['total_documents']}\")\n",
    "        print(f\"   Chunks: {stats['total_chunks']}\")\n",
    "        print(f\"   Types: {list(stats['document_types'].keys())}\")\n",
    "    \n",
    "    def search_documents(self, query: str, top_k: int = 5, \n",
    "                        doc_type: Optional[str] = None,\n",
    "                        min_similarity: float = 0.3,\n",
    "                        include_context: bool = True) -> List[Dict]:\n",
    "        \"\"\"Search documents with rich result formatting\"\"\"\n",
    "        if not self.index_built:\n",
    "            raise ValueError(\"Index not built. Call ingest_documents() first.\")\n",
    "        \n",
    "        # Perform search\n",
    "        raw_results = self.searcher.search(query, top_k, min_similarity, doc_type)\n",
    "        \n",
    "        # Format results with rich metadata\n",
    "        formatted_results = []\n",
    "        for chunk, similarity in raw_results:\n",
    "            doc_info = self.processor.documents[chunk.document_id]\n",
    "            \n",
    "            result = {\n",
    "                \"similarity_score\": similarity,\n",
    "                \"chunk_id\": chunk.chunk_id,\n",
    "                \"content\": chunk.content,\n",
    "                \"document\": {\n",
    "                    \"title\": doc_info.title,\n",
    "                    \"type\": doc_info.doc_type,\n",
    "                    \"filename\": doc_info.filename\n",
    "                },\n",
    "                \"chunk_info\": {\n",
    "                    \"index\": chunk.chunk_index,\n",
    "                    \"word_count\": chunk.word_count,\n",
    "                    \"char_count\": chunk.char_count\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add context if requested\n",
    "            if include_context:\n",
    "                context = self.get_document_context(chunk.chunk_id, context_size=1)\n",
    "                result[\"context\"] = context\n",
    "            \n",
    "            formatted_results.append(result)\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    def get_document_context(self, chunk_id: str, context_size: int = 2) -> Dict:\n",
    "        \"\"\"Get surrounding chunks for context\"\"\"\n",
    "        if chunk_id not in self.processor.chunks:\n",
    "            return {}\n",
    "        \n",
    "        target_chunk = self.processor.chunks[chunk_id]\n",
    "        doc_id = target_chunk.document_id\n",
    "        target_index = target_chunk.chunk_index\n",
    "        \n",
    "        # Find chunks from same document\n",
    "        doc_chunks = [chunk for chunk in self.processor.chunks.values() \n",
    "                     if chunk.document_id == doc_id]\n",
    "        doc_chunks.sort(key=lambda x: x.chunk_index)\n",
    "        \n",
    "        # Get context chunks\n",
    "        context = {\n",
    "            \"before\": [],\n",
    "            \"after\": []\n",
    "        }\n",
    "        \n",
    "        for chunk in doc_chunks:\n",
    "            if chunk.chunk_index < target_index and chunk.chunk_index >= target_index - context_size:\n",
    "                context[\"before\"].append({\n",
    "                    \"chunk_id\": chunk.chunk_id,\n",
    "                    \"content\": chunk.content[:100] + \"...\"\n",
    "                })\n",
    "            elif chunk.chunk_index > target_index and chunk.chunk_index <= target_index + context_size:\n",
    "                context[\"after\"].append({\n",
    "                    \"chunk_id\": chunk.chunk_id,\n",
    "                    \"content\": chunk.content[:100] + \"...\"\n",
    "                })\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Combine semantic and keyword-based search (simplified version)\"\"\"\n",
    "        # Get semantic search results\n",
    "        semantic_results = self.search_documents(query, top_k=top_k, include_context=False)\n",
    "        \n",
    "        # Simple keyword boost\n",
    "        query_words = set(query.lower().split())\n",
    "        \n",
    "        for result in semantic_results:\n",
    "            content_words = set(result[\"content\"].lower().split())\n",
    "            keyword_overlap = len(query_words.intersection(content_words))\n",
    "            \n",
    "            # Boost score based on keyword overlap\n",
    "            keyword_boost = keyword_overlap * 0.1\n",
    "            result[\"hybrid_score\"] = result[\"similarity_score\"] + keyword_boost\n",
    "            result[\"keyword_matches\"] = keyword_overlap\n",
    "        \n",
    "        # Re-sort by hybrid score\n",
    "        semantic_results.sort(key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
    "        \n",
    "        return semantic_results\n",
    "    \n",
    "    def get_system_summary(self) -> Dict:\n",
    "        \"\"\"Get comprehensive system summary\"\"\"\n",
    "        if not self.index_built:\n",
    "            return {\"status\": \"Index not built\"}\n",
    "        \n",
    "        doc_stats = self.processor.get_statistics()\n",
    "        search_stats = self.searcher.get_search_statistics()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"Ready\",\n",
    "            \"documents\": doc_stats,\n",
    "            \"search_index\": search_stats,\n",
    "            \"capabilities\": [\n",
    "                \"Semantic search\",\n",
    "                \"Document type filtering\",\n",
    "                \"Context retrieval\",\n",
    "                \"Hybrid search\",\n",
    "                \"Similarity scoring\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ FinancialDocumentRetriever class ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive System Test\n",
    "\n",
    "Let's test our complete retrieval system with real-world financial queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test the complete system\n",
    "retriever = FinancialDocumentRetriever(chunk_method=\"sentences\", chunk_size=400)\n",
    "\n",
    "# Get list of document paths\n",
    "doc_paths = [str(path) for path in Path(\"sample_docs\").glob(\"*.txt\")]\n",
    "\n",
    "# Ingest documents\n",
    "retriever.ingest_documents(doc_paths)\n",
    "\n",
    "# Get system summary\n",
    "summary = retriever.get_system_summary()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINANCIAL DOCUMENT RETRIEVAL SYSTEM\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Status: {summary['status']}\")\n",
    "print(f\"Documents: {summary['documents']['total_documents']}\")\n",
    "print(f\"Chunks: {summary['documents']['total_chunks']}\")\n",
    "print(f\"Document Types: {list(summary['documents']['document_types'].keys())}\")\n",
    "print(f\"Capabilities: {', '.join(summary['capabilities'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Query Testing\n",
    "\n",
    "Let's test with realistic financial analysis queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test real-world financial queries\n",
    "financial_queries = [\n",
    "    \"What does the company say about revenue growth?\",\n",
    "    \"Find information about debt and liabilities\",\n",
    "    \"Show risk factors mentioned in filings\",\n",
    "    \"What are the analysts saying about the stock price?\",\n",
    "    \"How is the cloud business performing?\"\n",
    "]\n",
    "\n",
    "print(\"\\nüíº REAL-WORLD FINANCIAL QUERIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, query in enumerate(financial_queries, 1):\n",
    "    print(f\"\\nüîç Query {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = retriever.search_documents(query, top_k=2, min_similarity=0.2)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"   No relevant results found.\")\n",
    "        continue\n",
    "    \n",
    "    for j, result in enumerate(results, 1):\n",
    "        print(f\"\\n   Result {j}: Score {result['similarity_score']:.3f}\")\n",
    "        print(f\"   üìÑ {result['document']['type']} | {result['document']['title']}\")\n",
    "        print(f\"   üí¨ \\\"{result['content'][:120]}...\\\"\")\n",
    "        \n",
    "        # Show context if available\n",
    "        if 'context' in result and result['context']['before']:\n",
    "            print(f\"   üìù Context: ...{result['context']['before'][-1]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hybrid Search\n",
    "\n",
    "Let's compare semantic search vs hybrid search that combines semantic similarity with keyword matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare semantic vs hybrid search\n",
    "test_query = \"cloud revenue growth performance\"\n",
    "\n",
    "print(f\"\\nüî¨ SEMANTIC vs HYBRID SEARCH COMPARISON\")\n",
    "print(f\"Query: \\\"{test_query}\\\"\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Semantic search\n",
    "semantic_results = retriever.search_documents(test_query, top_k=3, include_context=False)\n",
    "print(\"\\nüìä SEMANTIC SEARCH:\")\n",
    "for i, result in enumerate(semantic_results, 1):\n",
    "    print(f\"  {i}. Score: {result['similarity_score']:.3f} | {result['document']['type']}\")\n",
    "    print(f\"     \\\"{result['content'][:100]}...\\\"\\n\")\n",
    "\n",
    "# Hybrid search\n",
    "hybrid_results = retriever.hybrid_search(test_query, top_k=3)\n",
    "print(\"\\nüîÄ HYBRID SEARCH (Semantic + Keywords):\")\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"  {i}. Hybrid Score: {result['hybrid_score']:.3f} (Semantic: {result['similarity_score']:.3f}, Keywords: {result['keyword_matches']})\")\n",
    "    print(f\"     {result['document']['type']} | \\\"{result['content'][:100]}...\\\"\\n\")\n",
    "\n",
    "print(\"üí° Notice how hybrid search can rerank results based on keyword matches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Our Work for Day 2\n",
    "\n",
    "Let's save our processed documents and embeddings for use in tomorrow's RAG implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save document processing results for Day 2\n",
    "def save_retrieval_system(retriever: FinancialDocumentRetriever, save_dir: str = \"saved_index\"):\n",
    "    \"\"\"Save the complete retrieval system for reuse\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save document metadata\n",
    "    documents_data = {}\n",
    "    for doc_id, doc_info in retriever.processor.documents.items():\n",
    "        documents_data[doc_id] = {\n",
    "            \"filename\": doc_info.filename,\n",
    "            \"title\": doc_info.title,\n",
    "            \"doc_type\": doc_info.doc_type,\n",
    "            \"word_count\": doc_info.word_count,\n",
    "            \"char_count\": doc_info.char_count\n",
    "        }\n",
    "    \n",
    "    with open(save_path / \"documents.json\", 'w') as f:\n",
    "        json.dump(documents_data, f, indent=2)\n",
    "    \n",
    "    # Save chunks and embeddings\n",
    "    chunks_data = {}\n",
    "    for chunk_id, chunk_info in retriever.processor.chunks.items():\n",
    "        chunks_data[chunk_id] = {\n",
    "            \"document_id\": chunk_info.document_id,\n",
    "            \"content\": chunk_info.content,\n",
    "            \"chunk_index\": chunk_info.chunk_index,\n",
    "            \"char_count\": chunk_info.char_count,\n",
    "            \"word_count\": chunk_info.word_count,\n",
    "            \"embedding\": chunk_info.embedding\n",
    "        }\n",
    "    \n",
    "    with open(save_path / \"chunks_and_embeddings.json\", 'w') as f:\n",
    "        json.dump(chunks_data, f, indent=2)\n",
    "    \n",
    "    # Save system configuration\n",
    "    config = {\n",
    "        \"chunk_method\": retriever.processor.chunk_method,\n",
    "        \"chunk_size\": retriever.processor.chunk_size,\n",
    "        \"total_documents\": len(retriever.processor.documents),\n",
    "        \"total_chunks\": len(retriever.processor.chunks),\n",
    "        \"embedding_model\": \"text-embedding-3-small\"\n",
    "    }\n",
    "    \n",
    "    with open(save_path / \"config.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Retrieval system saved to: {save_path.absolute()}\")\n",
    "    print(f\"   üìÑ Documents: {len(documents_data)}\")\n",
    "    print(f\"   üîç Chunks with embeddings: {len(chunks_data)}\")\n",
    "    print(f\"   üìÅ Files: documents.json, chunks_and_embeddings.json, config.json\")\n",
    "\n",
    "# Save our system\n",
    "save_retrieval_system(retriever)\n",
    "\n",
    "print(\"\\nüéØ NOTEBOOK 2 COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚úÖ Document processing pipeline built\")\n",
    "print(\"‚úÖ Similarity search system implemented\")\n",
    "print(\"‚úÖ Hybrid search capabilities added\")\n",
    "print(\"‚úÖ Context retrieval working\")\n",
    "print(\"‚úÖ System saved for Day 2 RAG implementation\")\n",
    "print(\"\\nüìã Ready for Day 2: Complete RAG Pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What We've Built\n",
    "\n",
    "In this notebook, we've created a comprehensive document processing and search system:\n",
    "\n",
    "### **üèóÔ∏è Core Components:**\n",
    "1. **DocumentChunker**: Multiple chunking strategies (character, sentence, paragraph, overlap)\n",
    "2. **DocumentProcessor**: Complete document ingestion pipeline with metadata\n",
    "3. **DocumentSearcher**: Efficient similarity-based search with filtering\n",
    "4. **FinancialDocumentRetriever**: Complete system with advanced features\n",
    "\n",
    "### **‚ö° Key Features:**\n",
    "- **Flexible Chunking**: Choose optimal strategy for your use case\n",
    "- **Rich Metadata**: Track document types, sources, and chunk relationships\n",
    "- **Semantic Search**: Find relevant content by meaning, not just keywords\n",
    "- **Hybrid Search**: Combine semantic similarity with keyword matching\n",
    "- **Context Retrieval**: Get surrounding chunks for better understanding\n",
    "- **Filtering**: Search within specific document types\n",
    "- **Scalable**: Batch processing and efficient embedding management\n",
    "\n",
    "### **üìä Performance Insights:**\n",
    "- **Chunking Trade-offs**: Balance between context and precision\n",
    "- **Search Quality**: Semantic similarity captures meaning beyond keywords\n",
    "- **Efficiency**: Batch embedding generation and similarity calculations\n",
    "\n",
    "### **üöÄ Ready for Day 2:**\n",
    "Our retrieval system provides the perfect foundation for implementing a complete RAG (Retrieval-Augmented Generation) pipeline. We have:\n",
    "- ‚úÖ Document processing and chunking\n",
    "- ‚úÖ Embedding generation and storage\n",
    "- ‚úÖ Similarity search and ranking\n",
    "- ‚úÖ Context and metadata retrieval\n",
    "\n",
    "**Next**: We'll combine this retrieval system with LLM generation to create a complete question-answering system that can provide accurate, grounded responses about financial documents!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
