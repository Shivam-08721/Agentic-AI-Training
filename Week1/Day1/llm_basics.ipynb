{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models (LLMs)\n",
    "\n",
    "This notebook bridges the gap between rule-based agents and AI-powered agents by introducing the fundamentals of Large Language Models (LLMs). We'll cover what LLMs are, how to use them, and how they differ from the pattern-matching approach we saw in the SimpleAgent.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- What LLMs are and how they differ from rule-based systems\n",
    "- How to set up and use LLM APIs\n",
    "- Basic prompting techniques\n",
    "- Key concepts: tokens, temperature, and context windows\n",
    "- How to build simple LLM-powered agents\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed the `agent_demo.ipynb` notebook\n",
    "- Basic Python knowledge\n",
    "- API keys for OpenAI or OpenRouter (we'll use OpenRouter for flexibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Pattern Matching to Language Understanding\n",
    "\n",
    "### Rule-Based Approach (What We've Seen)\n",
    "\n",
    "In the SimpleAgent, we used pattern matching:\n",
    "```python\n",
    "if \"weather\" in user_input:\n",
    "    return self.get_weather()\n",
    "elif \"time\" in user_input:\n",
    "    return self.get_time()\n",
    "```\n",
    "\n",
    "**Limitations:**\n",
    "- Only understands exact patterns\n",
    "- Can't handle variations or context\n",
    "- Requires explicit programming for each case\n",
    "- No real \"understanding\" of language\n",
    "\n",
    "### LLM Approach (What We're Learning)\n",
    "\n",
    "LLMs understand language naturally:\n",
    "- \"What's the weather?\" âœ“\n",
    "- \"How's the weather looking?\" âœ“\n",
    "- \"Is it going to rain?\" âœ“\n",
    "- \"Do I need an umbrella?\" âœ“\n",
    "\n",
    "All these variations are understood without explicit programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What Are Large Language Models?\n",
    "\n",
    "**Large Language Models (LLMs)** are AI systems trained on vast amounts of text data to understand and generate human-like text. Think of them as incredibly sophisticated pattern recognition systems that have \"read\" much of the internet.\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "1. **Trained on Text**: Learned from billions of web pages, books, and articles\n",
    "2. **Probabilistic**: Generate responses based on patterns they've learned\n",
    "3. **Contextual**: Understand meaning based on surrounding context\n",
    "4. **Generative**: Can create new, coherent text\n",
    "5. **Versatile**: Can perform many tasks without specific programming\n",
    "\n",
    "### Popular LLMs for Educational Use:\n",
    "- **Claude Haiku** (Anthropic): Fast, reliable, and helpful - our default choice\n",
    "- **DeepSeek** (DeepSeek): Excellent balance of cost and capability, strong at coding\n",
    "- **Llama 3.3** (Meta): Open-source, multilingual, 70B parameters\n",
    "- **GPT-4o-mini** (OpenAI): Efficient and reliable for general tasks\n",
    "- **Gemma** (Google): Good for resource-constrained deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up LLM Access\n",
    "\n",
    "We'll use OpenRouter, which provides access to multiple LLMs through a single API. This gives us flexibility to try different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Make sure you have installed all dependencies from `requirements.txt` before running this notebook:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Understanding the LLM API Call Components\n",
    "\n",
    "Before we make our first API call, let's understand each component of an LLM API request:\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"anthropic/claude-3.5-haiku\",    # Which LLM to use\n",
    "    messages=[                             # The conversation history\n",
    "        {\n",
    "            \"role\": \"system\",              # Who is speaking\n",
    "            \"content\": \"You are helpful\"   # What they're saying\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello!\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.7,                       # Creativity level (0-2)\n",
    "    max_tokens=100,                        # Maximum response length\n",
    "    top_p=1.0,                            # Nucleus sampling (advanced)\n",
    "    frequency_penalty=0,                   # Reduce repetition\n",
    "    presence_penalty=0,                    # Encourage new topics\n",
    "    stream=False                          # Get response all at once\n",
    ")\n",
    "```\n",
    "\n",
    "### Key Components Explained:\n",
    "\n",
    "1. **client**: The OpenAI client configured with OpenRouter's endpoint\n",
    "   - Handles authentication and API communication\n",
    "   - Routes requests to different model providers\n",
    "\n",
    "2. **model**: Specifies which LLM to use\n",
    "   - Format: `provider/model-name` (e.g., \"anthropic/claude-3.5-haiku\")\n",
    "   - Different models have different strengths, costs, and speeds\n",
    "\n",
    "3. **messages**: The conversation context as a list of message objects\n",
    "   - Each message has a `role` and `content`\n",
    "   - Maintains conversation history for context\n",
    "\n",
    "4. **temperature** (0.0 - 2.0): Controls randomness\n",
    "   - 0: Deterministic, always picks most likely word\n",
    "   - 0.7: Balanced creativity (default)\n",
    "   - 2.0: Very creative, might be incoherent\n",
    "\n",
    "5. **max_tokens**: Maximum length of the response\n",
    "   - 1 token â‰ˆ 0.75 words\n",
    "   - Prevents runaway responses and controls costs\n",
    "   - Set based on your needs (e.g., 50 for short answers, 500 for essays)\n",
    "\n",
    "6. **Optional Parameters**:\n",
    "   - **top_p**: Alternative to temperature for controlling randomness\n",
    "   - **frequency_penalty** (-2.0 to 2.0): Reduces word repetition\n",
    "   - **presence_penalty** (-2.0 to 2.0): Encourages talking about new topics\n",
    "   - **stream**: If True, receive response in chunks (like ChatGPT typing)\n",
    "\n",
    "### Response Object Structure:\n",
    "\n",
    "```python\n",
    "response = {\n",
    "    \"id\": \"chatcmpl-abc123\",              # Unique identifier\n",
    "    \"object\": \"chat.completion\",           # Response type\n",
    "    \"created\": 1677858242,                 # Timestamp\n",
    "    \"model\": \"anthropic/claude-3.5-haiku\", # Model used\n",
    "    \"usage\": {                             # Token usage info\n",
    "        \"prompt_tokens\": 13,\n",
    "        \"completion_tokens\": 7,\n",
    "        \"total_tokens\": 20\n",
    "    },\n",
    "    \"choices\": [{                          # Array of responses (usually 1)\n",
    "        \"message\": {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Hello! How can I help you today?\"\n",
    "        },\n",
    "        \"finish_reason\": \"stop\",           # Why generation stopped\n",
    "        \"index\": 0\n",
    "    }]\n",
    "}\n",
    "```\n",
    "\n",
    "To extract the actual response text:\n",
    "```python\n",
    "response_text = response.choices[0].message.content\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "# Initialize OpenAI client with OpenRouter configuration\n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "# Check if credentials are loaded\n",
    "print(f\"API Base URL: {os.getenv('OPENROUTER_BASE_URL')}\")\n",
    "print(f\"API Key: {'Loaded successfully' if os.getenv('OPENROUTER_API_KEY') else 'Not found'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a complete API call with all components labeled\n",
    "def detailed_api_call_example():\n",
    "    \"\"\"Demonstrate a complete API call with all components\"\"\"\n",
    "    \n",
    "    # Configure the request\n",
    "    request_config = {\n",
    "        \"model\": \"anthropic/claude-3.5-haiku\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant who explains things clearly.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is photosynthesis?\"\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.7,      # Balanced creativity\n",
    "        \"max_tokens\": 150,       # Enough for a paragraph\n",
    "        \"top_p\": 1.0,           # Use all vocabulary (default)\n",
    "        \"frequency_penalty\": 0,  # No penalty for repeated words\n",
    "        \"presence_penalty\": 0    # No penalty for staying on topic\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ“¤ REQUEST CONFIGURATION:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, value in request_config.items():\n",
    "        if key != \"messages\":\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"\\nMessages:\")\n",
    "    for msg in request_config[\"messages\"]:\n",
    "        print(f\"  [{msg['role']}]: {msg['content']}\")\n",
    "    \n",
    "    # Make the API call\n",
    "    try:\n",
    "        response = client.chat.completions.create(**request_config)\n",
    "        \n",
    "        print(\"\\nðŸ“¥ RESPONSE DETAILS:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Response ID: {response.id}\")\n",
    "        print(f\"Model used: {response.model}\")\n",
    "        print(f\"Created at: {response.created}\")\n",
    "        \n",
    "        print(\"\\nðŸ’° TOKEN USAGE:\")\n",
    "        print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "        print(f\"Response tokens: {response.usage.completion_tokens}\")\n",
    "        print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "        \n",
    "        print(\"\\nðŸ¤– ASSISTANT'S RESPONSE:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(response.choices[0].message.content)\n",
    "        \n",
    "        print(\"\\nðŸ›‘ FINISH REASON:\", response.choices[0].finish_reason)\n",
    "        print(\"(Common reasons: 'stop' = natural end, 'length' = hit max_tokens)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {str(e)}\")\n",
    "\n",
    "# Run the example\n",
    "detailed_api_call_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your First LLM Call\n",
    "\n",
    "Let's start with the simplest possible LLM interaction - asking it a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple completion example\n",
    "def ask_llm(question, model=\"anthropic/claude-3.5-haiku\"):\n",
    "    \"\"\"Ask a question to an LLM and get a response\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Try it out\n",
    "response = ask_llm(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how different this is from our SimpleAgent! We didn't program any facts about France - the LLM knows this from its training.\n",
    "\n",
    "Let's compare with our SimpleAgent approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: Rule-based vs LLM approach\n",
    "\n",
    "# Rule-based approach (like SimpleAgent)\n",
    "def answer_capital_rule_based(country):\n",
    "    capitals = {\n",
    "        \"france\": \"Paris\",\n",
    "        \"spain\": \"Madrid\",\n",
    "        \"italy\": \"Rome\"\n",
    "        # We'd need to add every country manually!\n",
    "    }\n",
    "    return capitals.get(country.lower(), \"I don't know that capital\")\n",
    "\n",
    "# LLM approach\n",
    "def answer_capital_llm(country):\n",
    "    return ask_llm(f\"What is the capital of {country}?\")\n",
    "\n",
    "# Test both approaches\n",
    "test_countries = [\"France\", \"Spain\", \"Japan\", \"Kenya\", \"Brazil\"]\n",
    "\n",
    "print(\"Rule-based approach:\")\n",
    "for country in test_countries:\n",
    "    print(f\"  {country}: {answer_capital_rule_based(country)}\")\n",
    "\n",
    "print(\"\\nLLM approach:\")\n",
    "for country in test_countries:\n",
    "    print(f\"  {country}: {answer_capital_llm(country)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Chat Format\n",
    "\n",
    "As we saw in the API components section, modern LLMs use a \"chat\" format. Let's explore how this works in practice with different message combinations and see how the roles affect the AI's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llm(user_message, system_message=None, model=\"anthropic/claude-3.5-haiku\"):\n",
    "    \"\"\"Chat with an LLM using system and user messages\"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    # Add system message if provided\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    # Add user message\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Example 1: Without system message\n",
    "print(\"Without system message:\")\n",
    "print(chat_with_llm(\"Tell me a joke\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: With system message defining behavior\n",
    "print(\"With system message (pirate):\")\n",
    "print(chat_with_llm(\n",
    "    \"Tell me a joke\",\n",
    "    system_message=\"You are a friendly pirate. Always speak like a pirate would.\"\n",
    "))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 3: With system message for a specific task\n",
    "print(\"With system message (teacher):\")\n",
    "print(chat_with_llm(\n",
    "    \"What is 2+2?\",\n",
    "    system_message=\"You are a patient math teacher. Explain your answers step by step.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Concept: Tokens in Practice\n",
    "\n",
    "We learned that tokens are the basic units LLMs process. Now let's see how token usage affects your costs and performance in real scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how token usage works\n",
    "def analyze_token_usage(message, model=\"anthropic/claude-3.5-haiku\"):\n",
    "    \"\"\"Analyze token usage for a message\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        max_tokens=50  # Limit response length\n",
    "    )\n",
    "    \n",
    "    # Get token usage from response\n",
    "    usage = response.usage\n",
    "    \n",
    "    return {\n",
    "        \"response\": response.choices[0].message.content,\n",
    "        \"prompt_tokens\": usage.prompt_tokens,\n",
    "        \"completion_tokens\": usage.completion_tokens,\n",
    "        \"total_tokens\": usage.total_tokens\n",
    "    }\n",
    "\n",
    "# Test with different message lengths\n",
    "messages = [\n",
    "    \"Hi\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Can you explain quantum computing in simple terms that a child could understand?\"\n",
    "]\n",
    "\n",
    "for msg in messages:\n",
    "    result = analyze_token_usage(msg)\n",
    "    print(f\"Message: '{msg}'\")\n",
    "    print(f\"Prompt tokens: {result['prompt_tokens']}\")\n",
    "    print(f\"Response tokens: {result['completion_tokens']}\")\n",
    "    print(f\"Total tokens: {result['total_tokens']}\")\n",
    "    print(f\"Response: {result['response'][:1000]}...\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Concept: Temperature in Action\n",
    "\n",
    "We know temperature controls creativity (0 = deterministic, 2 = very creative). Let's experiment to see exactly how different temperature values affect various types of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_temperature(prompt, temperatures=[0, 0.5, 1.0, 1.5]):\n",
    "    \"\"\"Test how temperature affects responses\"\"\"\n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"anthropic/claude-3.5-haiku\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        print(f\"Temperature {temp}: {response.choices[0].message.content}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "# Test with a creative prompt\n",
    "test_temperature(\"Write a one-sentence story about a robot:\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Test with a factual prompt\n",
    "test_temperature(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how:\n",
    "- Creative prompts benefit from higher temperature (more variety)\n",
    "- Factual prompts should use lower temperature (consistent accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Basic Prompting Techniques\n",
    "\n",
    "Good prompting is key to getting useful responses from LLMs. Here are fundamental techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 1: Be Specific\n",
    "print(\"âŒ Vague prompt:\")\n",
    "print(ask_llm(\"Tell me about dogs\")[:200] + \"...\\n\")\n",
    "\n",
    "print(\"âœ… Specific prompt:\")\n",
    "print(ask_llm(\"List 3 interesting facts about golden retrievers' behavior\"))\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 2: Provide Examples (Few-shot prompting)\n",
    "few_shot_prompt = \"\"\"\n",
    "Classify the sentiment of these movie reviews as positive or negative:\n",
    "\n",
    "Review: \"This movie was absolutely fantastic! Best film of the year.\"\n",
    "Sentiment: positive\n",
    "\n",
    "Review: \"Terrible waste of time. I want my money back.\"\n",
    "Sentiment: negative\n",
    "\n",
    "Review: \"The cinematography was breathtaking and the acting was superb.\"\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(\"Few-shot prompting result:\")\n",
    "print(ask_llm(few_shot_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 3: Specify Output Format\n",
    "format_prompt = \"\"\"\n",
    "Extract information about this product and return it as JSON:\n",
    "\n",
    "\"The new iPhone 15 Pro features a titanium design and starts at $999. \n",
    "It comes in 128GB, 256GB, and 512GB storage options and is available \n",
    "in Natural Titanium, Blue Titanium, White Titanium, and Black Titanium colors.\"\n",
    "\n",
    "Return only valid JSON with keys: product_name, starting_price, storage_options, colors\n",
    "\"\"\"\n",
    "\n",
    "print(\"Structured output:\")\n",
    "response = ask_llm(format_prompt)\n",
    "print(response)\n",
    "\n",
    "# Try to parse it as JSON\n",
    "try:\n",
    "    data = json.loads(response)\n",
    "    print(\"\\nâœ… Successfully parsed as JSON!\")\n",
    "    print(json.dumps(data, indent=2))\n",
    "except:\n",
    "    print(\"\\nâŒ Failed to parse as JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 4: Role-playing / Personas\n",
    "def ask_with_persona(question, persona):\n",
    "    return chat_with_llm(\n",
    "        question,\n",
    "        system_message=persona\n",
    "    )\n",
    "\n",
    "question = \"How do I make a sandwich?\"\n",
    "\n",
    "personas = [\n",
    "    \"You are Gordon Ramsay, the famous chef. Be passionate but helpful.\",\n",
    "    \"You are a nutritionist focused on healthy eating.\",\n",
    "    \"You are explaining to a 5-year-old child. Use simple words.\"\n",
    "]\n",
    "\n",
    "for persona in personas:\n",
    "    print(f\"Persona: {persona}\")\n",
    "    print(ask_with_persona(question, persona)[:200] + \"...\\n\")\n",
    "    print(\"-\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Building Your First LLM-Powered Agent\n",
    "\n",
    "Now let's create an agent that's smarter than our SimpleAgent but simpler than the complex reasoning models we'll see later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgent:\n",
    "    \"\"\"A simple LLM-powered agent that can handle various tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"anthropic/claude-3.5-haiku\"):\n",
    "        self.model = model\n",
    "        self.system_prompt = \"\"\"\n",
    "You are a helpful AI assistant. You can help with various tasks including:\n",
    "- Answering questions\n",
    "- Providing weather information (you should mention you'd need current data)\n",
    "- Doing calculations\n",
    "- Setting reminders (you should acknowledge the reminder)\n",
    "- General conversation\n",
    "\n",
    "Be concise but friendly in your responses.\n",
    "\"\"\"\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def process_input(self, user_input):\n",
    "        \"\"\"Process user input and return a response\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Prepare messages for API\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ] + self.conversation_history\n",
    "        \n",
    "        try:\n",
    "            # Get response from LLM\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            # Extract the response\n",
    "            assistant_response = response.choices[0].message.content\n",
    "            \n",
    "            # Add to history\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": assistant_response\n",
    "            })\n",
    "            \n",
    "            # Keep history manageable (last 10 messages)\n",
    "            if len(self.conversation_history) > 10:\n",
    "                self.conversation_history = self.conversation_history[-10:]\n",
    "            \n",
    "            return assistant_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Create and test the agent\n",
    "agent = LLMAgent()\n",
    "\n",
    "# Test the same inputs we used with SimpleAgent\n",
    "test_inputs = [\n",
    "    \"What's the weather like?\",\n",
    "    \"What time is it?\",\n",
    "    \"Remind me to buy groceries\",\n",
    "    \"Calculate 15 * 3 + 7\",\n",
    "    \"How do I make coffee?\"\n",
    "]\n",
    "\n",
    "print(\"LLM Agent Responses:\\n\")\n",
    "for user_input in test_inputs:\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Agent: {agent.process_input(user_input)}\")\n",
    "    print(\"-\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM agent remembers the context (your name, party details) across messages - something our SimpleAgent couldn't do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparing Different Models\n",
    "\n",
    "Different LLMs have different strengths. Let's compare a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(prompt, models):\n",
    "    \"\"\"Compare responses from different models\"\"\"\n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "    \n",
    "    for model_name in models:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Response: {response.choices[0].message.content}\")\n",
    "            print(\"-\" * 70 + \"\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            print(\"-\" * 70 + \"\\n\")\n",
    "\n",
    "# Compare different models\n",
    "models_to_test = [\n",
    "    \"anthropic/claude-3.5-haiku\",  # Fast and efficient (default)\n",
    "    \"deepseek/deepseek-chat\",  # Very cost-effective, strong performance\n",
    "    \"meta-llama/llama-3.3-70b-instruct\",  # Open source, excellent general purpose\n",
    "    \"openai/gpt-4o-mini\",  # OpenAI's efficient model\n",
    "]\n",
    "\n",
    "# Note: Some models might require different API setup or credits\n",
    "# For educational purposes, we'll test the first two which work well with OpenRouter\n",
    "compare_models(\"Explain quantum computing in one sentence.\", models_to_test[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Common Pitfalls and Best Practices\n",
    "\n",
    "### Common Pitfalls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 1: Assuming the LLM always tells the truth\n",
    "print(\"Pitfall 1: Hallucination\")\n",
    "response = ask_llm(\"What happened in the 2027 World Cup?\")\n",
    "print(f\"Response: {response}\")\n",
    "print(\"Note: The LLM might make up events that haven't happened!\\n\")\n",
    "\n",
    "# Pitfall 2: Not handling API errors\n",
    "print(\"Pitfall 2: Not handling errors\")\n",
    "try:\n",
    "    # This might fail due to rate limits, network issues, etc.\n",
    "    response = ask_llm(\"Test\", model=\"invalid-model-name\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(\"Always wrap API calls in try-except blocks!\\n\")\n",
    "\n",
    "# Pitfall 3: Sending sensitive information\n",
    "print(\"Pitfall 3: Security concerns\")\n",
    "print(\"NEVER send:\")\n",
    "print(\"- Passwords or API keys\")\n",
    "print(\"- Personal identification (SSN, credit cards)\")\n",
    "print(\"- Confidential business data\")\n",
    "print(\"- Private user information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeLLMAgent:\n",
    "    \"\"\"An LLM agent with best practices implemented\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"anthropic/claude-3.5-haiku\", max_retries=3):\n",
    "        self.model = model\n",
    "        self.max_retries = max_retries\n",
    "        self.system_prompt = \"\"\"\n",
    "You are a helpful AI assistant. Always:\n",
    "- Be honest about what you know and don't know\n",
    "- Refuse to process sensitive information like passwords\n",
    "- Provide helpful, accurate responses\n",
    "- Acknowledge when you're not certain about something\n",
    "\"\"\"\n",
    "    \n",
    "    def process_input(self, user_input, temperature=0.7, max_tokens=None):\n",
    "        \"\"\"Process input with error handling and retries\"\"\"\n",
    "        \n",
    "        # Input validation\n",
    "        if not user_input or not isinstance(user_input, str):\n",
    "            return \"Please provide a valid text input.\"\n",
    "        \n",
    "        # Check for sensitive information (basic check)\n",
    "        sensitive_keywords = ['password', 'ssn', 'credit card', 'api key']\n",
    "        if any(keyword in user_input.lower() for keyword in sensitive_keywords):\n",
    "            return \"I cannot process requests containing sensitive information.\"\n",
    "        \n",
    "        # Retry logic for API calls\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_input}\n",
    "                    ],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens\n",
    "                )\n",
    "                \n",
    "                return response.choices[0].message.content\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    return f\"I'm having trouble processing your request. Error: {str(e)}\"\n",
    "    \n",
    "    def estimate_cost(self, prompt, max_tokens=100):\n",
    "        \"\"\"Estimate the cost of a request (example for Claude Haiku)\"\"\"\n",
    "        # Rough estimation: ~4 characters per token\n",
    "        prompt_tokens = len(prompt) / 4\n",
    "        total_tokens = prompt_tokens + max_tokens\n",
    "        \n",
    "        # Claude Haiku pricing (check current prices on OpenRouter)\n",
    "        # As of 2025, Claude Haiku is fast and cost-effective\n",
    "        cost_per_1k_tokens = 0.00025  # $0.25 per 1M input tokens\n",
    "        estimated_cost = (total_tokens / 1000) * cost_per_1k_tokens\n",
    "        \n",
    "        return {\n",
    "            \"estimated_tokens\": int(total_tokens),\n",
    "            \"estimated_cost\": f\"${estimated_cost:.6f}\"\n",
    "        }\n",
    "\n",
    "# Test the safe agent\n",
    "safe_agent = SafeLLMAgent()\n",
    "\n",
    "# Test normal input\n",
    "print(\"Normal input:\")\n",
    "print(safe_agent.process_input(\"What is machine learning?\"))\n",
    "print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "\n",
    "# Test sensitive input\n",
    "print(\"Sensitive input:\")\n",
    "print(safe_agent.process_input(\"My password is 12345\"))\n",
    "print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "\n",
    "# Test cost estimation\n",
    "print(\"Cost estimation:\")\n",
    "prompt = \"Explain the theory of relativity\"\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(safe_agent.estimate_cost(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. From Simple to Advanced: What's Next?\n",
    "\n",
    "We've covered the basics of LLMs. Here's how this connects to what's coming:\n",
    "\n",
    "### What We've Learned:\n",
    "1. **LLMs vs Rule-Based**: LLMs understand language naturally\n",
    "2. **API Usage**: How to call LLMs and handle responses\n",
    "3. **Key Concepts**: Tokens, temperature, prompting\n",
    "4. **Basic Agent**: Built a simple LLM-powered agent\n",
    "\n",
    "### What's Coming Next:\n",
    "1. **Thinking Models**: Models that show their reasoning process\n",
    "2. **Advanced Prompting**: Chain of thought, ReAct, Tree of thoughts\n",
    "3. **Tool Use**: Agents that can use external tools and APIs\n",
    "4. **Memory Systems**: Long-term memory and context management\n",
    "5. **Multi-Agent Systems**: Multiple agents working together\n",
    "\n",
    "### Quick Comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a visual comparison of the three approaches\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    \"Aspect\": [\n",
    "        \"Language Understanding\",\n",
    "        \"Flexibility\",\n",
    "        \"Context Awareness\",\n",
    "        \"Learning Ability\",\n",
    "        \"Transparency\",\n",
    "        \"Cost\",\n",
    "        \"Speed\",\n",
    "        \"Reliability\"\n",
    "    ],\n",
    "    \"Rule-Based (SimpleAgent)\": [\n",
    "        \"âŒ Only exact patterns\",\n",
    "        \"âŒ Rigid, needs programming\",\n",
    "        \"âŒ No context memory\",\n",
    "        \"âŒ Cannot learn\",\n",
    "        \"âœ… Fully transparent\",\n",
    "        \"âœ… Free to run\",\n",
    "        \"âœ… Instant\",\n",
    "        \"âœ… 100% predictable\"\n",
    "    ],\n",
    "    \"Basic LLM (This notebook)\": [\n",
    "        \"âœ… Natural language\",\n",
    "        \"âœ… Very flexible\",\n",
    "        \"âœ… Short-term context\",\n",
    "        \"âŒ No in-session learning\",\n",
    "        \"âš ï¸  Black box\",\n",
    "        \"âš ï¸  Pay per use\",\n",
    "        \"âš ï¸  ~1-2 seconds\",\n",
    "        \"âš ï¸  Mostly reliable\"\n",
    "    ],\n",
    "    \"Advanced AI Agent (Next)\": [\n",
    "        \"âœ… Deep understanding\",\n",
    "        \"âœ… Extremely flexible\",\n",
    "        \"âœ… Long-term memory\",\n",
    "        \"âœ… Can adapt and improve\",\n",
    "        \"âœ… Shows reasoning\",\n",
    "        \"âŒ More expensive\",\n",
    "        \"âŒ Slower (3-10s)\",\n",
    "        \"âš ï¸  Complex behavior\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"Evolution of AI Agents:\")\n",
    "print(\"=\" * 100)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try these exercises to solidify your understanding:\n",
    "\n",
    "### Exercise 1: Prompt Engineering\n",
    "Create prompts to make the LLM:\n",
    "1. Extract key points from a paragraph\n",
    "2. Translate text to another language\n",
    "3. Generate test data in a specific format\n",
    "\n",
    "### Exercise 2: Build a Specialized Agent\n",
    "Modify the LLMAgent class to create:\n",
    "1. A customer service agent\n",
    "2. A coding assistant\n",
    "3. A creative writing helper\n",
    "\n",
    "### Exercise 3: Experiment with Parameters\n",
    "1. Test different temperature values for various tasks\n",
    "2. Compare response quality across different models\n",
    "3. Measure token usage for different prompt styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your practice code\n",
    "\n",
    "# Exercise 1 Example:\n",
    "def extract_key_points(text):\n",
    "    prompt = f\"\"\"\n",
    "Extract the 3 most important points from this text.\n",
    "Return them as a numbered list.\n",
    "\n",
    "Text: {text}\n",
    "\"\"\"\n",
    "    return ask_llm(prompt)\n",
    "\n",
    "# Try it out!\n",
    "sample_text = \"\"\"\n",
    "Large Language Models have revolutionized natural language processing. \n",
    "They can understand context, generate human-like text, and perform \n",
    "various tasks without specific programming. However, they require \n",
    "careful prompting and have limitations like hallucination and \n",
    "high computational costs.\n",
    "\"\"\"\n",
    "\n",
    "print(extract_key_points(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Core Agentic Components\n",
    "\n",
    "Now that you understand how to work with LLMs, let's explore the four foundational components that make AI agents truly powerful:\n",
    "\n",
    "1. **Planning** - Breaking down complex tasks into manageable steps\n",
    "2. **Reasoning** - Thinking through problems logically \n",
    "3. **Tools** - Using external capabilities beyond text generation\n",
    "4. **Memory** - Retaining and using information across interactions\n",
    "\n",
    "We've already seen Planning and Reasoning in our examples. Now let's explore Tools and Memory, which transform simple LLMs into capable agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1 Tools: Extending Agent Capabilities\n",
    "\n",
    "**Tools** allow AI agents to interact with the external world beyond just generating text. Instead of being limited to what they learned during training, agents can:\n",
    "\n",
    "- **Calculate** precise mathematical results\n",
    "- **Search** the web for current information  \n",
    "- **Query** databases for specific data\n",
    "- **Call APIs** to access external services\n",
    "- **Execute code** to perform computations\n",
    "- **Generate images** or process multimedia\n",
    "\n",
    "#### Why Tools Matter:\n",
    "\n",
    "1. **Accuracy**: LLMs can hallucinate facts or make calculation errors. Tools provide reliable, verifiable results.\n",
    "2. **Currency**: Training data has a cutoff date. Tools enable access to real-time information.\n",
    "3. **Capabilities**: Tools extend what agents can do beyond text processing.\n",
    "\n",
    "Let's explore how this works in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Examples You've Already Seen\n",
    "\n",
    "In the `agent_demo.ipynb` notebook, the SimpleAgent already demonstrated basic tool usage:\n",
    "- **Weather API**: Getting real-time weather data from external services\n",
    "- **Calculator**: Performing mathematical operations reliably  \n",
    "- **Time Functions**: Accessing system information\n",
    "- **File Operations**: Reading from CSV databases\n",
    "\n",
    "#### Advanced Tool Concepts for AI Agents\n",
    "\n",
    "Modern AI agents can use much more sophisticated tools:\n",
    "\n",
    "**1. Function Calling**: LLMs can be trained to generate structured function calls\n",
    "```json\n",
    "{\n",
    "  \"function\": \"get_weather\",\n",
    "  \"parameters\": {\"city\": \"San Francisco\", \"units\": \"celsius\"}\n",
    "}\n",
    "```\n",
    "\n",
    "**2. Tool Chaining**: Using output from one tool as input to another\n",
    "- Search web â†’ Extract URLs â†’ Summarize content â†’ Generate report\n",
    "\n",
    "**3. Conditional Tool Use**: Using reasoning to decide when tools are needed\n",
    "- \"If the user asks about current events, use web search\"\n",
    "- \"If the user asks for calculations, use calculator\"\n",
    "\n",
    "**4. Error Handling**: Gracefully handling tool failures\n",
    "- Retry with different parameters\n",
    "- Fall back to alternative tools\n",
    "- Inform user about limitations\n",
    "\n",
    "The key insight: **Tools transform LLMs from text generators into capable agents that can interact with the real world.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 Memory: Enabling Context and Learning\n",
    "\n",
    "**Memory** allows AI agents to retain and use information across interactions, making them more context-aware and capable of learning from experience.\n",
    "\n",
    "#### Types of Memory in AI Agents:\n",
    "\n",
    "**1. Short-term Memory (Working Memory)**\n",
    "- Conversation history within a single session\n",
    "- Recent context and user preferences\n",
    "- Temporary variables and state\n",
    "\n",
    "**2. Long-term Memory (Persistent Memory)** \n",
    "- Information that persists across sessions\n",
    "- User profiles and learned preferences\n",
    "- Knowledge base of facts and procedures\n",
    "- Historical interactions and outcomes\n",
    "\n",
    "**3. Episodic Memory**\n",
    "- Specific events and experiences\n",
    "- \"Remember when we discussed project X?\"\n",
    "- Context about past conversations\n",
    "\n",
    "**4. Semantic Memory**\n",
    "- General knowledge and concepts\n",
    "- Rules, procedures, and methodologies\n",
    "- Domain-specific expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating Memory Concepts with our LLM Agent\n",
    "class MemoryAwareLLMAgent:\n",
    "    \"\"\"Enhanced LLM agent that demonstrates different types of memory\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"anthropic/claude-3.5-haiku\"):\n",
    "        self.model = model\n",
    "        \n",
    "        # Short-term memory: conversation history\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Long-term memory: persistent information\n",
    "        self.user_preferences = {}\n",
    "        self.learned_facts = {}\n",
    "        self.interaction_count = 0\n",
    "        \n",
    "        # System prompt that includes memory capabilities\n",
    "        self.system_prompt = \"\"\"\n",
    "You are a helpful AI assistant with memory capabilities. You can:\n",
    "- Remember information from this conversation (short-term memory)\n",
    "- Learn and recall user preferences (long-term memory)\n",
    "- Reference past interactions when relevant\n",
    "\n",
    "When users share personal information, preferences, or important facts, \n",
    "respond with: \"I'll remember that [summarize what you learned]\"\n",
    "\n",
    "If you detect that the user is sharing information worth remembering, \n",
    "start your response with \"MEMORY_STORE:\" followed by key-value pairs in the format:\n",
    "MEMORY_STORE: preference_type=value, fact_name=fact_value\n",
    "\n",
    "Example: \"MEMORY_STORE: name=John, food_preference=vegetarian\"\n",
    "\"\"\"\n",
    "    \n",
    "    def extract_memory_from_response(self, response):\n",
    "        \"\"\"Extract memory instructions from LLM response\"\"\"\n",
    "        if \"MEMORY_STORE:\" in response:\n",
    "            memory_line = response.split(\"MEMORY_STORE:\")[1].split(\"\\n\")[0].strip()\n",
    "            # Parse key=value pairs\n",
    "            pairs = memory_line.split(\",\")\n",
    "            for pair in pairs:\n",
    "                if \"=\" in pair:\n",
    "                    key, value = pair.split(\"=\", 1)\n",
    "                    key = key.strip()\n",
    "                    value = value.strip()\n",
    "                    \n",
    "                    if key.startswith(\"preference_\"):\n",
    "                        self.user_preferences[key[11:]] = value  # Remove \"preference_\" prefix\n",
    "                        print(f\"ðŸ’¾ Stored preference: {key[11:]} = {value}\")\n",
    "                    else:\n",
    "                        self.learned_facts[key] = value\n",
    "                        print(f\"ðŸ’¾ Stored fact: {key} = {value}\")\n",
    "    \n",
    "    def build_memory_context(self):\n",
    "        \"\"\"Build memory context for the system prompt\"\"\"\n",
    "        memory_context = \"\"\n",
    "        if self.user_preferences:\n",
    "            memory_context += f\"Known user preferences: {self.user_preferences}\\n\"\n",
    "        if self.learned_facts:\n",
    "            memory_context += f\"Known facts about user: {self.learned_facts}\\n\"\n",
    "        return memory_context\n",
    "    \n",
    "    def process_with_memory(self, user_input):\n",
    "        \"\"\"Process input while leveraging memory\"\"\"\n",
    "        self.interaction_count += 1\n",
    "        \n",
    "        # Build memory context\n",
    "        memory_context = self.build_memory_context()\n",
    "        \n",
    "        # Create enhanced system prompt with memory\n",
    "        enhanced_system_prompt = self.system_prompt\n",
    "        if memory_context:\n",
    "            enhanced_system_prompt += f\"\\nCurrent memory about this user:\\n{memory_context}\"\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Prepare messages with memory context\n",
    "        messages = [{\"role\": \"system\", \"content\": enhanced_system_prompt}]\n",
    "        messages.extend(self.conversation_history[-6:])  # Last 6 messages for context\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            \n",
    "            assistant_response = response.choices[0].message.content\n",
    "            \n",
    "            # Let the LLM decide what to remember and extract it\n",
    "            self.extract_memory_from_response(assistant_response)\n",
    "            \n",
    "            # Clean up the response (remove MEMORY_STORE lines from user-facing response)\n",
    "            clean_response = assistant_response.split(\"MEMORY_STORE:\")[0].strip()\n",
    "            \n",
    "            # Add assistant response to history\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": clean_response\n",
    "            })\n",
    "            \n",
    "            return clean_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def show_memory_state(self):\n",
    "        \"\"\"Display current memory state\"\"\"\n",
    "        print(f\"\\nðŸ“Š Memory State - Interactions: {self.interaction_count}\")\n",
    "        print(f\"   Preferences: {self.user_preferences}\")\n",
    "        print(f\"   Facts: {self.learned_facts}\")\n",
    "\n",
    "# Demonstrate memory capabilities\n",
    "memory_agent = MemoryAwareLLMAgent()\n",
    "\n",
    "print(\"MEMORY-AWARE AGENT DEMONSTRATION:\")\n",
    "print(\"(The LLM decides what to remember, not keyword matching)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Conversation that demonstrates memory\n",
    "conversation_turns = [\n",
    "    \"Hi, my name is Sarah and I prefer vegetarian food\",\n",
    "    \"What do you remember about me?\", \n",
    "    \"Can you suggest a restaurant for dinner?\",\n",
    "    \"I also really love spicy food and I'm from Italy\",\n",
    "    \"What do you know about my food preferences now?\"\n",
    "]\n",
    "\n",
    "for turn in conversation_turns:\n",
    "    print(f\"\\nðŸ‘¤ User: {turn}\")\n",
    "    response = memory_agent.process_with_memory(turn)\n",
    "    print(f\"ðŸ¤– Agent: {response}\")\n",
    "    memory_agent.show_memory_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Insights from Memory Demonstration\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **LLM-Driven Memory**: Instead of keyword matching, we let the LLM decide what information is worth remembering\n",
    "2. **Structured Memory Storage**: Using a simple protocol (`MEMORY_STORE:`) to extract structured data from natural language responses  \n",
    "3. **Context Injection**: Feeding remembered information back into the system prompt for future conversations\n",
    "4. **Memory Types**: Demonstrating both facts (name, location) and preferences (food choices)\n",
    "\n",
    "#### Memory in Production Systems\n",
    "\n",
    "**Real-world memory systems use:**\n",
    "\n",
    "- **Vector Databases**: Store conversation embeddings for semantic search\n",
    "- **Knowledge Graphs**: Model relationships between entities (user â†’ likes â†’ spicy food)\n",
    "- **Retrieval-Augmented Generation (RAG)**: Search relevant memories before generating responses\n",
    "- **Memory Summarization**: Compress long conversation histories into key facts\n",
    "- **Privacy Controls**: Allow users to view/edit/delete their stored information\n",
    "\n",
    "#### Memory Challenges\n",
    "\n",
    "- **Context Window Limits**: LLMs can only process limited tokens at once\n",
    "- **Relevance Filtering**: Not all memories are relevant to current conversation\n",
    "- **Memory Conflicts**: Handling contradictory information over time\n",
    "- **Privacy & Security**: Protecting sensitive personal information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3 The Four Pillars Working Together\n",
    "\n",
    "The true power of AI agents emerges when all four components work in harmony:\n",
    "\n",
    "**Example: A Personal Assistant Agent**\n",
    "\n",
    "1. **Planning**: \"User wants to plan a vacation - I need to break this into steps: budget, dates, preferences, booking\"\n",
    "\n",
    "2. **Reasoning**: \"Based on their previous trips (memory) and stated budget constraints, I should recommend mid-range options in Europe rather than luxury resorts\"\n",
    "\n",
    "3. **Tools**: \"Let me search flight prices (web search tool), check weather forecasts (weather API), and calculate total costs (calculator tool)\"\n",
    "\n",
    "4. **Memory**: \"I remember they prefer cultural experiences over beaches, are vegetarian, and had a great time in Italy last year\"\n",
    "\n",
    "**The Result**: A capable agent that can handle complex, multi-step tasks while personalizing the experience based on learned preferences and real-time information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. From Simple to Advanced: What's Next?\n",
    "\n",
    "We've now covered the complete foundation of agentic AI systems. Here's how this connects to what's coming:\n",
    "\n",
    "### What We've Learned:\n",
    "1. **LLMs vs Rule-Based**: LLMs understand language naturally\n",
    "2. **API Usage**: How to call LLMs and handle responses\n",
    "3. **Key Concepts**: Tokens, temperature, prompting\n",
    "4. **Basic Agent**: Built a simple LLM-powered agent\n",
    "5. **Four Pillars of Agentic AI**: Planning, Reasoning, Tools, Memory\n",
    "\n",
    "### What's Coming Next:\n",
    "1. **Thinking Models**: Models that show their reasoning process (you've seen this in the DeepSeek notebook)\n",
    "2. **Advanced Prompting**: Chain of thought, ReAct, Tree of thoughts\n",
    "3. **Production Tools**: Real API integrations and tool frameworks\n",
    "4. **Advanced Memory**: Vector databases, RAG systems, knowledge graphs\n",
    "5. **Complex Workflows**: Agents handling multi-step business processes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_agents_venv",
   "language": "python",
   "name": "ai_agents_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
