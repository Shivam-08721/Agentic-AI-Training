{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating Agentic Capabilities with DeepSeek Thinking Models\n",
    "\n",
    "This notebook demonstrates how to use DeepSeek thinking models via OpenRouter to showcase agentic capabilities without using complex frameworks. We'll see how LLMs can:\n",
    "\n",
    "1. **Plan** - Create a structured approach to solve problems\n",
    "2. **Reason** - Think through intermediate steps logically\n",
    "3. **Reflect** - Evaluate their own work and identify issues\n",
    "4. **Revise** - Make changes based on reflection\n",
    "5. **Produce** - Generate final outputs\n",
    "\n",
    "## What are Thinking Models?\n",
    "\n",
    "Traditional LLMs generate responses directly, giving you only the final answer. **Thinking models** (also called reasoning models) show their \"thought process\" - the internal reasoning steps they take to arrive at an answer. This transparency helps us:\n",
    "\n",
    "- Understand how the model solved a problem\n",
    "- Verify the logic and catch potential errors\n",
    "- Learn from the model's reasoning approach\n",
    "- Build more reliable AI systems\n",
    "\n",
    "## Why This Matters for AI Agents\n",
    "\n",
    "AI agents need to be more than just question-answering systems. They need to:\n",
    "- Break down complex problems into steps\n",
    "- Plan their approach before acting\n",
    "- Reflect on their work and self-correct\n",
    "- Show their reasoning for accountability\n",
    "\n",
    "DeepSeek's thinking models demonstrate these capabilities naturally, making them excellent for learning about agentic AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment by loading the OpenRouter API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "# Initialize OpenAI client with OpenRouter configuration\n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "# Check if credentials are loaded\n",
    "print(f\"API Base URL: {os.getenv('OPENROUTER_BASE_URL')}\")\n",
    "print(f\"API Key: {'Loaded successfully' if os.getenv('OPENROUTER_API_KEY') else 'Not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available DeepSeek Models\n",
    "\n",
    "DeepSeek offers several models with thinking/reasoning capabilities through OpenRouter:\n",
    "\n",
    "### Model Comparison:\n",
    "\n",
    "| Model | Parameters | Best For | Cost | Key Features |\n",
    "|-------|------------|----------|------|--------------|\n",
    "| **DeepSeek R1** | 671B (37B active) | Complex reasoning, production use | \\$\\$\\$ | Full reasoning capabilities, highest quality |\n",
    "| **DeepSeek R1 Distill Qwen 32B** | 32B | Balanced performance/cost | \\$\\$ | Good reasoning, faster than R1 |\n",
    "| **DeepSeek R1 Distill Llama 70B** | 70B | High quality, open-source compatible | \\$\\$ | Strong reasoning, Llama architecture |\n",
    "| **DeepSeek Chat** | Varies | General conversation | \\$ | No reasoning visibility, fast |\n",
    "\n",
    "For this notebook, we'll use the paid DeepSeek R1 for best results, but you can experiment with others.\n",
    "\n",
    "Let's set up our helper functions to work with these models and correctly access their reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_deepseek_thinking_model(prompt, model=\"deepseek/deepseek-r1\"):\n",
    "    \"\"\"Call a DeepSeek thinking-enabled model with the OpenRouter API.\n",
    "    \n",
    "    This function accesses the reasoning capabilities of DeepSeek models\n",
    "    through OpenRouter by using the include_reasoning parameter.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Using the include_reasoning parameter to access the model's thinking process\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            extra_body={\n",
    "                \"include_reasoning\": True  # This is the key parameter to get reasoning\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Extract the reasoning (thinking) process\n",
    "        thinking = \"No explicit thinking process detected\"\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Check if there's an extra field containing reasoning\n",
    "        if hasattr(response, 'choices') and hasattr(response.choices[0].message, 'reasoning'):\n",
    "            thinking = response.choices[0].message.reasoning\n",
    "        # Fall back to the older format for reasoning_content\n",
    "        elif hasattr(response, 'choices') and hasattr(response.choices[0].message, 'reasoning_content'):\n",
    "            thinking = response.choices[0].message.reasoning_content\n",
    "        # If reasoning is not in response fields, try to extract from the <think> tags in the content\n",
    "        elif \"<think>\" in answer and \"</think>\" in answer:\n",
    "            thinking_start = answer.find(\"<think>\") + 7\n",
    "            thinking_end = answer.find(\"</think>\")\n",
    "            thinking = answer[thinking_start:thinking_end].strip()\n",
    "            # Get the final answer (after </think>)\n",
    "            answer = answer[thinking_end + 8:].strip()  # +8 to skip the closing tag\n",
    "            \n",
    "        return {\n",
    "            \"thinking\": thinking,\n",
    "            \"answer\": answer,\n",
    "            \"raw_response\": response\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def call_regular_model(prompt, model=\"deepseek/deepseek-chat\"):\n",
    "    \"\"\"Call a regular model without thinking capabilities.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def display_thinking_response(result):\n",
    "    \"\"\"Pretty print the thinking and response from a model.\"\"\"\n",
    "    if \"error\" in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üß† THINKING PROCESS:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result[\"thinking\"] if result[\"thinking\"] else \"No explicit thinking process detected\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚ú® FINAL RESPONSE:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result[\"answer\"])\n",
    "    \n",
    "    # Optional: Print the raw response for debugging\n",
    "    if \"debug\" in globals() and debug:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üîç RAW RESPONSE:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(result.get(\"raw_response\", \"No raw response available\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on OpenRouter Parameters\n",
    "\n",
    "OpenRouter supports different parameter formats for accessing reasoning capabilities:\n",
    "- `include_reasoning: True` - The format we'll use in this notebook\n",
    "- `reasoning: {}` - An alternative unified parameter format\n",
    "\n",
    "Both work similarly. We'll use `include_reasoning` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to see raw responses (helpful for debugging)\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Planning and Reasoning\n",
    "\n",
    "Let's start with a simple example that demonstrates basic planning and reasoning - solving a word problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_prompt = \"\"\"\n",
    "A farmer has 17 sheep, all but 9 die. How many sheep are left? \n",
    "\"\"\"\n",
    "\n",
    "# First let's see how a regular model responds\n",
    "regular_result = call_regular_model(simple_prompt)\n",
    "print(\"REGULAR MODEL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(regular_result[\"answer\"])\n",
    "\n",
    "# Now let's see the thinking model's response\n",
    "thinking_result = call_deepseek_thinking_model(simple_prompt)\n",
    "display_thinking_response(thinking_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multi-Step Reasoning with Reflection\n",
    "\n",
    "Now let's try a more complex problem that requires multi-step reasoning and reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_prompt = \"\"\"\n",
    "Alice, Bob, and Charlie are playing a game with marbles. \n",
    "- Alice starts with twice as many marbles as Bob. \n",
    "- Bob starts with 3 times as many marbles as Charlie. \n",
    "- Charlie starts with 5 marbles.\n",
    "- After the first round, Alice loses half her marbles to Bob.\n",
    "- After the second round, Bob gives 1/3 of his marbles to Charlie.\n",
    "\n",
    "How many marbles does each person have at the end? Show your reasoning step by step.\n",
    "\"\"\"\n",
    "\n",
    "thinking_result = call_deepseek_thinking_model(complex_prompt)\n",
    "display_thinking_response(thinking_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Creativity with Planning and Self-Critique\n",
    "\n",
    "Let's now see how the model handles a creative task that requires planning and then critiquing its own work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_prompt = \"\"\"\n",
    "Task: Create a short story (about 150 words) about an astronaut discovering something unexpected on Mars.\n",
    "\n",
    "Please follow these steps:\n",
    "1. Plan the key elements of the story (characters, setting, conflict, resolution)\n",
    "2. Write a first draft\n",
    "3. Review and critique your draft, identifying at least 2 aspects to improve\n",
    "4. Create a revised, final version of the story based on your critique\n",
    "\n",
    "Make sure the final story is engaging and contains an interesting twist.\n",
    "\"\"\"\n",
    "\n",
    "thinking_result = call_deepseek_thinking_model(creative_prompt)\n",
    "display_thinking_response(thinking_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Emulating an Agent for Problem Solving\n",
    "\n",
    "Now, let's emulate a more complex agent-like behavior where the model needs to break down a problem, research information (simulated), and iteratively improve its solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = \"\"\"\n",
    "You are an AI agent tasked with planning a 3-day itinerary for a family visiting San Francisco. \n",
    "\n",
    "The family has the following requirements:\n",
    "- They have two children (ages 8 and 12)\n",
    "- They want a mix of tourist attractions and local experiences\n",
    "- Their budget is moderate (not luxury, but not extremely budget-conscious)\n",
    "- They will be staying near Union Square\n",
    "- One family member has mild mobility issues and can't walk long distances\n",
    "\n",
    "Please act as an agent that:\n",
    "1. First identifies the key constraints and goals of this task\n",
    "2. Brainstorms potentially suitable attractions and activities\n",
    "3. Researches and evaluates options (you can simulate this research process)\n",
    "4. Organizes them into a coherent 3-day itinerary\n",
    "5. Reflects on the itinerary, identifies potential problems or improvements\n",
    "6. Revises the itinerary based on your reflection\n",
    "7. Presents the final itinerary in a user-friendly format\n",
    "\n",
    "For each step in your process, explain your reasoning and how it contributes to the final solution.\n",
    "\"\"\"\n",
    "\n",
    "thinking_result = call_deepseek_thinking_model(agent_prompt)\n",
    "display_thinking_response(thinking_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Multi-Agent Simulation\n",
    "\n",
    "Finally, let's demonstrate a more complex scenario where the model simulates multiple agents with different roles working together to solve a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agent_prompt = \"\"\"\n",
    "Simulate a dialogue between three specialized AI agents collaborating to design a sustainable smart home system:\n",
    "\n",
    "1. EngineeringAgent: Expert in technical feasibility, energy systems, and IoT infrastructure\n",
    "2. SustainabilityAgent: Expert in environmental impact, resource efficiency, and sustainable materials\n",
    "3. UserExperienceAgent: Expert in human-centered design, accessibility, and usability\n",
    "\n",
    "Their goal is to design a comprehensive smart home system that is technically sound, environmentally sustainable, and user-friendly.\n",
    "\n",
    "The dialogue should include:\n",
    "- Each agent introducing their initial ideas from their area of expertise\n",
    "- Agents identifying conflicts between their individual priorities\n",
    "- Collaborative problem-solving to resolve these conflicts\n",
    "- Iterations of the design based on feedback from other agents\n",
    "- A final consensus design that balances all three perspectives\n",
    "\n",
    "For each turn in the dialogue, each agent should explicitly reference their specialized knowledge and reasoning process.\n",
    "\"\"\"\n",
    "\n",
    "thinking_result = call_deepseek_thinking_model(multi_agent_prompt)\n",
    "display_thinking_response(thinking_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with Different DeepSeek Distilled Models\n",
    "\n",
    "DeepSeek provides several distilled models that offer a good balance between performance and cost. Let's try a couple of them on a more complex reasoning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A problem involving logic and reasoning\n",
    "logic_puzzle = \"\"\"\n",
    "Five friends (Alex, Brook, Charlie, Dana, and Evan) are sitting in a row of five chairs, \n",
    "numbered 1 to 5 from left to right. We know that:\n",
    "- Alex is sitting in chair 1 or chair 5.\n",
    "- Brook is sitting in an even-numbered chair.\n",
    "- Charlie and Dana are sitting next to each other.\n",
    "- Evan is not sitting next to Alex or Brook.\n",
    "\n",
    "Determine who is sitting in each chair.\n",
    "\"\"\"\n",
    "\n",
    "# Try with a distilled model if available\n",
    "try:\n",
    "    distilled_model_result = call_deepseek_thinking_model(logic_puzzle, model=\"deepseek/deepseek-r1-distill-qwen-14b\")\n",
    "    print(\"\\nUSING DEEPSEEK R1 DISTILL QWEN 14B MODEL:\\n\")\n",
    "    display_thinking_response(distilled_model_result)\n",
    "except Exception as e:\n",
    "    print(f\"Error with distilled model: {e}\")\n",
    "    print(\"Falling back to free model...\")\n",
    "    fallback_result = call_deepseek_thinking_model(logic_puzzle)\n",
    "    display_thinking_response(fallback_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Comparison of Models\n",
    "\n",
    "Let's compare different DeepSeek models on the same challenging math problem to see how their thinking styles differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A challenging algebra problem\n",
    "math_problem = \"\"\"\n",
    "If the sum of two numbers is 15 and their product is 56, \n",
    "what is the sum of their cubes?\n",
    "\"\"\"\n",
    "\n",
    "# Regular model (no thinking)\n",
    "regular_result = call_regular_model(math_problem)\n",
    "print(\"REGULAR MODEL (NO THINKING):\\n\")\n",
    "print(regular_result[\"answer\"])\n",
    "\n",
    "# DeepSeek R1 (paid model with best reasoning)\n",
    "deepseek_r1_result = call_deepseek_thinking_model(math_problem, model=\"deepseek/deepseek-r1\")\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"DEEPSEEK R1 MODEL (WITH THINKING):\")\n",
    "print(\"=\" * 100)\n",
    "display_thinking_response(deepseek_r1_result)\n",
    "\n",
    "# Optional: Try a distilled model for comparison\n",
    "try:\n",
    "    distilled_result = call_deepseek_thinking_model(math_problem, model=\"deepseek/deepseek-r1-distill-qwen-32b\")\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"DEEPSEEK R1 DISTILL (32B) MODEL:\")\n",
    "    print(\"=\" * 100)\n",
    "    display_thinking_response(distilled_result)\n",
    "except Exception as e:\n",
    "    print(f\"\\nSkipping distilled model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These examples demonstrate how DeepSeek's thinking models can exhibit agentic capabilities - planning, reasoning, reflecting, revising, and producing outputs - all within a single prompt. \n",
    "\n",
    "Key observations:\n",
    "\n",
    "1. **Transparency**: OpenRouter allows us to access the model's reasoning process, giving us insight into how it arrived at its conclusions.\n",
    "\n",
    "2. **Self-correction**: The thinking models can identify mistakes and make corrections during their reasoning process.\n",
    "\n",
    "3. **Structured thinking**: The models naturally break down complex problems into manageable steps.\n",
    "\n",
    "4. **Adaptability**: These models can apply reasoning capabilities to various tasks, from mathematical problems to creative writing to complex planning.\n",
    "\n",
    "5. **Cost-effectiveness**: By using DeepSeek's models, particularly the free tier or the distilled versions, we can demonstrate agentic capabilities at a fraction of the cost of the largest frontier models.\n",
    "\n",
    "This approach offers a simple yet powerful way to introduce the concept of AI agents without the complexity of frameworks, focusing instead on the fundamental reasoning capabilities that make agentic behavior possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: Planning & Reasoning Approaches Comparison\n",
    "\n",
    "This extension demonstrates three different planning and reasoning approaches using DeepSeek models, showing how different prompting techniques can be used to guide model thought processes in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper function to display thinking process and answer\n",
    "def display_comparison_result(title, result):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üîç {title}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        return\n",
    "        \n",
    "    # Display thinking process\n",
    "    print(\"\\nüìù THINKING PROCESS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result[\"thinking\"] if result[\"thinking\"] else \"No explicit thinking process detected\")\n",
    "    \n",
    "    # Display answer\n",
    "    print(\"\\nüéØ ANSWER:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result[\"answer\"])\n",
    "    \n",
    "    # Display metrics if available\n",
    "    if \"metrics\" in result:\n",
    "        print(\"\\nüìä METRICS:\")\n",
    "        print(\"-\" * 40)\n",
    "        for key, value in result[\"metrics\"].items():\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_cot_model(problem, model=\"deepseek/deepseek-r1:free\"):\n",
    "    \"\"\"Call model with Chain of Thought prompting.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    {problem}\n",
    "    \n",
    "    Let's solve this step-by-step to ensure we get the correct answer.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            extra_body={\n",
    "                \"include_reasoning\": True\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Extract thinking and response\n",
    "        thinking = response.choices[0].message.reasoning if hasattr(response.choices[0].message, \"reasoning\") else \"\"\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"response_time\": round(end_time - start_time, 2),\n",
    "            \"thinking_length\": len(thinking) if thinking else 0,\n",
    "            \"answer_length\": len(answer)\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"thinking\": thinking,\n",
    "            \"answer\": answer,\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_react_model(problem, model=\"deepseek/deepseek-r1:free\"):\n",
    "    \"\"\"Call model with ReAct (Reasoning + Acting) prompting.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    {problem}\n",
    "    \n",
    "    Let's solve this by following the ReAct approach (Reasoning + Acting):\n",
    "    \n",
    "    Step 1: Think about what information we need\n",
    "    Step 2: Define the actions we need to take\n",
    "    Step 3: Take each action, one at a time\n",
    "    Step 4: Observe the results\n",
    "    Step 5: Reason about the next step based on observations\n",
    "    \n",
    "    When solving, clearly indicate each Thought, Action, and Observation step.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            extra_body={\n",
    "                \"include_reasoning\": True\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Extract thinking and response\n",
    "        thinking = response.choices[0].message.reasoning if hasattr(response.choices[0].message, \"reasoning\") else \"\"\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"response_time\": round(end_time - start_time, 2),\n",
    "            \"thinking_length\": len(thinking) if thinking else 0,\n",
    "            \"answer_length\": len(answer),\n",
    "            \"action_count\": answer.lower().count(\"action:\") if answer else 0\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"thinking\": thinking,\n",
    "            \"answer\": answer,\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_tot_model(problem, model=\"deepseek/deepseek-r1\"):\n",
    "    \"\"\"Call model with Tree of Thoughts prompting.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    IMPORTANT: Focus only on solving the following dinner party planning problem. Do not generate any other content.\n",
    "    \n",
    "    {problem}\n",
    "    \n",
    "    Solve this using the Tree of Thoughts approach:\n",
    "    \n",
    "    1. First, generate 3 different initial approaches to solve this problem\n",
    "    2. For each approach, evaluate its potential success (High/Medium/Low)\n",
    "    3. Choose the most promising approach and develop it further\n",
    "    4. If you encounter difficulties, backtrack and try another approach\n",
    "    5. Continue until you find the solution\n",
    "    \n",
    "    Present your thought process as a tree structure with different branches of reasoning.\n",
    "    Focus entirely on the dinner party planning constraints and provide a practical solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            extra_body={\n",
    "                \"include_reasoning\": True\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Extract thinking and response\n",
    "        thinking = \"\"\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract reasoning from different possible locations\n",
    "        if hasattr(response.choices[0].message, 'reasoning'):\n",
    "            thinking = response.choices[0].message.reasoning\n",
    "        elif hasattr(response.choices[0].message, 'reasoning_content'):\n",
    "            thinking = response.choices[0].message.reasoning_content\n",
    "        elif \"<think>\" in answer and \"</think>\" in answer:\n",
    "            thinking_start = answer.find(\"<think>\") + 7\n",
    "            thinking_end = answer.find(\"</think>\")\n",
    "            thinking = answer[thinking_start:thinking_end].strip()\n",
    "            answer = answer[thinking_end + 8:].strip()\n",
    "        \n",
    "        # Filter out garbage responses (like changelogs)\n",
    "        garbage_indicators = [\"ÂàùÂßãÂåñÈ°πÁõÆ\", \"github.com\", \"commit\", \"c0a0a0a\", \"1.0.0 (2023\", \"### Features\"]\n",
    "        \n",
    "        if any(indicator in thinking for indicator in garbage_indicators):\n",
    "            thinking = \"The model generated irrelevant content instead of solving the problem. This appears to be a model issue.\"\n",
    "        \n",
    "        if any(indicator in answer for indicator in garbage_indicators):\n",
    "            answer = \"The model generated irrelevant output (appears to be a changelog) instead of solving the dinner party problem. This may be due to model confusion. Please try running the cell again or switch to a different model.\"\n",
    "        \n",
    "        # Ensure we have actual content\n",
    "        if len(answer.strip()) == 0:\n",
    "            answer = \"The model did not generate a proper response. Please try again.\"\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"response_time\": round(end_time - start_time, 2),\n",
    "            \"thinking_length\": len(thinking) if thinking else 0,\n",
    "            \"answer_length\": len(answer),\n",
    "            \"approaches_count\": answer.lower().count(\"approach\") if answer else 0\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"thinking\": thinking if thinking else \"No explicit thinking process detected\",\n",
    "            \"answer\": answer,\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex problem to test different planning approaches\n",
    "planning_problem = \"\"\"\n",
    "You are planning a dinner party for 8 people with the following constraints:\n",
    "- Two guests are vegetarian\n",
    "- One guest has a severe nut allergy\n",
    "- One guest doesn't eat gluten\n",
    "- You have a budget of $120 for all food and drinks\n",
    "- You need to prepare an appetizer, main course, and dessert\n",
    "- You have only 4 hours to prepare everything\n",
    "- Your kitchen is small with only 4 burners and one oven\n",
    "\n",
    "Create a detailed plan for the dinner party that satisfies all constraints.\n",
    "\"\"\"\n",
    "\n",
    "# Test each approach\n",
    "print(\"Testing different planning and reasoning approaches on the same problem...\\n\")\n",
    "\n",
    "# Chain of Thought\n",
    "cot_result = call_cot_model(planning_problem)\n",
    "display_comparison_result(\"CHAIN OF THOUGHT APPROACH\", cot_result)\n",
    "\n",
    "# ReAct\n",
    "react_result = call_react_model(planning_problem)\n",
    "display_comparison_result(\"REACT (REASONING + ACTING) APPROACH\", react_result)\n",
    "\n",
    "# Tree of Thoughts\n",
    "tot_result = call_tot_model(planning_problem)\n",
    "display_comparison_result(\"TREE OF THOUGHTS APPROACH\", tot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative metrics visualization\n",
    "def compare_approaches():\n",
    "    approaches = [\"Chain of Thought\", \"ReAct\", \"Tree of Thoughts\"]\n",
    "    results = [cot_result, react_result, tot_result]\n",
    "    \n",
    "    # Filter out results with errors\n",
    "    valid_approaches = []\n",
    "    valid_results = []\n",
    "    for approach, result in zip(approaches, results):\n",
    "        if \"error\" not in result:\n",
    "            valid_approaches.append(approach)\n",
    "            valid_results.append(result)\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(\"No valid results to compare\")\n",
    "        return\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    metrics = {}\n",
    "    for metric in valid_results[0][\"metrics\"].keys():\n",
    "        metrics[metric] = [result[\"metrics\"].get(metric, 0) for result in valid_results]\n",
    "    \n",
    "    df = pd.DataFrame(metrics, index=valid_approaches)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, len(df.columns), figsize=(15, 5))\n",
    "    \n",
    "    for i, column in enumerate(df.columns):\n",
    "        df[column].plot(kind='bar', ax=axes[i], title=column)\n",
    "        axes[i].set_ylabel(column)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display comparison table\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df)\n",
    "    \n",
    "    # Qualitative analysis\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üîç QUALITATIVE COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\"\"\n",
    "    Chain of Thought (CoT):\n",
    "    - Strengths: Linear reasoning, direct path to solution\n",
    "    - Weaknesses: May get stuck in a single approach\n",
    "    - Best for: Well-defined problems with clear solution paths\n",
    "    \n",
    "    ReAct (Reasoning + Acting):\n",
    "    - Strengths: Combines reasoning with simulated actions\n",
    "    - Weaknesses: May require more computation time\n",
    "    - Best for: Problems requiring information gathering and step-by-step verification\n",
    "    \n",
    "    Tree of Thoughts (ToT):\n",
    "    - Strengths: Explores multiple solution pathways, can backtrack if needed\n",
    "    - Weaknesses: More complex, potentially redundant exploration\n",
    "    - Best for: Complex problems with multiple valid approaches or potential dead ends\n",
    "    \"\"\")\n",
    "\n",
    "# Run the comparison\n",
    "compare_approaches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_custom_problem():\n",
    "    \"\"\"Let user try different approaches with a custom problem\"\"\"\n",
    "    \n",
    "    custom_problem = input(\"Enter your own problem to test different planning approaches:\\n\")\n",
    "    \n",
    "    approach = input(\"\\nSelect approach (1=CoT, 2=ReAct, 3=ToT, 4=All): \")\n",
    "    \n",
    "    if approach == \"1\" or approach == \"4\":\n",
    "        cot_custom = call_cot_model(custom_problem)\n",
    "        display_comparison_result(\"CHAIN OF THOUGHT - CUSTOM PROBLEM\", cot_custom)\n",
    "    \n",
    "    if approach == \"2\" or approach == \"4\":\n",
    "        react_custom = call_react_model(custom_problem)\n",
    "        display_comparison_result(\"REACT - CUSTOM PROBLEM\", react_custom)\n",
    "    \n",
    "    if approach == \"3\" or approach == \"4\":\n",
    "        tot_custom = call_tot_model(custom_problem)\n",
    "        display_comparison_result(\"TREE OF THOUGHTS - CUSTOM PROBLEM\", tot_custom)\n",
    "\n",
    "# Uncomment to run the interactive test\n",
    "#test_custom_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways: Planning & Reasoning Approaches\n",
    "\n",
    "This extension demonstrated three different planning and reasoning approaches using DeepSeek models:\n",
    "\n",
    "1. **Chain of Thought (CoT)**\n",
    "   - Linear step-by-step reasoning\n",
    "   - Provides clear explanations for each step\n",
    "   - Works well for straightforward problems\n",
    "\n",
    "2. **ReAct (Reasoning + Acting)**\n",
    "   - Combines reasoning with simulated actions\n",
    "   - Structures thinking into Thought-Action-Observation cycles\n",
    "   - Effective for problems requiring information gathering or verification\n",
    "\n",
    "3. **Tree of Thoughts (ToT)**\n",
    "   - Explores multiple solution paths simultaneously\n",
    "   - Evaluates different approaches and selects the most promising\n",
    "   - Can backtrack from unproductive paths\n",
    "   - Useful for complex problems with multiple viable approaches\n",
    "\n",
    "## When to Use Each Approach:\n",
    "\n",
    "- **Use CoT** when you need a straightforward solution path with clear explanations\n",
    "- **Use ReAct** when your task involves tools, information gathering, or environmental interaction\n",
    "- **Use ToT** when facing complex problems with multiple potential approaches or dead ends\n",
    "\n",
    "## Choosing the Right DeepSeek Model:\n",
    "\n",
    "- **DeepSeek R1 (deepseek/deepseek-r1)**: Best for complex reasoning tasks where quality matters most\n",
    "- **DeepSeek R1 Distill Models**: Good balance of performance and cost for production use\n",
    "- **DeepSeek Chat**: Fast responses for simpler tasks without reasoning visibility\n",
    "\n",
    "These approaches can be combined or customized to create hybrid strategies for different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how DeepSeek's thinking models provide a window into AI reasoning processes, showcasing fundamental agentic capabilities without complex frameworks.\n",
    "\n",
    "### Key Learning Points:\n",
    "\n",
    "1. **Transparency in AI Reasoning**: \n",
    "   - Unlike traditional LLMs that only show final answers, thinking models reveal the step-by-step reasoning process\n",
    "   - This transparency helps us understand, verify, and learn from AI decision-making\n",
    "\n",
    "2. **Core Agentic Capabilities**:\n",
    "   - **Planning**: Breaking down complex problems into manageable steps\n",
    "   - **Reasoning**: Logical thinking through intermediate steps\n",
    "   - **Reflection**: Self-evaluation and error detection\n",
    "   - **Revision**: Adapting approaches based on reflection\n",
    "   - **Production**: Generating final, refined outputs\n",
    "\n",
    "3. **Practical Applications**:\n",
    "   - **Problem Solving**: Mathematical calculations, logic puzzles\n",
    "   - **Creative Tasks**: Story writing with iterative improvement\n",
    "   - **Planning Tasks**: Multi-constraint optimization (dinner party example)\n",
    "   - **Multi-Agent Simulation**: Complex collaborative scenarios\n",
    "\n",
    "4. **Different Reasoning Approaches**:\n",
    "   - **Chain of Thought**: Linear, step-by-step reasoning\n",
    "   - **ReAct**: Combining reasoning with simulated actions\n",
    "   - **Tree of Thoughts**: Exploring multiple solution paths\n",
    "\n",
    "5. **Cost-Effective AI Development**:\n",
    "   - DeepSeek models offer powerful reasoning capabilities at a fraction of the cost of frontier models\n",
    "   - The paid models (like deepseek/deepseek-r1) provide reliable performance for educational and production use\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Experiment with Different Problems**: Try the models on your own complex reasoning tasks\n",
    "2. **Compare Approaches**: Test which reasoning approach works best for different problem types\n",
    "3. **Build Applications**: Use these reasoning capabilities as building blocks for more complex AI systems\n",
    "4. **Explore Frameworks**: Once comfortable with basic reasoning, explore agent frameworks like LangChain or AutoGen\n",
    "\n",
    "This foundation in understanding AI reasoning will help you build more sophisticated, reliable, and transparent AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:agentic_course] *",
   "language": "python",
   "name": "conda-env-agentic_course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
