{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Management\n",
    "\n",
    "This notebook explores token management techniques for working with Large Language Models (LLMs). Topics include:\n",
    "\n",
    "- Understanding and counting tokens\n",
    "- Context window optimization\n",
    "- Chunking strategies for long inputs\n",
    "- Token usage estimation and cost calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from IPython.display import display, HTML, clear_output, Markdown\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import our custom utilities\n",
    "sys.path.append('.')\n",
    "from api_utils import (\n",
    "    call_openrouter,\n",
    "    extract_text_response,\n",
    "    estimate_cost\n",
    ")\n",
    "from token_counter import (\n",
    "    count_tokens,\n",
    "    count_message_tokens,\n",
    "    chunk_text,\n",
    "    get_context_window,\n",
    "    truncate_to_token_limit,\n",
    "    optimize_context,\n",
    "    optimize_messages_for_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Tokens\n",
    "\n",
    "Tokens are the basic units that LLMs process. A token is not the same as a word or character. Instead, it's a piece of text determined by the model's tokenizer. Understanding how text is tokenized is essential for efficiently using LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is a Token?\n",
    "\n",
    "Let's explore how different text gets tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tokenization(text, model=\"gpt-4o\"):\n",
    "    \"\"\"Show how a piece of text is tokenized.\"\"\"\n",
    "    # Determine the encoding based on the model\n",
    "    if model.startswith(\"gpt-4\"):\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    elif model.startswith(\"gpt-3.5\"):\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    else:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")  # Default to cl100k_base\n",
    "    \n",
    "    # Encode the text\n",
    "    tokens = encoding.encode(text)\n",
    "    \n",
    "    # Decode each token to show the breakdown\n",
    "    token_texts = [encoding.decode([token]) for token in tokens]\n",
    "    \n",
    "    # Create a DataFrame to display the tokens\n",
    "    df = pd.DataFrame({\n",
    "        'Token ID': tokens,\n",
    "        'Token Text': token_texts,\n",
    "        'Bytes': [len(t.encode('utf-8')) for t in token_texts],\n",
    "        'Chars': [len(t) for t in token_texts]\n",
    "    })\n",
    "    \n",
    "    display(Markdown(f\"**Total tokens:** {len(tokens)}\"))\n",
    "    return df\n",
    "\n",
    "# Test with a simple example\n",
    "simple_text = \"Hello, world! This is a test of tokenization.\"\n",
    "show_tokenization(simple_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with some more complex examples to see how different types of text get tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some examples from our test cases file\n",
    "with open('token_test_cases.txt', 'r', encoding='utf-8', errors='replace') as f:\n",
    "    token_test_cases = f.read()\n",
    "\n",
    "# Split the test cases by the markdown headers\n",
    "import re\n",
    "test_cases = re.split(r'# .*?\\n', token_test_cases)[1:]\n",
    "test_case_names = re.findall(r'# (.*?)\\n', token_test_cases)\n",
    "\n",
    "# Create a dictionary of test cases\n",
    "test_case_dict = {name.strip(): case.strip() for name, case in zip(test_case_names, test_cases)}\n",
    "\n",
    "# Display the test cases we have\n",
    "content = \"## Available test cases:\\n\\n\"\n",
    "for name in test_case_dict.keys():\n",
    "    content += f\"- {name}\\n\"\n",
    "\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different types of content\n",
    "test_examples = [\n",
    "    (\"Short Text\", test_case_dict[\"Short Text\"]),\n",
    "    (\"Code Sample (first 100 chars)\", test_case_dict[\"Code Sample\"][:100]),\n",
    "    (\"Mathematical Text (first 100 chars)\", test_case_dict[\"Mathematical Text\"][:100]),\n",
    "    (\"Multilingual Text (first 100 chars)\", test_case_dict[\"Multilingual Text\"][:100])\n",
    "]\n",
    "\n",
    "for name, text in test_examples:\n",
    "    content = f\"## {name}:\\n\\n{text}\\n\\n\"\n",
    "    display(Markdown(content))\n",
    "    display(show_tokenization(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization Rules\n",
    "\n",
    "Based on the examples above, we can observe several patterns in how text is tokenized:\n",
    "\n",
    "1. **Common Words**: Frequent words like \"the\", \"and\", \"is\" often get their own tokens\n",
    "2. **Whitespace**: Spaces and newlines are part of the tokens that follow them\n",
    "3. **Capitalization**: \"Hello\" and \"hello\" may be tokenized differently\n",
    "4. **Punctuation**: Punctuation marks often get their own tokens or are grouped with adjacent characters\n",
    "5. **Numbers**: Numbers are often broken down digit by digit\n",
    "6. **Special Characters**: Special characters and symbols may get their own tokens\n",
    "\n",
    "Let's compare token counts for various text patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of test phrases to see tokenization patterns\n",
    "test_phrases = [\n",
    "    \"Hello\",\n",
    "    \"hello\",  # case difference\n",
    "    \"hello world\",\n",
    "    \"helloworld\",  # spacing difference\n",
    "    \"Hello, world!\",  # punctuation\n",
    "    \"1234567890\",  # numbers\n",
    "    \"ChatGPT\",  # mixed case\n",
    "    \"chat gpt\",  # space vs. no space\n",
    "    \"deeplearning\",\n",
    "    \"deep learning\",\n",
    "    \"‚≠êüòäüî•\",  # emojis\n",
    "    \"https://example.com\",  # URL\n",
    "    \"const x = 10;\",  # code\n",
    "    \"e=mc¬≤\",  # special characters\n",
    "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\",  # non-latin script (Hindi)\n",
    "    \"„Åì„Çì„Å´„Å°„ÅØ\"  # non-latin script (Japanese)\n",
    "]\n",
    "\n",
    "# Count tokens for each phrase\n",
    "results = []\n",
    "for phrase in test_phrases:\n",
    "    tokens = count_tokens(phrase)\n",
    "    results.append({\n",
    "        \"phrase\": phrase,\n",
    "        \"characters\": len(phrase),\n",
    "        \"tokens\": tokens,\n",
    "        \"chars_per_token\": round(len(phrase) / tokens, 2) if tokens > 0 else 0\n",
    "    })\n",
    "\n",
    "# Display the results\n",
    "df = pd.DataFrame(results)\n",
    "display(df)\n",
    "\n",
    "# Visualize the relationship between characters and tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['characters'], df['tokens'])\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Tokens')\n",
    "plt.title('Characters vs. Tokens')\n",
    "for i, row in df.iterrows():\n",
    "    plt.annotate(row['phrase'], (row['characters'], row['tokens']), textcoords=\"offset points\", \n",
    "                 xytext=(0, 10), ha='center')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Token Counts for Different Content Types\n",
    "\n",
    "Now let's analyze token counts for longer pieces of content from our test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens for each test case\n",
    "token_counts = {}\n",
    "char_counts = {}\n",
    "\n",
    "for name, text in test_case_dict.items():\n",
    "    tokens = count_tokens(text)\n",
    "    chars = len(text)\n",
    "    token_counts[name] = tokens\n",
    "    char_counts[name] = chars\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "token_df = pd.DataFrame({\n",
    "    'Test Case': list(token_counts.keys()),\n",
    "    'Tokens': list(token_counts.values()),\n",
    "    'Characters': list(char_counts.values())\n",
    "})\n",
    "\n",
    "token_df['Chars per Token'] = token_df['Characters'] / token_df['Tokens']\n",
    "token_df['Tokens per Char'] = token_df['Tokens'] / token_df['Characters']\n",
    "\n",
    "# Sort by tokens count\n",
    "token_df = token_df.sort_values('Tokens', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "display(token_df)\n",
    "\n",
    "# Visualize token counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(token_df['Test Case'], token_df['Tokens'], color='skyblue')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Test Case')\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.title('Token Counts for Different Content Types')\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Visualize characters per token\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(token_df['Test Case'], token_df['Chars per Token'], color='coral')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Test Case')\n",
    "plt.ylabel('Characters per Token')\n",
    "plt.title('Characters per Token for Different Content Types')\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chat messages from the test cases\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is artificial intelligence?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. It encompasses various technologies including machine learning, natural language processing, computer vision, and robotics. AI systems can perform tasks that typically require human intelligence such as visual perception, speech recognition, decision-making, and language translation.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How is AI different from machine learning?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chat_messages(messages):\n",
    "    \"\"\"Analyze token usage in chat messages.\"\"\"\n",
    "    # Calculate overall token count\n",
    "    total_tokens = count_message_tokens(messages)\n",
    "    \n",
    "    # Calculate token count for each message individually\n",
    "    individual_counts = []\n",
    "    for msg in messages:\n",
    "        content_tokens = count_tokens(msg[\"content\"])\n",
    "        role_tokens = count_tokens(msg[\"role\"])\n",
    "        individual_counts.append({\n",
    "            \"role\": msg[\"role\"],\n",
    "            \"content_chars\": len(msg[\"content\"]),\n",
    "            \"content_tokens\": content_tokens,\n",
    "            \"role_tokens\": role_tokens,\n",
    "            \"message_tokens\": content_tokens + role_tokens\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(individual_counts)\n",
    "    \n",
    "    # Add message format overhead\n",
    "    sum_of_parts = df[\"message_tokens\"].sum()\n",
    "    format_overhead = total_tokens - sum_of_parts\n",
    "    \n",
    "    content = f\"\"\"## Chat Message Analysis\n",
    "\n",
    "**Total tokens in the entire message structure:** {total_tokens}\n",
    "**Sum of individual message tokens:** {sum_of_parts}\n",
    "**Format overhead tokens:** {format_overhead}\n",
    "**Overhead tokens per message:** {format_overhead / len(messages):.1f}\"\"\"\n",
    "    \n",
    "    display(Markdown(content))\n",
    "    return df\n",
    "\n",
    "# Analyze our chat messages\n",
    "chat_analysis = analyze_chat_messages(chat_messages)\n",
    "display(chat_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there's overhead in the chat message format beyond just the content tokens. This includes tokens for marking message boundaries, role indicators, and other structural elements.\n",
    "\n",
    "Let's create a function to estimate total tokens for a chat conversation with a certain number of messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_chat_tokens(avg_message_length, num_messages, avg_overhead=4):\n",
    "    \"\"\"Estimate total tokens for a chat conversation.\n",
    "    \n",
    "    Args:\n",
    "        avg_message_length: Average number of tokens per message content\n",
    "        num_messages: Number of messages in the conversation\n",
    "        avg_overhead: Average overhead tokens per message (typically 3-5)\n",
    "        \n",
    "    Returns:\n",
    "        Estimated total tokens\n",
    "    \"\"\"\n",
    "    return num_messages * (avg_message_length + avg_overhead)\n",
    "\n",
    "# Create a table of token estimates for different conversation sizes\n",
    "conversation_sizes = [\n",
    "    {\"messages\": 5, \"avg_length\": 50},  # Small conversation\n",
    "    {\"messages\": 10, \"avg_length\": 100},  # Medium conversation\n",
    "    {\"messages\": 20, \"avg_length\": 150},  # Large conversation\n",
    "    {\"messages\": 50, \"avg_length\": 100},  # Very large conversation\n",
    "    {\"messages\": 100, \"avg_length\": 80},  # Extremely large conversation\n",
    "]\n",
    "\n",
    "conversation_estimates = []\n",
    "for size in conversation_sizes:\n",
    "    tokens = estimate_chat_tokens(size[\"avg_length\"], size[\"messages\"])\n",
    "    conversation_estimates.append({\n",
    "        \"num_messages\": size[\"messages\"],\n",
    "        \"avg_message_length\": size[\"avg_length\"],\n",
    "        \"estimated_tokens\": tokens,\n",
    "        \"fits_in_8k_context\": tokens <= 8000,\n",
    "        \"fits_in_16k_context\": tokens <= 16000,\n",
    "        \"fits_in_128k_context\": tokens <= 128000\n",
    "    })\n",
    "\n",
    "# Display the estimates\n",
    "estimate_df = pd.DataFrame(conversation_estimates)\n",
    "display(estimate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Window Optimization\n",
    "\n",
    "The context window is the amount of text (measured in tokens) that a model can process at once. Different models have different context window sizes. Let's explore how to optimize within these constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Context Window Sizes\n",
    "\n",
    "Let's first look at the context window sizes for different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of popular models (updated for 2025)\n",
    "models = [\n",
    "    \"openai/gpt-4o-2024-08-06\",\n",
    "    \"openai/gpt-4o-mini-2024-07-18\", \n",
    "    \"openai/gpt-4-turbo\",\n",
    "    \"anthropic/claude-3-5-sonnet-20241022\",\n",
    "    \"anthropic/claude-3-5-haiku-20241022\",\n",
    "    \"anthropic/claude-3-opus-20240229\",\n",
    "    \"google/gemini-2.0-flash-exp\",\n",
    "    \"google/gemini-1.5-pro\",\n",
    "    \"google/gemini-1.5-flash\",\n",
    "    \"deepseek/deepseek-r1\",\n",
    "    \"deepseek/deepseek-chat-v3\"\n",
    "]\n",
    "\n",
    "# Get context window sizes\n",
    "context_windows = {model: get_context_window(model) for model in models}\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "context_df = pd.DataFrame({\n",
    "    'Model': list(context_windows.keys()),\n",
    "    'Context Window (tokens)': list(context_windows.values())\n",
    "})\n",
    "\n",
    "# Sort by context window size for better visualization\n",
    "context_df = context_df.sort_values('Context Window (tokens)', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "display(context_df)\n",
    "\n",
    "# Visualize context window sizes\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.barh(context_df['Model'], context_df['Context Window (tokens)'], color='skyblue')\n",
    "plt.xscale('log')  # Log scale for better visualization of the wide range\n",
    "plt.xlabel('Context Window Size (tokens)')\n",
    "plt.title('Context Window Sizes for Latest LLM Models (2025)')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add labels to the bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    label_x_pos = width * 1.1  # Position label slightly to the right of the bar\n",
    "    plt.text(label_x_pos, bar.get_y() + bar.get_height()/2, f'{int(width):,}',\n",
    "             va='center', ha='left', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Context Window Best Practices\n",
    "\n",
    "When working with the context window, consider these best practices:\n",
    "\n",
    "1. **Reserve tokens for the response**: Don't fill the entire context window with input\n",
    "2. **Use a buffer**: Leave some tokens as a safety margin\n",
    "3. **Prioritize recent messages**: In a conversation, recent messages are often more important\n",
    "4. **Truncate or summarize older content**: For long conversations, summarize earlier exchanges\n",
    "\n",
    "Let's create utility functions to optimize content for the context window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a long piece of text from our test cases\n",
    "long_text = test_case_dict[\"Long Text\"]\n",
    "\n",
    "# Count the tokens in the long text\n",
    "long_text_tokens = count_tokens(long_text)\n",
    "content = f\"\"\"## Processing Long Text\n",
    "\n",
    "**Long text contains:** {long_text_tokens} tokens and {len(long_text)} characters.\"\"\"\n",
    "\n",
    "# Let's simulate a prompt that includes this long text\n",
    "prompt = f\"Summarize the following text in 3-5 bullet points:\\n\\n{long_text}\"\n",
    "prompt_tokens = count_tokens(prompt)\n",
    "content += f\"\\n**Full prompt contains:** {prompt_tokens} tokens.\"\n",
    "\n",
    "# Check if it fits in different context windows\n",
    "content += \"\\n\\n### Context Window Compatibility:\\n\\n\"\n",
    "for window_size in [8000, 16000, 32000, 64000, 128000]:\n",
    "    fits_with_buffer = prompt_tokens + 500 <= window_size  # 500 tokens reserved for response\n",
    "    status = \"‚úÖ\" if fits_with_buffer else \"‚ùå\"\n",
    "    content += f\"- **{window_size} token window** with response buffer: {status}\\n\"\n",
    "\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the long text to fit in a smaller window\n",
    "smaller_window = 4000  # Simulate a smaller context window\n",
    "reserved_tokens = 500  # Reserve tokens for the response\n",
    "\n",
    "# Check if we need optimization\n",
    "base_prompt = \"Summarize the following text in 3-5 bullet points:\\n\\n\"\n",
    "base_prompt_tokens = count_tokens(base_prompt)\n",
    "available_tokens = smaller_window - base_prompt_tokens - reserved_tokens\n",
    "\n",
    "content = f\"\"\"## Context Window Optimization\n",
    "\n",
    "**Base prompt uses:** {base_prompt_tokens} tokens\n",
    "**Available tokens for content:** {available_tokens}\n",
    "**Original content tokens:** {long_text_tokens}\"\"\"\n",
    "\n",
    "if long_text_tokens > available_tokens:\n",
    "    content += f\"\\n**Need to truncate content by:** {long_text_tokens - available_tokens} tokens\"\n",
    "    optimized_text = truncate_to_token_limit(long_text, available_tokens)\n",
    "    optimized_tokens = count_tokens(optimized_text)\n",
    "    content += f\"\\n**Optimized content tokens:** {optimized_tokens}\"\n",
    "    content += f\"\\n\\n**Truncated text (showing first 200 chars):**\\n\\n{optimized_text[:200]}...\"\n",
    "    \n",
    "    # Double-check the total\n",
    "    optimized_prompt = f\"{base_prompt}{optimized_text}\"\n",
    "    optimized_prompt_tokens = count_tokens(optimized_prompt)\n",
    "    content += f\"\\n\\n**Final prompt uses:** {optimized_prompt_tokens} tokens\"\n",
    "    fits = optimized_prompt_tokens + reserved_tokens <= smaller_window\n",
    "    status = \"‚úÖ\" if fits else \"‚ùå\"\n",
    "    content += f\"\\n**Fits in {smaller_window} token window with response buffer:** {status}\"\n",
    "else:\n",
    "    content += \"\\n\\n**Content already fits in the available space.**\"\n",
    "\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Optimize Chat History\n",
    "\n",
    "For multi-turn conversations, we often need to optimize the chat history to fit within the context window. Let's implement a function to manage chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_conversation(num_turns, message_length=100):\n",
    "    \"\"\"Create a sample conversation with the specified number of turns.\"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}\n",
    "    ]\n",
    "    \n",
    "    # Each turn consists of a user message and an assistant response\n",
    "    for i in range(num_turns):\n",
    "        # Add user message\n",
    "        conversation.append({\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"This is user message {i+1}. \" + \"A \" * message_length\n",
    "        })\n",
    "        \n",
    "        # Add assistant response\n",
    "        conversation.append({\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": f\"This is assistant response {i+1}. \" + \"B \" * message_length\n",
    "        })\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "# Create a long conversation\n",
    "long_conversation = create_sample_conversation(15, message_length=50)\n",
    "\n",
    "# Count the total tokens\n",
    "conversation_tokens = count_message_tokens(long_conversation)\n",
    "\n",
    "content = f\"\"\"## Sample Long Conversation\n",
    "\n",
    "**Long conversation has:** {len(long_conversation)} messages and uses {conversation_tokens} tokens.\n",
    "\n",
    "### First 2 messages:\"\"\"\n",
    "\n",
    "for msg in long_conversation[:2]:\n",
    "    content += f\"\\n**[{msg['role']}]:** {msg['content'][:50]}...\"\n",
    "    \n",
    "content += \"\\n\\n### Last 2 messages:\"\n",
    "for msg in long_conversation[-2:]:\n",
    "    content += f\"\\n**[{msg['role']}]:** {msg['content'][:50]}...\"\n",
    "\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the conversation for different context windows\n",
    "def optimize_for_target_window(messages, target_window, reserved_tokens=500):\n",
    "    \"\"\"Optimize messages to fit within a specific target context window.\"\"\"\n",
    "    available_tokens = target_window - reserved_tokens\n",
    "    \n",
    "    # If messages already fit, return as is\n",
    "    current_tokens = count_message_tokens(messages)\n",
    "    if current_tokens <= available_tokens:\n",
    "        return messages\n",
    "    \n",
    "    # Keep removing oldest messages until we fit (preserve system message)\n",
    "    optimized = messages.copy()\n",
    "    while optimized and count_message_tokens(optimized) > available_tokens:\n",
    "        # Remove the oldest message (excluding system message)\n",
    "        if len(optimized) > 1 and optimized[0][\"role\"] == \"system\":\n",
    "            optimized.pop(1)  # Remove first non-system message\n",
    "        elif len(optimized) > 0:\n",
    "            optimized.pop(0)  # Remove first message if no system message\n",
    "        else:\n",
    "            break  # Safety break\n",
    "    \n",
    "    return optimized\n",
    "\n",
    "results_content = []\n",
    "\n",
    "for window_size in [2000, 4000, 8000]:\n",
    "    # Reserve tokens for the response\n",
    "    reserved_tokens = 500\n",
    "    \n",
    "    # Optimize the conversation for this specific window size\n",
    "    try:\n",
    "        optimized_conversation = optimize_for_target_window(\n",
    "            long_conversation, \n",
    "            target_window=window_size,\n",
    "            reserved_tokens=reserved_tokens\n",
    "        )\n",
    "        \n",
    "        # Count tokens in the optimized conversation\n",
    "        optimized_tokens = count_message_tokens(optimized_conversation)\n",
    "        \n",
    "        result = f\"\"\"### Context window: {window_size} tokens\n",
    "\n",
    "**Original conversation:** {len(long_conversation)} messages, {conversation_tokens} tokens\n",
    "**Optimized conversation:** {len(optimized_conversation)} messages, {optimized_tokens} tokens\n",
    "**Messages removed:** {len(long_conversation) - len(optimized_conversation)}\n",
    "**Fits in context window with response buffer:** {\"‚úÖ\" if optimized_tokens + reserved_tokens <= window_size else \"‚ùå\"}\n",
    "\n",
    "\"\"\"\n",
    "        results_content.append(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        result = f\"\"\"### Context window: {window_size} tokens\n",
    "\n",
    "**Error optimizing conversation:** {str(e)}\n",
    "\n",
    "\"\"\"\n",
    "        results_content.append(result)\n",
    "\n",
    "# Display all results\n",
    "content = \"## Conversation Optimization Results\\n\\n\" + \"\\n\".join(results_content)\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunking Strategies for Long Inputs\n",
    "\n",
    "When dealing with very long texts that exceed the context window, we need strategies to chunk the content and process it in parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Chunking\n",
    "\n",
    "Let's implement and demonstrate a basic chunking strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our chunking function on the long text\n",
    "chunks = chunk_text(long_text, chunk_size=500, overlap=50)\n",
    "\n",
    "content = f\"\"\"## Basic Text Chunking\n",
    "\n",
    "**Divided text into {len(chunks)} chunks with 50 token overlap**\"\"\"\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    content += f\"\\n\\n### Chunk {i+1} ({chunk_tokens} tokens):\\n\\n{chunk[:100]}...\"\n",
    "\n",
    "# If there are more than 3 chunks, show the last one too\n",
    "if len(chunks) > 3:\n",
    "    last_chunk = chunks[-1]\n",
    "    last_chunk_tokens = count_tokens(last_chunk)\n",
    "    content += f\"\\n\\n### Last chunk ({last_chunk_tokens} tokens):\\n\\n{last_chunk[:100]}...\"\n",
    "\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Intelligent Chunking\n",
    "\n",
    "Basic chunking splits text based solely on token count. Let's implement a more intelligent chunking strategy that tries to respect document structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_structure(text, max_chunk_size=500, overlap=50):\n",
    "    \"\"\"Chunk text based on structural elements like paragraphs.\"\"\"\n",
    "    # Split by paragraph breaks first\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_chunk_tokens = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_tokens = count_tokens(paragraph)\n",
    "        \n",
    "        # If the paragraph itself is too large, we need to split it\n",
    "        if paragraph_tokens > max_chunk_size:\n",
    "            # Process the current chunk if it's not empty\n",
    "            if current_chunk_tokens > 0:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "                current_chunk_tokens = 0\n",
    "            \n",
    "            # Split the large paragraph and add as separate chunks\n",
    "            paragraph_chunks = chunk_text(paragraph, max_chunk_size, overlap)\n",
    "            chunks.extend(paragraph_chunks)\n",
    "        \n",
    "        # If adding this paragraph would exceed the limit, finalize the current chunk\n",
    "        elif current_chunk_tokens + paragraph_tokens > max_chunk_size:\n",
    "            chunks.append(current_chunk)\n",
    "            \n",
    "            # Start a new chunk with overlap\n",
    "            if overlap > 0 and current_chunk_tokens > 0:\n",
    "                # Get the last ~N tokens for overlap\n",
    "                # This is an approximation since we're working with text\n",
    "                words = current_chunk.split()\n",
    "                overlap_text = \" \".join(words[-overlap*2:])  # rough estimate\n",
    "                current_chunk = overlap_text + \"\\n\\n\" + paragraph\n",
    "                current_chunk_tokens = count_tokens(current_chunk)\n",
    "            else:\n",
    "                current_chunk = paragraph\n",
    "                current_chunk_tokens = paragraph_tokens\n",
    "        else:\n",
    "            # Add paragraph to the current chunk\n",
    "            if current_chunk:\n",
    "                current_chunk += \"\\n\\n\" + paragraph\n",
    "            else:\n",
    "                current_chunk = paragraph\n",
    "            current_chunk_tokens = count_tokens(current_chunk)\n",
    "    \n",
    "    # Don't forget to add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test the structural chunking\n",
    "structure_chunks = chunk_by_structure(long_text, max_chunk_size=500, overlap=50)\n",
    "\n",
    "content = f\"\"\"## Intelligent Structural Chunking\n",
    "\n",
    "**Divided text into {len(structure_chunks)} chunks based on structure**\"\"\"\n",
    "\n",
    "for i, chunk in enumerate(structure_chunks[:3]):  # Show first 3 chunks\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    content += f\"\\n\\n### Chunk {i+1} ({chunk_tokens} tokens):\\n\\n{chunk[:100]}...\"\n",
    "\n",
    "# If there are more than 3 chunks, show the last one too\n",
    "if len(structure_chunks) > 3:\n",
    "    last_chunk = structure_chunks[-1]\n",
    "    last_chunk_tokens = count_tokens(last_chunk)\n",
    "    content += f\"\\n\\n### Last chunk ({last_chunk_tokens} tokens):\\n\\n{last_chunk[:100]}...\"\n",
    "\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Processing Strategies for Chunked Text\n",
    "\n",
    "Once we've chunked the text, there are several strategies for processing it:\n",
    "\n",
    "1. **Process Each Chunk Independently**: Good for tasks like summarization or classification\n",
    "2. **Chain of Thought**: Process chunks sequentially, carrying information forward\n",
    "3. **Map-Reduce**: Process each chunk independently, then combine the results\n",
    "\n",
    "Let's implement a simple map-reduce approach for summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce_summarize(text, max_chunk_size=500, model=\"openai/gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Summarize long text using a map-reduce approach.\"\"\"\n",
    "    # Map phase: Split the text into chunks and summarize each chunk\n",
    "    chunks = chunk_by_structure(text, max_chunk_size)\n",
    "    \n",
    "    display(Markdown(f\"## Map-Reduce Summarization\\n\\n**Divided text into {len(chunks)} chunks**\"))\n",
    "    \n",
    "    chunk_summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        display(Markdown(f\"**Processing chunk {i+1}/{len(chunks)}...**\"))\n",
    "        \n",
    "        # Call the API to summarize this chunk\n",
    "        map_prompt = f\"Summarize the following text in 2-3 sentences, capturing key points:\\n\\n{chunk}\"\n",
    "        response = call_openrouter(\n",
    "            prompt=map_prompt,\n",
    "            model=model,\n",
    "            temperature=0.3,  # Lower temperature for more focused summaries\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        if response.get(\"success\", False):\n",
    "            chunk_summary = extract_text_response(response)\n",
    "            chunk_summaries.append(chunk_summary)\n",
    "        else:\n",
    "            error_msg = f\"[Error processing chunk {i+1}]\"\n",
    "            display(Markdown(f\"‚ùå Error summarizing chunk {i+1}: {response.get('error', 'Unknown error')}\"))\n",
    "            chunk_summaries.append(error_msg)\n",
    "    \n",
    "    # Reduce phase: Combine the summaries into a single summary\n",
    "    display(Markdown(\"**Combining summaries...**\"))\n",
    "    combined_summaries = \"\\n\\n\".join([f\"Chunk {i+1}: {summary}\" for i, summary in enumerate(chunk_summaries)])\n",
    "    \n",
    "    reduce_prompt = f\"\"\"You have been given summaries of different chunks of a larger text. \n",
    "    Based on these summaries, create a cohesive summary of the entire text in 4-5 bullet points.\n",
    "    \n",
    "    Chunk summaries:\n",
    "    {combined_summaries}\"\"\"\n",
    "    \n",
    "    final_response = call_openrouter(\n",
    "        prompt=reduce_prompt,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    if final_response.get(\"success\", False):\n",
    "        final_summary = extract_text_response(final_response)\n",
    "        return {\n",
    "            \"chunk_summaries\": chunk_summaries,\n",
    "            \"final_summary\": final_summary\n",
    "        }\n",
    "    else:\n",
    "        display(Markdown(f\"‚ùå Error in reduce phase: {final_response.get('error', 'Unknown error')}\"))\n",
    "        return {\n",
    "            \"chunk_summaries\": chunk_summaries,\n",
    "            \"final_summary\": \"[Error generating final summary]\"\n",
    "        }\n",
    "\n",
    "# Test the map-reduce summarization\n",
    "summary_results = map_reduce_summarize(long_text, max_chunk_size=600)\n",
    "\n",
    "content = \"## Individual Chunk Summaries\\n\\n\"\n",
    "for i, summary in enumerate(summary_results[\"chunk_summaries\"]):\n",
    "    content += f\"### Chunk {i+1}:\\n\\n{summary}\\n\\n\"\n",
    "\n",
    "content += \"---\\n\\n## Final Combined Summary\\n\\n\"\n",
    "content += summary_results[\"final_summary\"]\n",
    "\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Token Usage Estimation and Cost Calculation\n",
    "\n",
    "Understanding and managing token usage is important both for ensuring content fits within context windows and for managing costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Token Usage Patterns\n",
    "\n",
    "Let's analyze how tokens are used in different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common tasks and their token usage patterns\n",
    "tasks = [\n",
    "    {\"name\": \"Simple Question\", \"input_tokens\": 10, \"output_tokens\": 50},\n",
    "    {\"name\": \"Detailed Question\", \"input_tokens\": 50, \"output_tokens\": 200},\n",
    "    {\"name\": \"Text Summarization (1 page)\", \"input_tokens\": 500, \"output_tokens\": 100},\n",
    "    {\"name\": \"Text Summarization (5 pages)\", \"input_tokens\": 2500, \"output_tokens\": 200},\n",
    "    {\"name\": \"Essay Generation\", \"input_tokens\": 100, \"output_tokens\": 1000},\n",
    "    {\"name\": \"Code Generation\", \"input_tokens\": 200, \"output_tokens\": 500},\n",
    "    {\"name\": \"Chat Conversation (10 turns)\", \"input_tokens\": 1000, \"output_tokens\": 1000},\n",
    "    {\"name\": \"Document Analysis\", \"input_tokens\": 4000, \"output_tokens\": 300},\n",
    "]\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "task_df = pd.DataFrame(tasks)\n",
    "task_df[\"total_tokens\"] = task_df[\"input_tokens\"] + task_df[\"output_tokens\"]\n",
    "\n",
    "# Visualize token distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "task_df.plot(x=\"name\", y=[\"input_tokens\", \"output_tokens\"], kind=\"bar\", stacked=True, \n",
    "            color=[\"skyblue\", \"coral\"], figsize=(12, 6))\n",
    "plt.title(\"Token Usage Patterns for Different Tasks\")\n",
    "plt.xlabel(\"Task\")\n",
    "plt.ylabel(\"Number of Tokens\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend(title=\"Token Type\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Cost Calculation\n",
    "\n",
    "Now, let's calculate the cost of these tasks for different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate costs for each task across different models\n",
    "model_costs = []\n",
    "\n",
    "for task in tasks:\n",
    "    task_name = task[\"name\"]\n",
    "    input_tokens = task[\"input_tokens\"]\n",
    "    output_tokens = task[\"output_tokens\"]\n",
    "    \n",
    "    for model in models[:5]:  # Use first 5 models to keep the table manageable\n",
    "        cost = estimate_cost(model, input_tokens, output_tokens)\n",
    "        model_costs.append({\n",
    "            \"task\": task_name,\n",
    "            \"model\": model,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": input_tokens + output_tokens,\n",
    "            \"cost_usd\": cost\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "cost_df = pd.DataFrame(model_costs)\n",
    "\n",
    "# Pivot the table to show costs by task and model\n",
    "pivot_df = cost_df.pivot(index=\"task\", columns=\"model\", values=\"cost_usd\")\n",
    "\n",
    "# Format the pivot table for display (round to 6 decimal places and format as currency)\n",
    "formatted_pivot = pivot_df.applymap(lambda x: f\"${x:.6f}\")\n",
    "display(formatted_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cost comparison for a specific task\n",
    "def visualize_task_costs(task_name):\n",
    "    \"\"\"Visualize costs for a specific task across models.\"\"\"\n",
    "    task_costs = cost_df[cost_df[\"task\"] == task_name]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(task_costs[\"model\"], task_costs[\"cost_usd\"], color=\"purple\")\n",
    "    plt.title(f\"Cost Comparison for '{task_name}' Across Models\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Cost (USD)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    # Add cost labels above the bars\n",
    "    for i, cost in enumerate(task_costs[\"cost_usd\"]):\n",
    "        plt.text(i, cost + 0.00001, f\"${cost:.6f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test with a couple of tasks\n",
    "visualize_task_costs(\"Document Analysis\")\n",
    "visualize_task_costs(\"Simple Question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Cost Optimization Strategies\n",
    "\n",
    "Let's explore some strategies for optimizing token usage and reducing costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cost optimization strategies\n",
    "optimization_strategies = [\n",
    "    {\n",
    "        \"strategy\": \"Use smaller models for simpler tasks\",\n",
    "        \"description\": \"Use GPT-4o mini or Gemini Flash for straightforward tasks, reserving larger models for complex reasoning.\",\n",
    "        \"token_savings\": \"0%\",\n",
    "        \"cost_savings\": \"50-90%\"\n",
    "    },\n",
    "    {\n",
    "        \"strategy\": \"Prompt engineering to reduce verbosity\",\n",
    "        \"description\": \"Request concise responses and specify exactly what you need.\",\n",
    "        \"token_savings\": \"20-50%\",\n",
    "        \"cost_savings\": \"10-25%\"\n",
    "    },\n",
    "    {\n",
    "        \"strategy\": \"Pre-process and filter inputs\",\n",
    "        \"description\": \"Remove irrelevant content before sending to the model, especially for document processing.\",\n",
    "        \"token_savings\": \"30-70%\",\n",
    "        \"cost_savings\": \"30-70%\"\n",
    "    },\n",
    "    {\n",
    "        \"strategy\": \"Reuse responses for repeated questions\",\n",
    "        \"description\": \"Implement caching for common queries to avoid redundant API calls.\",\n",
    "        \"token_savings\": \"Varies\",\n",
    "        \"cost_savings\": \"Up to 90%\"\n",
    "    },\n",
    "    {\n",
    "        \"strategy\": \"Use map-reduce for large documents\",\n",
    "        \"description\": \"Process large documents in chunks then combine, rather than exceeding context limits.\",\n",
    "        \"token_savings\": \"Varies\",\n",
    "        \"cost_savings\": \"20-50%\"\n",
    "    },\n",
    "    {\n",
    "        \"strategy\": \"Truncate chat history\",\n",
    "        \"description\": \"Maintain only the most relevant conversation history, summarizing older messages.\",\n",
    "        \"token_savings\": \"40-80%\",\n",
    "        \"cost_savings\": \"40-80%\"\n",
    "    },\n",
    "    {\n",
    "        \"strategy\": \"Use embeddings for retrieval\",\n",
    "        \"description\": \"For large knowledge bases, use embeddings to retrieve relevant content instead of including everything.\",\n",
    "        \"token_savings\": \"80-95%\",\n",
    "        \"cost_savings\": \"80-95%\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display the strategies\n",
    "strategy_df = pd.DataFrame(optimization_strategies)\n",
    "display(strategy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement one of these strategies to demonstrate token savings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Prompt Engineering for Verbosity Control\n",
    "\n",
    "verbose_prompt = \"Tell me about the history of artificial intelligence and its major developments over time.\"\n",
    "concise_prompt = \"List the 5 most important milestones in AI history with their dates. Keep it brief.\"\n",
    "\n",
    "# Model to use\n",
    "test_model = \"openai/gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "# Get verbose response\n",
    "verbose_response = call_openrouter(\n",
    "    prompt=verbose_prompt,\n",
    "    model=test_model,\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Get concise response\n",
    "concise_response = call_openrouter(\n",
    "    prompt=concise_prompt,\n",
    "    model=test_model,\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "if verbose_response.get(\"success\", False) and concise_response.get(\"success\", False):\n",
    "    verbose_text = extract_text_response(verbose_response)\n",
    "    concise_text = extract_text_response(concise_response)\n",
    "    \n",
    "    verbose_tokens = count_tokens(verbose_text)\n",
    "    concise_tokens = count_tokens(concise_text)\n",
    "    \n",
    "    verbose_cost = estimate_cost(test_model, count_tokens(verbose_prompt), verbose_tokens)\n",
    "    concise_cost = estimate_cost(test_model, count_tokens(concise_prompt), concise_tokens)\n",
    "    \n",
    "    print(f\"Verbose prompt: {verbose_prompt}\")\n",
    "    print(f\"Concise prompt: {concise_prompt}\\n\")\n",
    "    \n",
    "    print(f\"Verbose response tokens: {verbose_tokens}\")\n",
    "    print(f\"Concise response tokens: {concise_tokens}\")\n",
    "    print(f\"Token reduction: {(verbose_tokens - concise_tokens) / verbose_tokens:.1%}\\n\")\n",
    "    \n",
    "    print(f\"Verbose response cost: ${verbose_cost:.6f}\")\n",
    "    print(f\"Concise response cost: ${concise_cost:.6f}\")\n",
    "    print(f\"Cost savings: ${verbose_cost - concise_cost:.6f} ({(verbose_cost - concise_cost) / verbose_cost:.1%})\\n\")\n",
    "    \n",
    "    print(\"Verbose Response:\")\n",
    "    print(verbose_text)\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    print(\"Concise Response:\")\n",
    "    print(concise_text)\n",
    "else:\n",
    "    print(\"Error getting responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Application: Token-Aware Document Processor\n",
    "\n",
    "Let's build a practical application that demonstrates token-aware processing of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAwareDocumentProcessor:\n",
    "    \"\"\"A processor that intelligently handles documents with token awareness.\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"openai/gpt-4o-mini-2024-07-18\", max_tokens_per_chunk=700):\n",
    "        self.model = model\n",
    "        self.max_tokens_per_chunk = max_tokens_per_chunk\n",
    "        self.context_window = get_context_window(model)\n",
    "        self.reserved_tokens = 300  # Reserve tokens for system prompts and responses\n",
    "    \n",
    "    def _estimate_processing_cost(self, document):\n",
    "        \"\"\"Estimate the cost of processing the document.\"\"\"\n",
    "        doc_tokens = count_tokens(document)\n",
    "        chunks = math.ceil(doc_tokens / self.max_tokens_per_chunk)\n",
    "        \n",
    "        # Estimate input tokens (chunks + overhead)\n",
    "        estimated_input_tokens = doc_tokens + (chunks * 50)  # 50 tokens overhead per chunk\n",
    "        \n",
    "        # Estimate output tokens (varies by operation)\n",
    "        operations = {\n",
    "            \"summarize\": doc_tokens * 0.2,  # Summary is ~20% of original length\n",
    "            \"analyze\": doc_tokens * 0.3,    # Analysis is ~30% of original length\n",
    "            \"extract\": doc_tokens * 0.1     # Extraction is ~10% of original length\n",
    "        }\n",
    "        \n",
    "        # Calculate costs for each operation\n",
    "        costs = {}\n",
    "        for op, output_tokens in operations.items():\n",
    "            costs[op] = estimate_cost(self.model, estimated_input_tokens, output_tokens)\n",
    "        \n",
    "        return {\n",
    "            \"document_tokens\": doc_tokens,\n",
    "            \"estimated_chunks\": chunks,\n",
    "            \"estimated_input_tokens\": estimated_input_tokens,\n",
    "            \"estimated_output_tokens\": operations,\n",
    "            \"estimated_costs\": costs\n",
    "        }\n",
    "    \n",
    "    def _determine_processing_strategy(self, document):\n",
    "        \"\"\"Determine the best processing strategy based on document length.\"\"\"\n",
    "        doc_tokens = count_tokens(document)\n",
    "        available_tokens = self.context_window - self.reserved_tokens\n",
    "        \n",
    "        if doc_tokens <= available_tokens:\n",
    "            return \"single_pass\"\n",
    "        else:\n",
    "            return \"map_reduce\"\n",
    "    \n",
    "    def summarize(self, document):\n",
    "        \"\"\"Summarize the document.\"\"\"\n",
    "        strategy = self._determine_processing_strategy(document)\n",
    "        \n",
    "        if strategy == \"single_pass\":\n",
    "            print(\"Using single pass processing strategy\")\n",
    "            system_prompt = \"You are an expert summarizer. Create a concise, accurate summary of the text.\"\n",
    "            prompt = f\"Please summarize the following document in a few paragraphs, highlighting the main points:\\n\\n{document}\"\n",
    "            \n",
    "            response = call_openrouter(\n",
    "                prompt=prompt,\n",
    "                model=self.model,\n",
    "                system_prompt=system_prompt,\n",
    "                temperature=0.3,\n",
    "                max_tokens=self.reserved_tokens\n",
    "            )\n",
    "            \n",
    "            if response.get(\"success\", False):\n",
    "                return extract_text_response(response)\n",
    "            else:\n",
    "                return f\"Error: {response.get('error', 'Unknown error')}\"\n",
    "        else:\n",
    "            print(\"Using map-reduce processing strategy\")\n",
    "            result = map_reduce_summarize(document, self.max_tokens_per_chunk, self.model)\n",
    "            return result[\"final_summary\"]\n",
    "    \n",
    "    def analyze_document(self, document):\n",
    "        \"\"\"Analyze the document.\"\"\"\n",
    "        strategy = self._determine_processing_strategy(document)\n",
    "        \n",
    "        if strategy == \"single_pass\":\n",
    "            print(\"Using single pass processing strategy\")\n",
    "            system_prompt = \"You are an expert document analyst. Provide clear, insightful analysis.\"\n",
    "            prompt = f\"\"\"Please analyze the following document and provide insights on:\n",
    "            1. Main themes and topics\n",
    "            2. Key arguments or points\n",
    "            3. Tone and style\n",
    "            4. Intended audience\n",
    "            \n",
    "            Document:\n",
    "            {document}\"\"\"\n",
    "            \n",
    "            response = call_openrouter(\n",
    "                prompt=prompt,\n",
    "                model=self.model,\n",
    "                system_prompt=system_prompt,\n",
    "                temperature=0.3,\n",
    "                max_tokens=self.reserved_tokens\n",
    "            )\n",
    "            \n",
    "            if response.get(\"success\", False):\n",
    "                return extract_text_response(response)\n",
    "            else:\n",
    "                return f\"Error: {response.get('error', 'Unknown error')}\"\n",
    "        else:\n",
    "            print(\"Using map-reduce processing strategy\")\n",
    "            # Split into chunks\n",
    "            chunks = chunk_by_structure(document, self.max_tokens_per_chunk)\n",
    "            \n",
    "            # Analyze each chunk\n",
    "            chunk_analyses = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "                \n",
    "                chunk_prompt = f\"\"\"Analyze this document segment and identify:\n",
    "                1. Main themes and topics\n",
    "                2. Key points or arguments\n",
    "                3. Tone and style\n",
    "                \n",
    "                Document segment:\n",
    "                {chunk}\"\"\"\n",
    "                \n",
    "                response = call_openrouter(\n",
    "                    prompt=chunk_prompt,\n",
    "                    model=self.model,\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=200\n",
    "                )\n",
    "                \n",
    "                if response.get(\"success\", False):\n",
    "                    chunk_analysis = extract_text_response(response)\n",
    "                    chunk_analyses.append(chunk_analysis)\n",
    "                else:\n",
    "                    chunk_analyses.append(f\"[Error analyzing chunk {i+1}]\")\n",
    "            \n",
    "            # Combine the analyses\n",
    "            print(\"Combining analyses...\")\n",
    "            combined_analyses = \"\\n\\n\".join([f\"Segment {i+1} Analysis:\\n{analysis}\" for i, analysis in enumerate(chunk_analyses)])\n",
    "            \n",
    "            final_prompt = f\"\"\"You have been given analyses of different segments of a document.\n",
    "            Based on these segment analyses, provide a comprehensive analysis of the entire document, covering:\n",
    "            1. Main themes and topics across the entire document\n",
    "            2. Key arguments or points and how they develop\n",
    "            3. Overall tone and style\n",
    "            4. Likely intended audience\n",
    "            \n",
    "            Segment analyses:\n",
    "            {combined_analyses}\"\"\"\n",
    "            \n",
    "            final_response = call_openrouter(\n",
    "                prompt=final_prompt,\n",
    "                model=self.model,\n",
    "                temperature=0.3,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            \n",
    "            if final_response.get(\"success\", False):\n",
    "                return extract_text_response(final_response)\n",
    "            else:\n",
    "                return f\"Error in final analysis: {final_response.get('error', 'Unknown error')}\"\n",
    "    \n",
    "    def process_document(self, document):\n",
    "        \"\"\"Process the document with cost estimation and strategy selection.\"\"\"\n",
    "        # Get cost estimates\n",
    "        estimates = self._estimate_processing_cost(document)\n",
    "        \n",
    "        # Display processing information\n",
    "        print(f\"Document size: {estimates['document_tokens']} tokens\")\n",
    "        print(f\"Processing strategy: {self._determine_processing_strategy(document)}\")\n",
    "        print(f\"Estimated chunks needed: {estimates['estimated_chunks']}\")\n",
    "        print(f\"Estimated cost for summarization: ${estimates['estimated_costs']['summarize']:.6f}\")\n",
    "        print(f\"Estimated cost for analysis: ${estimates['estimated_costs']['analyze']:.6f}\")\n",
    "        print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "        \n",
    "        # Process the document\n",
    "        results = {\n",
    "            \"summary\": self.summarize(document),\n",
    "            \"analysis\": self.analyze_document(document)\n",
    "        }\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the TokenAwareDocumentProcessor with our long text\n",
    "processor = TokenAwareDocumentProcessor(model=\"openai/gpt-4o-mini-2024-07-18\")\n",
    "results = processor.process_document(long_text)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOCUMENT SUMMARY:\")\n",
    "print(results[\"summary\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOCUMENT ANALYSIS:\")\n",
    "print(results[\"analysis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "Here are some exercises to practice token management techniques:\n",
    "\n",
    "1. **Token Counter**: Create a tool that counts tokens for common file formats (TXT, PDF, Markdown, etc.) and provides a cost estimate for processing them with different models\n",
    "\n",
    "2. **Context Window Optimizer**: Develop a function that dynamically decides whether to use a whole-document approach or a chunking approach based on token counts\n",
    "\n",
    "3. **Chat History Manager**: Implement a system that maintains chat history while keeping token count under control (summarizing old messages or removing less relevant ones)\n",
    "\n",
    "4. **Token-Efficient Embeddings**: Create a system that generates embeddings for document chunks while minimizing token usage and maximizing information retention\n",
    "\n",
    "5. **Cost Comparison Tool**: Build a tool that compares the cost of running the same task across different models, helping users choose the most cost-effective option for their needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this notebook, we've explored token management strategies for working with LLMs, including:\n",
    "\n",
    "- Understanding how text is tokenized and how different content types affect token counts\n",
    "- Implementing techniques to optimize content for different context window sizes\n",
    "- Developing chunking strategies for processing long documents\n",
    "- Creating map-reduce approaches for handling content that exceeds context limits\n",
    "- Estimating token usage and calculating costs across different models\n",
    "- Building a token-aware document processor that selects optimal strategies based on content length\n",
    "\n",
    "Effective token management is essential for maximizing the capabilities of LLMs while controlling costs. By applying these techniques, you can handle documents of any size and optimize your API usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
