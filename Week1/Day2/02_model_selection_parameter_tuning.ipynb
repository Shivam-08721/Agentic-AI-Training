{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Parameter Tuning\n",
    "\n",
    "This notebook explores how to select the appropriate models for different tasks and how to tune parameters to get the best results. Topics include:\n",
    "\n",
    "- Comparing different models (capabilities/costs)\n",
    "- Experimenting with temperature, top-p, and other parameters\n",
    "- Understanding the effects of system prompts on model behavior\n",
    "- Balancing between deterministic and creative outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML, clear_output, Markdown\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import our custom utilities\n",
    "sys.path.append('.')\n",
    "from api_utils import (\n",
    "    call_openrouter,\n",
    "    extract_text_response,\n",
    "    get_available_models,\n",
    "    estimate_cost\n",
    ")\n",
    "from token_counter import count_tokens, count_message_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our API key is loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the API key is loaded\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"‚úÖ API key loaded successfully!\")\n",
    "    # Show first and last three characters for verification\n",
    "    masked_key = f\"{api_key[:3]}...{api_key[-3:]}\" if len(api_key) > 6 else \"[key too short]\"\n",
    "    print(f\"API key: {masked_key}\")\n",
    "else:\n",
    "    print(\"‚ùå API key not found! Make sure you've created a .env file with your OPENROUTER_API_KEY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Comparison Framework\n",
    "\n",
    "Let's create a framework to compare different models on the same prompts, so we can analyze their capabilities, performance, and cost-effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(prompt, models, system_prompt=None, temperature=0.7, max_tokens=300):\n",
    "    \"\"\"Compare responses from multiple models on the same prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the models\n",
    "        models: List of model names to compare\n",
    "        system_prompt: Optional system prompt\n",
    "        temperature: Temperature setting (0-1)\n",
    "        max_tokens: Maximum tokens in the response\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with model responses and metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Format the prompt as a message if it's a string\n",
    "    if isinstance(prompt, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if system_prompt:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    else:\n",
    "        # Assume it's already in message format\n",
    "        messages = prompt\n",
    "    \n",
    "    # Calculate input tokens\n",
    "    input_tokens = count_message_tokens(messages)\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"üìä Testing model: {model}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Call the API\n",
    "        response = call_openrouter(\n",
    "            prompt=messages,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        if response.get(\"success\", False):\n",
    "            text_response = extract_text_response(response)\n",
    "            \n",
    "            # Calculate output tokens\n",
    "            output_tokens = count_tokens(text_response)\n",
    "            \n",
    "            # Estimate cost\n",
    "            cost = estimate_cost(model, input_tokens, output_tokens)\n",
    "            \n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"response\": text_response,\n",
    "                \"duration\": round(duration, 2),\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"total_tokens\": input_tokens + output_tokens,\n",
    "                \"cost_usd\": cost\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"response\": f\"Error: {response.get('error', 'Unknown error')}\",\n",
    "                \"duration\": round(duration, 2),\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": 0,\n",
    "                \"total_tokens\": input_tokens,\n",
    "                \"cost_usd\": 0\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "def display_comparison(df, show_metrics=True):\n",
    "    \"\"\"Display the model comparison results in a readable format.\"\"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        model = row['model']\n",
    "        response = row['response']\n",
    "        \n",
    "        # Create markdown content\n",
    "        content = f\"## Model: {model}\\n\\n\"\n",
    "        \n",
    "        if show_metrics:\n",
    "            content += f\"**Metrics:** Time: {row['duration']}s | Input tokens: {row['input_tokens']} | Output tokens: {row['output_tokens']}\\n\\n\"\n",
    "            content += f\"**Total tokens:** {row['total_tokens']} | **Estimated cost:** ${row['cost_usd']:.6f}\\n\\n\"\n",
    "        \n",
    "        content += f\"**Response:**\\n\\n{response}\\n\\n\"\n",
    "        content += \"---\\n\\n\"\n",
    "        \n",
    "        display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Compare Different Models\n",
    "\n",
    "Let's compare some popular models on the same prompt to see how they differ in terms of quality, speed, and cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models_to_compare = [\n",
    "    \"openai/gpt-4o-mini-2024-07-18\",  # Cost-efficient GPT-4o mini\n",
    "    \"openai/gpt-4o-2024-08-06\",       # Full GPT-4o\n",
    "    \"anthropic/claude-3.5-haiku\",  # Fastest Claude model\n",
    "    \"google/gemini-2.5-flash-preview-05-20\"         # Fast Gemini model\n",
    "]\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Explain the concept of neural networks to a high school student\"\n",
    "\n",
    "# Compare models\n",
    "comparison_results = compare_models(\n",
    "    prompt=test_prompt,\n",
    "    models=models_to_compare,\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display_comparison(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize Performance Metrics\n",
    "\n",
    "Let's create some visualizations to better understand the differences between models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_metrics(df):\n",
    "    \"\"\"Plot performance metrics for compared models.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot response times\n",
    "    axes[0, 0].bar(df['model'], df['duration'], color='skyblue')\n",
    "    axes[0, 0].set_title('Response Time (seconds)')\n",
    "    axes[0, 0].set_xticklabels(df['model'], rotation=45, ha='right')\n",
    "    axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot token counts\n",
    "    axes[0, 1].bar(df['model'], df['input_tokens'], label='Input Tokens', color='lightgreen')\n",
    "    axes[0, 1].bar(df['model'], df['output_tokens'], bottom=df['input_tokens'], label='Output Tokens', color='coral')\n",
    "    axes[0, 1].set_title('Token Usage')\n",
    "    axes[0, 1].set_xticklabels(df['model'], rotation=45, ha='right')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot cost\n",
    "    axes[1, 0].bar(df['model'], df['cost_usd'], color='purple')\n",
    "    axes[1, 0].set_title('Estimated Cost (USD)')\n",
    "    axes[1, 0].set_xticklabels(df['model'], rotation=45, ha='right')\n",
    "    axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot tokens per second\n",
    "    tokens_per_second = df['output_tokens'] / df['duration']\n",
    "    axes[1, 1].bar(df['model'], tokens_per_second, color='orange')\n",
    "    axes[1, 1].set_title('Output Tokens per Second')\n",
    "    axes[1, 1].set_xticklabels(df['model'], rotation=45, ha='right')\n",
    "    axes[1, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot metrics for our comparison\n",
    "plot_model_metrics(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temperature and Creativity\n",
    "\n",
    "The temperature parameter controls randomness in the model's responses. Let's experiment with different temperature settings to see how they affect creativity and determinism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_temperatures(prompt, model, temperatures, system_prompt=None, max_tokens=300, iterations=1):\n",
    "    \"\"\"Compare the same model's responses at different temperature settings.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the model\n",
    "        model: The model to use\n",
    "        temperatures: List of temperature values to test\n",
    "        system_prompt: Optional system prompt\n",
    "        max_tokens: Maximum tokens in the response\n",
    "        iterations: Number of iterations per temperature (to check consistency)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with responses at different temperatures\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Format the prompt as a message if it's a string\n",
    "    if isinstance(prompt, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if system_prompt:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    else:\n",
    "        # Assume it's already in message format\n",
    "        messages = prompt\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        for i in range(iterations):\n",
    "            print(f\"üìù Testing temperature: {temp} (iteration {i+1}/{iterations})\")\n",
    "            \n",
    "            # Call the API\n",
    "            response = call_openrouter(\n",
    "                prompt=messages,\n",
    "                model=model,\n",
    "                temperature=temp,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            if response.get(\"success\", False):\n",
    "                text_response = extract_text_response(response)\n",
    "                \n",
    "                results.append({\n",
    "                    \"temperature\": temp,\n",
    "                    \"iteration\": i+1,\n",
    "                    \"response\": text_response,\n",
    "                })\n",
    "            else:\n",
    "                results.append({\n",
    "                    \"temperature\": temp,\n",
    "                    \"iteration\": i+1,\n",
    "                    \"response\": f\"Error: {response.get('error', 'Unknown error')}\",\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "def display_temperature_comparison(df):\n",
    "    \"\"\"Display responses at different temperatures in a readable format.\"\"\"\n",
    "    # Group by temperature and get unique temperatures\n",
    "    temperatures = df['temperature'].unique()\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        temp_df = df[df['temperature'] == temp]\n",
    "        \n",
    "        # Create markdown content\n",
    "        content = f\"## Temperature: {temp}\\n\\n\"\n",
    "        \n",
    "        for idx, row in temp_df.iterrows():\n",
    "            iteration = row['iteration']\n",
    "            response = row['response']\n",
    "            \n",
    "            content += f\"**Iteration {iteration}:**\\n\\n{response}\\n\\n\"\n",
    "        \n",
    "        content += \"---\\n\\n\"\n",
    "        display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temperatures to test\n",
    "temperatures_to_test = [0.0, 0.3, 0.7, 1.0]\n",
    "\n",
    "# Creative writing prompt to show temperature effects clearly\n",
    "creative_prompt = \"Write a short, creative story about a robot discovering emotions\"\n",
    "\n",
    "# Compare responses at different temperatures\n",
    "temperature_results = compare_temperatures(\n",
    "    prompt=creative_prompt,\n",
    "    model=\"openai/gpt-4o-mini-2024-07-18\",  # Using a cost-effective model\n",
    "    temperatures=temperatures_to_test,\n",
    "    max_tokens=200,\n",
    "    iterations=2  # Run each temperature twice to check consistency\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display_temperature_comparison(temperature_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Temperature Selection Guidelines\n",
    "\n",
    "Based on our experiments, here are some guidelines for temperature selection:\n",
    "\n",
    "| Temperature | Best Used For | Characteristics |\n",
    "|-------------|--------------|------------------|\n",
    "| 0.0-0.2 | Factual responses, coding, logical tasks | Deterministic, consistent, focused |\n",
    "| 0.3-0.5 | Balanced responses, explanations | Mostly consistent with some variation |\n",
    "| 0.6-0.8 | Creative writing, brainstorming | Good balance of coherence and creativity |\n",
    "| 0.9-1.0 | Maximum creativity, diverse ideas | Most random, potentially unfocused |\n",
    "\n",
    "Let's test this on a factual task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a factual question with different temperatures\n",
    "factual_prompt = \"What are three ways to prevent overfitting in machine learning models?\"\n",
    "\n",
    "factual_temperature_results = compare_temperatures(\n",
    "    prompt=factual_prompt,\n",
    "    model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "    temperatures=[0.0, 0.7],  # Compare low vs. medium temperature\n",
    "    max_tokens=300,\n",
    "    iterations=1\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display_temperature_comparison(factual_temperature_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top-p (Nucleus Sampling)\n",
    "\n",
    "Another parameter that controls text generation is top_p, which defines the percentage of probability mass to consider when sampling. Let's see how it affects responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_top_p(prompt, model, top_p_values, temperature=0.7, system_prompt=None, max_tokens=300):\n",
    "    \"\"\"Compare the model's responses with different top_p settings.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the model\n",
    "        model: The model to use\n",
    "        top_p_values: List of top_p values to test\n",
    "        temperature: Temperature setting\n",
    "        system_prompt: Optional system prompt\n",
    "        max_tokens: Maximum tokens in the response\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with responses at different top_p values\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Format the prompt as a message if it's a string\n",
    "    if isinstance(prompt, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if system_prompt:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    else:\n",
    "        # Assume it's already in message format\n",
    "        messages = prompt\n",
    "    \n",
    "    for top_p in top_p_values:\n",
    "        print(f\"üìä Testing top_p: {top_p}\")\n",
    "        \n",
    "        # Call the API\n",
    "        response = call_openrouter(\n",
    "            prompt=messages,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        if response.get(\"success\", False):\n",
    "            text_response = extract_text_response(response)\n",
    "            \n",
    "            results.append({\n",
    "                \"top_p\": top_p,\n",
    "                \"response\": text_response,\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"top_p\": top_p,\n",
    "                \"response\": f\"Error: {response.get('error', 'Unknown error')}\",\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "def display_top_p_comparison(df):\n",
    "    \"\"\"Display responses at different top_p values in a readable format.\"\"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        top_p = row['top_p']\n",
    "        response = row['response']\n",
    "        \n",
    "        # Create markdown content\n",
    "        content = f\"## Top-p: {top_p}\\n\\n\"\n",
    "        content += f\"**Response:**\\n\\n{response}\\n\\n\"\n",
    "        content += \"---\\n\\n\"\n",
    "        \n",
    "        display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top_p values to test\n",
    "top_p_values = [0.1, 0.5, 0.9]\n",
    "\n",
    "# Creative prompt to showcase top_p effects\n",
    "creative_prompt = \"Come up with five unusual uses for a paperclip\"\n",
    "\n",
    "# Compare responses with different top_p values\n",
    "top_p_results = compare_top_p(\n",
    "    prompt=creative_prompt,\n",
    "    model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "    top_p_values=top_p_values,\n",
    "    temperature=0.7,  # Keep temperature constant\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display_top_p_comparison(top_p_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Temperature vs. Top-p\n",
    "\n",
    "| Parameter | Function | When to Use | Effect of Low Value | Effect of High Value |\n",
    "|-----------|----------|-------------|---------------------|----------------------|\n",
    "| Temperature | Controls randomness directly | Primary control for creativity | Deterministic | Creative, diverse |\n",
    "| Top-p | Controls token selection pool | Fine-tuning after setting temperature | Only most likely tokens used | Wider distribution of tokens |\n",
    "\n",
    "In practice, most developers focus primarily on the temperature parameter and leave top-p at its default (usually 1.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Impact of System Prompts\n",
    "\n",
    "System prompts are a powerful way to guide model behavior. Let's experiment with different system prompts and see how they affect responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_system_prompts(prompt, model, system_prompts, temperature=0.7, max_tokens=300):\n",
    "    \"\"\"Compare the model's responses with different system prompts.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The user prompt to send to the model\n",
    "        model: The model to use\n",
    "        system_prompts: Dictionary of {label: system_prompt}\n",
    "        temperature: Temperature setting\n",
    "        max_tokens: Maximum tokens in the response\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with responses using different system prompts\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for label, system_prompt in system_prompts.items():\n",
    "        print(f\"üìù Testing system prompt: {label}\")\n",
    "        \n",
    "        # Create messages with the system prompt\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # Call the API\n",
    "        response = call_openrouter(\n",
    "            prompt=messages,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        if response.get(\"success\", False):\n",
    "            text_response = extract_text_response(response)\n",
    "            \n",
    "            results.append({\n",
    "                \"label\": label,\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"response\": text_response,\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"label\": label,\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"response\": f\"Error: {response.get('error', 'Unknown error')}\",\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "def display_system_prompt_comparison(df):\n",
    "    \"\"\"Display responses with different system prompts in a readable format.\"\"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        label = row['label']\n",
    "        system_prompt = row['system_prompt']\n",
    "        response = row['response']\n",
    "        \n",
    "        # Create markdown content\n",
    "        content = f\"## System Prompt: {label}\\n\\n\"\n",
    "        content += f\"**Prompt text:** {system_prompt}\\n\\n\"\n",
    "        content += f\"**Response:**\\n\\n{response}\\n\\n\"\n",
    "        content += \"---\\n\\n\"\n",
    "        \n",
    "        display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of different system prompts to test\n",
    "system_prompts = {\n",
    "    \"Default\": \"You are a helpful AI assistant.\",\n",
    "    \"Expert\": \"You are an expert in machine learning with a PhD in computer science. Provide detailed technical explanations.\",\n",
    "    \"Teacher\": \"You are a patient teacher explaining concepts to high school students. Use simple language and analogies.\",\n",
    "    \"Skeptic\": \"You are a skeptical thinker who points out limitations and potential problems in every concept.\",\n",
    "    \"Concise\": \"You provide extremely concise answers using as few words as possible.\"\n",
    "}\n",
    "\n",
    "# Use a prompt that can be answered differently based on the system prompt\n",
    "technical_prompt = \"Explain how deep learning works\"\n",
    "\n",
    "# Compare responses with different system prompts\n",
    "system_prompt_results = compare_system_prompts(\n",
    "    prompt=technical_prompt,\n",
    "    model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "    system_prompts=system_prompts,\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display_system_prompt_comparison(system_prompt_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 System Prompt Design Principles\n",
    "\n",
    "Based on our experiments, here are some principles for effective system prompt design:\n",
    "\n",
    "1. **Be specific about the role** - Define who the AI is supposed to be (expert, teacher, etc.)\n",
    "2. **Set clear constraints** - Mention limitations or restrictions (word count, style, etc.)\n",
    "3. **Define the format** - Specify how the response should be structured\n",
    "4. **Outline the tone** - Indicate the desired tone (formal, casual, enthusiastic)\n",
    "5. **Provide context** - Give relevant background information\n",
    "\n",
    "Let's test a more comprehensive system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced system prompt example\n",
    "advanced_system_prompt = \"\"\"\n",
    "You are a financial advisor with 15+ years of experience specializing in retirement planning. \n",
    "Follow these guidelines:\n",
    "1. Provide personalized advice based on the information given\n",
    "2. Always consider multiple strategies and explain tradeoffs\n",
    "3. Include specific numbers and calculations when relevant\n",
    "4. Use plain language, avoiding jargon when possible\n",
    "5. Structure your response with clear headings\n",
    "6. Always mention tax implications\n",
    "7. End with 2-3 actionable next steps\n",
    "\n",
    "Your goal is to help the user make informed financial decisions without overwhelming them.\n",
    "\"\"\"\n",
    "\n",
    "# Financial question\n",
    "financial_prompt = \"I'm 35 years old and want to retire by 60. I currently have $50,000 saved and can contribute $1,000 monthly. What strategies should I consider?\"\n",
    "\n",
    "# Test the advanced system prompt\n",
    "advanced_prompt_response = call_openrouter(\n",
    "    prompt=financial_prompt,\n",
    "    model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "    system_prompt=advanced_system_prompt,\n",
    "    temperature=0.7,\n",
    "    max_tokens=700\n",
    ")\n",
    "\n",
    "if advanced_prompt_response.get(\"success\", False):\n",
    "    financial_answer = extract_text_response(advanced_prompt_response)\n",
    "    display(Markdown(f\"## Advanced System Prompt Response\\n\\n{financial_answer}\"))\n",
    "else:\n",
    "    print(f\"Error: {advanced_prompt_response.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Other Parameters\n",
    "\n",
    "There are several other parameters that can influence model outputs. Let's explore some of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Frequency Penalty and Presence Penalty\n",
    "\n",
    "These parameters control repetition in the model's responses:\n",
    "\n",
    "- **Frequency Penalty**: Reduces repetition of specific tokens\n",
    "- **Presence Penalty**: Reduces repetition of topics or concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_repetition_parameters(prompt, model):\n",
    "    \"\"\"Test how frequency and presence penalties affect repetition.\"\"\"\n",
    "    parameter_sets = [\n",
    "        {\"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"label\": \"No penalties\"},\n",
    "        {\"frequency_penalty\": 1.0, \"presence_penalty\": 0.0, \"label\": \"High frequency penalty\"},\n",
    "        {\"frequency_penalty\": 0.0, \"presence_penalty\": 1.0, \"label\": \"High presence penalty\"},\n",
    "        {\"frequency_penalty\": 1.0, \"presence_penalty\": 1.0, \"label\": \"Both penalties high\"}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for params in parameter_sets:\n",
    "        print(f\"üìù Testing: {params['label']}\")\n",
    "        \n",
    "        # Call the API with specified parameters\n",
    "        response = call_openrouter(\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            temperature=0.7,\n",
    "            max_tokens=300,\n",
    "            frequency_penalty=params[\"frequency_penalty\"],\n",
    "            presence_penalty=params[\"presence_penalty\"]\n",
    "        )\n",
    "        \n",
    "        if response.get(\"success\", False):\n",
    "            text_response = extract_text_response(response)\n",
    "            \n",
    "            results.append({\n",
    "                \"label\": params['label'],\n",
    "                \"frequency_penalty\": params[\"frequency_penalty\"],\n",
    "                \"presence_penalty\": params[\"presence_penalty\"],\n",
    "                \"response\": text_response\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"label\": params['label'],\n",
    "                \"frequency_penalty\": params[\"frequency_penalty\"],\n",
    "                \"presence_penalty\": params[\"presence_penalty\"],\n",
    "                \"response\": f\"Error: {response.get('error', 'Unknown error')}\"\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Prompt designed to potentially cause repetition\n",
    "repetition_prompt = \"List 10 synonyms for 'good'\"\n",
    "\n",
    "# Test repetition parameters\n",
    "repetition_results = test_repetition_parameters(\n",
    "    prompt=repetition_prompt,\n",
    "    model=\"openai/gpt-4o-mini-2024-07-18\"\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for idx, row in repetition_results.iterrows():\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"{row['label']} (frequency={row['frequency_penalty']}, presence={row['presence_penalty']})\")\n",
    "    print(f\"\\nResponse:\\n{row['response']}\")\n",
    "    print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deterministic vs. Creative Outputs\n",
    "\n",
    "Different tasks require different levels of creativity. Let's compare approaches for deterministic vs. creative outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for deterministic output\n",
    "deterministic_settings = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"frequency_penalty\": 0.0,\n",
    "    \"presence_penalty\": 0.0,\n",
    "    \"system_prompt\": \"You are a precise, deterministic assistant. Provide consistent, factual responses.\"\n",
    "}\n",
    "\n",
    "# Settings for creative output\n",
    "creative_settings = {\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_p\": 0.9,\n",
    "    \"frequency_penalty\": 0.5,\n",
    "    \"presence_penalty\": 0.5,\n",
    "    \"system_prompt\": \"You are a creative, imaginative assistant. Think outside the box and provide diverse, unique perspectives.\"\n",
    "}\n",
    "\n",
    "# Test prompts for different types of tasks\n",
    "test_prompts = {\n",
    "    \"Factual\": \"What are the key components of a computer's CPU?\",\n",
    "    \"Creative\": \"Imagine a world where humans can communicate with plants. Describe what a day might be like.\"\n",
    "}\n",
    "\n",
    "# Test each prompt with both settings\n",
    "for prompt_type, prompt in test_prompts.items():\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"Testing {prompt_type} prompt: {prompt}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    \n",
    "    # Deterministic response\n",
    "    print(\"\\nDeterministic settings:\")\n",
    "    deterministic_response = call_openrouter(\n",
    "        prompt=prompt,\n",
    "        model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "        temperature=deterministic_settings[\"temperature\"],\n",
    "        top_p=deterministic_settings[\"top_p\"],\n",
    "        frequency_penalty=deterministic_settings[\"frequency_penalty\"],\n",
    "        presence_penalty=deterministic_settings[\"presence_penalty\"],\n",
    "        system_prompt=deterministic_settings[\"system_prompt\"],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    if deterministic_response.get(\"success\", False):\n",
    "        print(extract_text_response(deterministic_response))\n",
    "    else:\n",
    "        print(f\"Error: {deterministic_response.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Creative response\n",
    "    print(\"\\nCreative settings:\")\n",
    "    creative_response = call_openrouter(\n",
    "        prompt=prompt,\n",
    "        model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "        temperature=creative_settings[\"temperature\"],\n",
    "        top_p=creative_settings[\"top_p\"],\n",
    "        frequency_penalty=creative_settings[\"frequency_penalty\"],\n",
    "        presence_penalty=creative_settings[\"presence_penalty\"],\n",
    "        system_prompt=creative_settings[\"system_prompt\"],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    if creative_response.get(\"success\", False):\n",
    "        print(extract_text_response(creative_response))\n",
    "    else:\n",
    "        print(f\"Error: {creative_response.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Parameter Selection Guide\n",
    "\n",
    "Based on our experiments, here's a guide for choosing parameters for different types of tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parameter selection guide as an HTML table with latest 2025 models\n",
    "parameter_guide = \"\"\"\n",
    "<table style=\"width:100%; border-collapse: collapse; border: 1px solid #ddd;\">\n",
    "  <tr style=\"background-color: #f2f2f2;\">\n",
    "    <th style=\"padding: 12px; border: 1px solid #ddd;\">Task Type</th>\n",
    "    <th style=\"padding: 12px; border: 1px solid #ddd;\">Temperature</th>\n",
    "    <th style=\"padding: 12px; border: 1px solid #ddd;\">Top-p</th>\n",
    "    <th style=\"padding: 12px; border: 1px solid #ddd;\">Frequency Penalty</th>\n",
    "    <th style=\"padding: 12px; border: 1px solid #ddd;\">Presence Penalty</th>\n",
    "    <th style=\"padding: 12px; border: 1px solid #ddd;\">System Prompt Focus</th>\n",
    "    <th style=\"padding: 12px; border: 1px solid #ddd;\">Recommended Models (2025)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Factual Q&A</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.0-0.3</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Accuracy, precision, factuality</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 Flash</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #f9f9f9;\">\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Code Generation</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.0-0.2</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.9-1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Code correctness, documentation</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">GPT-4o, DeepSeek R1, Claude 3.5 Sonnet</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Tutorials/Explanations</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.3-0.5</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.1-0.3</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.1-0.3</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Clarity, examples, step-by-step</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Claude 3.5 Sonnet, GPT-4o, Gemini 1.5 Pro</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #f9f9f9;\">\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Creative Writing</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.7-0.9</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.9-1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.5-0.8</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.5-0.8</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Creativity, style, tone</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Brainstorming</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.8-1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.9-1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.8-1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.8-1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Diverse ideas, out-of-box thinking</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 Flash</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #f9f9f9;\">\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Summarization</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.3-0.5</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.9-1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.3-0.5</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.3-0.5</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Conciseness, key points</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Claude 3.5 Haiku, GPT-4o mini, Gemini 1.5 Flash</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Conversational Chat</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.5-0.7</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.9-1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.5</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.5</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Personality, engagement</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Claude 3.5 Haiku, Gemini 2.0 Flash, GPT-4o mini</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #f9f9f9;\">\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Mathematical/Reasoning</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.0-0.2</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Logical reasoning, step-by-step</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">DeepSeek R1, GPT-4o, Claude 3.5 Sonnet</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Long Document Analysis</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.3-0.5</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">1.0</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.2-0.4</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">0.2-0.4</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Comprehensive analysis, context retention</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Gemini 1.5 Pro, Claude 3.5 Sonnet, GPT-4o</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "# Display the guide\n",
    "display(HTML(parameter_guide))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-world Application: Customized Content Generator\n",
    "\n",
    "Let's apply what we've learned to create a customized content generator where users can select parameters based on their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_customized_content(topic, content_type, audience, tone, model=\"openai/gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Generate customized content based on user preferences.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to generate content about\n",
    "        content_type: Type of content (article, social post, email, etc.)\n",
    "        audience: Target audience (beginners, experts, children, etc.)\n",
    "        tone: Desired tone (formal, casual, humorous, etc.)\n",
    "        model: Model to use\n",
    "        \n",
    "    Returns:\n",
    "        Generated content\n",
    "    \"\"\"\n",
    "    # Select parameters based on content type\n",
    "    params = {\n",
    "        \"article\": {\"temperature\": 0.4, \"presence_penalty\": 0.3, \"frequency_penalty\": 0.3},\n",
    "        \"social post\": {\"temperature\": 0.7, \"presence_penalty\": 0.5, \"frequency_penalty\": 0.5},\n",
    "        \"email\": {\"temperature\": 0.3, \"presence_penalty\": 0.2, \"frequency_penalty\": 0.2},\n",
    "        \"tutorial\": {\"temperature\": 0.3, \"presence_penalty\": 0.1, \"frequency_penalty\": 0.1},\n",
    "        \"creative\": {\"temperature\": 0.8, \"presence_penalty\": 0.6, \"frequency_penalty\": 0.6}\n",
    "    }\n",
    "    \n",
    "    # Use default parameters if content type not found\n",
    "    content_params = params.get(content_type.lower(), {\"temperature\": 0.7, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0})\n",
    "    \n",
    "    # Create system prompt based on inputs\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert content creator specializing in {content_type.lower()}s.\n",
    "    Create content about '{topic}' for a {audience.lower()} audience.\n",
    "    Use a {tone.lower()} tone throughout.\n",
    "    Format the content appropriately for a {content_type.lower()}.\n",
    "    Ensure the content is engaging, accurate, and valuable to the reader.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create user prompt\n",
    "    user_prompt = f\"Create a {content_type.lower()} about {topic} for {audience.lower()} with a {tone.lower()} tone.\"\n",
    "    \n",
    "    # Generate content\n",
    "    response = call_openrouter(\n",
    "        prompt=user_prompt,\n",
    "        model=model,\n",
    "        system_prompt=system_prompt,\n",
    "        temperature=content_params[\"temperature\"],\n",
    "        frequency_penalty=content_params[\"frequency_penalty\"],\n",
    "        presence_penalty=content_params[\"presence_penalty\"],\n",
    "        max_tokens=700\n",
    "    )\n",
    "    \n",
    "    if response.get(\"success\", False):\n",
    "        return extract_text_response(response)\n",
    "    else:\n",
    "        return f\"Error: {response.get('error', 'Unknown error')}\"\n",
    "\n",
    "# Example usage\n",
    "topic = \"artificial intelligence ethics\"\n",
    "content_type = \"article\"\n",
    "audience = \"college students\"\n",
    "tone = \"informative\"\n",
    "\n",
    "print(f\"Generating {tone} {content_type} about '{topic}' for {audience}...\\n\")\n",
    "content = generate_customized_content(topic, content_type, audience, tone)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "Here are some exercises to practice what you've learned:\n",
    "\n",
    "1. Create a function that tests different temperature values on the same prompt and calculates a \"creativity score\" based on lexical diversity or uniqueness of the responses\n",
    "\n",
    "2. Design a system prompt optimization tool that tries several variations of a system prompt and evaluates which one better achieves a specific goal\n",
    "\n",
    "3. Implement a function that dynamically adjusts model parameters based on user feedback (e.g., \"make it more creative\" or \"make it more factual\")\n",
    "\n",
    "4. Create a specialized parameter preset for a specific domain (e.g., medical explanations, technical tutorials, or marketing copy) and test it against generic parameters\n",
    "\n",
    "5. Implement a cost optimization function that selects the most cost-effective model for a given task based on performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "- How to compare different models for capability and cost-effectiveness\n",
    "- The impact of temperature on model creativity and determinism\n",
    "- How top-p (nucleus sampling) affects token selection\n",
    "- The powerful effect of system prompts on model behavior\n",
    "- Using frequency and presence penalties to reduce repetition\n",
    "- Parameter selection guidelines for different types of tasks\n",
    "- Building a customized content generator with appropriate parameters\n",
    "\n",
    "By understanding and effectively using these parameters, you can get the most out of LLMs for different use cases, optimizing for quality, cost, or creativity as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
