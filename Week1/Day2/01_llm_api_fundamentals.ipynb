{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced LLM API Techniques\n",
    "\n",
    "**Prerequisites:** Complete Day1 `llm_basics.ipynb` first - this notebook builds on the fundamental concepts covered there.\n",
    "\n",
    "This notebook covers advanced techniques for working with Large Language Model APIs, including:\n",
    "\n",
    "- Streaming responses for better user experience\n",
    "- Exploring available models across providers\n",
    "- Building production-ready applications\n",
    "- Managing complex prompts and conversations\n",
    "- Performance optimization techniques\n",
    "\n",
    "We'll continue using OpenRouter as our API gateway to access models from multiple providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites Check\n",
    "\n",
    "Since this notebook builds on Day1 concepts, let's quickly verify your environment is set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick environment check - assumes Day1 setup completed\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import our API utilities (from Day1 setup)\n",
    "try:\n",
    "    from api_utils import (\n",
    "        call_openrouter,\n",
    "        extract_text_response,\n",
    "        process_streaming_response,\n",
    "        get_available_models\n",
    "    )\n",
    "    print(\"✅ API utilities loaded successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing API utilities: {e}\")\n",
    "    print(\"Please ensure you've completed Day1 setup and api_utils.py is available.\")\n",
    "\n",
    "# Import display utilities for markdown rendering\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Verify API key\n",
    "if os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"✅ API key found!\")\n",
    "else:\n",
    "    print(\"❌ API key not found! Please check your .env file from Day1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Streaming Responses\n",
    "\n",
    "Building on Day1's basic API calls, let's explore streaming - a crucial technique for responsive applications. Streaming allows you to receive responses in chunks as they're generated, rather than waiting for the complete response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import IPython display utilities for interactive streaming\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Make a streaming API call\n",
    "streaming_response = call_openrouter(\n",
    "    prompt=\"Explain quantum computing to a high school student, step by step\",\n",
    "    model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "\n",
    "if streaming_response.get(\"success\", False):\n",
    "    # Process the streaming response\n",
    "    collected_response = \"\"\n",
    "    \n",
    "    print(\"Streaming response:\")\n",
    "    for chunk in process_streaming_response(streaming_response):\n",
    "        collected_response += chunk\n",
    "        # Clear previous output and show the updated response\n",
    "        clear_output(wait=True)\n",
    "        print(\"Streaming response:\")\n",
    "        display(Markdown(collected_response))\n",
    "        # Small delay to make the streaming visible\n",
    "        time.sleep(0.01)\n",
    "else:\n",
    "    print(f\"Error: {streaming_response.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Available Models through OpenRouter\n",
    "\n",
    "OpenRouter provides access to models from multiple providers. Let's explore what's available and understand the ecosystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available models\n",
    "available_models = get_available_models()\n",
    "\n",
    "# Group models by provider for easier viewing\n",
    "models_by_provider = {}\n",
    "for model in available_models:\n",
    "    provider = model.get('id', '').split('/')[0] if '/' in model.get('id', '') else 'unknown'\n",
    "    if provider not in models_by_provider:\n",
    "        models_by_provider[provider] = []\n",
    "    models_by_provider[provider].append(model.get('id'))\n",
    "\n",
    "# Show top providers with sample models\n",
    "top_providers = ['openai', 'anthropic', 'meta-llama', 'google', 'deepseek']\n",
    "\n",
    "# Build formatted output with markdown\n",
    "output_text = \"## Sample models by provider:\\n\\n\"\n",
    "\n",
    "for provider in top_providers:\n",
    "    if provider in models_by_provider:\n",
    "        models = sorted(models_by_provider[provider])[:3]  # Show first 3\n",
    "        output_text += f\"### {provider.upper()}\\n\"\n",
    "        for model in models:\n",
    "            output_text += f\"- `{model}`\\n\"\n",
    "        if len(models_by_provider[provider]) > 3:\n",
    "            output_text += f\"- *... and {len(models_by_provider[provider]) - 3} more models*\\n\"\n",
    "        output_text += \"\\n\"\n",
    "\n",
    "output_text += f\"**Total models available:** {len(available_models)}\\n\\n\"\n",
    "output_text += \"*For advanced model selection and comparison, see 02_model_selection_parameter_tuning.ipynb*\"\n",
    "\n",
    "# Display with simple markdown rendering\n",
    "display(Markdown(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Applications\n",
    "\n",
    "Let's build a simple but complete application that demonstrates practical API usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the QA bot implementation here (from cell 10)\n",
    "def qa_bot(model=\"openai/gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Simple interactive Q&A bot using the specified model.\"\"\"\n",
    "    # Initialize conversation with a system prompt\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that provides clear and concise answers.\"}\n",
    "    ]\n",
    "    \n",
    "    print(f\"🤖 Q&A Bot (using {model})\")\n",
    "    print(\"Type 'exit' to end the conversation.\")\n",
    "    print(\"This demonstrates basic conversation management and API integration.\")\n",
    "    print(\"\\nTry asking: 'What is machine learning?' or 'How do neural networks work?'\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"\\n🤖 Goodbye! Have a great day!\")\n",
    "            break\n",
    "        \n",
    "        # Add user input to conversation\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Call the API\n",
    "        response = call_openrouter(\n",
    "            prompt=conversation,\n",
    "            model=model,\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        if response.get(\"success\", False):\n",
    "            answer = extract_text_response(response)\n",
    "            print(f\"\\nBot:\")\n",
    "            display(Markdown(answer))\n",
    "            \n",
    "            # Add assistant's response to conversation history\n",
    "            conversation.append({\"role\": \"assistant\", \"content\": answer})\n",
    "            \n",
    "            # Keep conversation manageable (basic memory management)\n",
    "            if len(conversation) > 11:  # Keep system + last 10 messages\n",
    "                conversation = [conversation[0]] + conversation[-10:]\n",
    "        else:\n",
    "            print(f\"\\nError: {response.get('error', 'Unknown error')}\")\n",
    "            print(\"For comprehensive error handling patterns, see 04_error_handling_for_api_calls.ipynb\")\n",
    "\n",
    "# Run the interactive bot (uncomment the line below to use)\n",
    "# qa_bot()\n",
    "\n",
    "print(\"💡 Note: To run the interactive Q&A bot, uncomment the line above and run this cell.\")\n",
    "print(\"   The bot uses input() which works best when run interactively in Jupyter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structured Prompt Management\n",
    "\n",
    "For real applications, managing prompts in external files is more maintainable than hardcoding them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and demonstrate structured prompts\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Load sample prompts from JSON file\n",
    "    with open('example_prompts.json', 'r') as f:\n",
    "        sample_prompts = json.load(f)\n",
    "    \n",
    "    print(\"Available prompt categories:\")\n",
    "    for category, prompts in sample_prompts.items():\n",
    "        print(f\"- {category}: {len(prompts)} examples\")\n",
    "    \n",
    "    # Demo: Use a system prompt from the JSON file\n",
    "    if \"system_prompt_examples\" in sample_prompts:\n",
    "        math_tutor = sample_prompts[\"system_prompt_examples\"][1]  # Math tutor\n",
    "        print(f\"\\nUsing system prompt: {math_tutor['title']}\")\n",
    "        \n",
    "        # Test it with a math question\n",
    "        response = call_openrouter(\n",
    "            prompt=\"How do I solve 2x + 5 = 13?\",\n",
    "            model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "            system_prompt=math_tutor['system_prompt'],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        if response.get(\"success\"):\n",
    "            print(f\"\\nMath Tutor Response:\")\n",
    "            display(Markdown(extract_text_response(response)))\n",
    "        else:\n",
    "            print(f\"Error: {response.get('error')}\")\n",
    "    \n",
    "    print(\"\\nFor advanced prompt engineering and optimization, see 02_model_selection_parameter_tuning.ipynb\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"example_prompts.json not found - this demonstrates external prompt management\")\n",
    "    print(\"In production, store prompts in external files for easy updates and version control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hands-On Exercises\n",
    "\n",
    "Practice these fundamental techniques to prepare for advanced notebooks:\n",
    "\n",
    "**Exercise 1**: Implement streaming with different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try streaming responses from different providers (Claude, Gemini, etc.)\n",
    "\n",
    "claude_streaming = call_openrouter(\n",
    "    prompt=\"Compare Python and JavaScript for web development\",\n",
    "    model=\"anthropic/claude-3.5-haiku\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=400,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Process the streaming response and compare the experience\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Streaming Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare streaming vs non-streaming response times and user experience\n",
    "\n",
    "import time\n",
    "\n",
    "def analyze_streaming_performance(prompt, model=\"openai/gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"\n",
    "    Compare streaming vs non-streaming performance for the same prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to test\n",
    "        model: Model to use for testing\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with performance metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Test non-streaming\n",
    "    print(\"Testing non-streaming...\")\n",
    "    start_time = time.time()\n",
    "    non_streaming = call_openrouter(prompt=prompt, model=model, max_tokens=200, stream=False)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if non_streaming.get(\"success\"):\n",
    "        results[\"non_streaming\"] = {\n",
    "            \"total_time\": end_time - start_time,\n",
    "            \"response\": extract_text_response(non_streaming)[:100] + \"...\",\n",
    "            \"time_to_first_token\": end_time - start_time  # All at once\n",
    "        }\n",
    "    \n",
    "    # Test streaming\n",
    "    print(\"Testing streaming...\")\n",
    "    start_time = time.time()\n",
    "    first_token_time = None\n",
    "    streaming = call_openrouter(prompt=prompt, model=model, max_tokens=200, stream=True)\n",
    "    \n",
    "    if streaming.get(\"success\"):\n",
    "        collected = \"\"\n",
    "        for i, chunk in enumerate(process_streaming_response(streaming)):\n",
    "            if i == 0 and first_token_time is None:\n",
    "                first_token_time = time.time()\n",
    "            collected += chunk\n",
    "        end_time = time.time()\n",
    "        \n",
    "        results[\"streaming\"] = {\n",
    "            \"total_time\": end_time - start_time,\n",
    "            \"time_to_first_token\": first_token_time - start_time if first_token_time else 0,\n",
    "            \"response\": collected[:100] + \"...\",\n",
    "            \"perceived_responsiveness\": first_token_time - start_time if first_token_time else 0\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n",
    "# Test the performance analysis:\n",
    "performance_results = analyze_streaming_performance(\n",
    "    \"Write a short story about a robot learning to paint\"\n",
    ")\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "for method, metrics in performance_results.items():\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    print(f\"  Time to first token: {metrics.get('time_to_first_token', 0):.2f}s\")\n",
    "    print(f\"  Total time: {metrics['total_time']:.2f}s\")\n",
    "    print(f\"  Response preview: {metrics['response']}\")\n",
    "\n",
    "# Your task: Run the analysis and discuss when streaming is most beneficial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps\n",
    "\n",
    "In this foundational notebook, we covered the essential API techniques that bridge Day1 concepts with production applications:\n",
    "\n",
    "✅ **Streaming responses** - For responsive user experiences  \n",
    "✅ **Model exploration** - Understanding the available ecosystem  \n",
    "✅ **Interactive applications** - Basic conversation management  \n",
    "✅ **Structured prompts** - External prompt management  \n",
    "✅ **Performance analysis** - Streaming vs non-streaming comparison  \n",
    "✅ **Token awareness** - Basic cost considerations  \n",
    "\n",
    "### Continue Your Learning Journey:\n",
    "\n",
    "**Next recommended notebooks:**\n",
    "- **02_model_selection_parameter_tuning.ipynb** - Advanced model optimization and comparison\n",
    "- **03_token_management.ipynb** - Production-scale cost optimization and memory management  \n",
    "- **04_error_handling_for_api_calls.ipynb** - Robust error handling and resilience patterns\n",
    "- **asyncio_tutorial.ipynb** - Performance optimization with concurrent API calls\n",
    "\n",
    "These fundamentals prepare you for building production-ready LLM applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
