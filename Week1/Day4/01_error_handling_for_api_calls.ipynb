{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Handling for API Calls\n",
    "\n",
    "This notebook covers strategies for handling errors when working with LLM APIs. Topics include:\n",
    "\n",
    "- Rate limit handling and backoff strategies\n",
    "- Timeout management\n",
    "- Fallback options between models\n",
    "- Logging and monitoring API usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('llm_api')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import our custom utilities\n",
    "sys.path.append('.')\n",
    "from api_utils import (\n",
    "    call_openrouter,\n",
    "    extract_text_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our API key is loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the API key is loaded\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"✅ API key loaded successfully!\")\n",
    "    # Show first and last three characters for verification\n",
    "    masked_key = f\"{api_key[:3]}...{api_key[-3:]}\" if len(api_key) > 6 else \"[key too short]\"\n",
    "    print(f\"API key: {masked_key}\")\n",
    "else:\n",
    "    print(\"❌ API key not found! Make sure you've created a .env file with your OPENROUTER_API_KEY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding API Errors\n",
    "\n",
    "Before we implement error handling strategies, let's understand the types of errors we might encounter when working with LLM APIs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Common Error Types\n",
    "\n",
    "| Error Type | HTTP Status | Description | Handling Strategy |\n",
    "|------------|-------------|-------------|-------------------|\n",
    "| Authentication | 401 | Invalid API key or authentication error | Check API key configuration |\n",
    "| Authorization | 403 | Permission denied | Verify account permissions |\n",
    "| Rate Limit | 429 | Too many requests | Implement backoff and retry |\n",
    "| Server Error | 500-599 | Server-side error | Retry with backoff or fallback to another provider |\n",
    "| Invalid Request | 400 | Malformed request | Fix request parameters |\n",
    "| Timeout | N/A | Request took too long | Implement timeouts and retry logic |\n",
    "| Network Error | N/A | Connection problems | Retry with backoff |\n",
    "| Content Filtering | 400/403 | Content violates policy | Handle content filtering edge cases |\n",
    "\n",
    "Let's simulate some of these errors to understand how they look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_error(error_type):\n",
    "    \"\"\"Simulate different API error responses.\"\"\"\n",
    "    if error_type == \"authentication\":\n",
    "        return {\n",
    "            \"error\": {\n",
    "                \"message\": \"Authentication error: invalid API key provided\",\n",
    "                \"type\": \"authentication_error\",\n",
    "                \"code\": 401\n",
    "            }\n",
    "        }\n",
    "    elif error_type == \"rate_limit\":\n",
    "        return {\n",
    "            \"error\": {\n",
    "                \"message\": \"Rate limit exceeded. Please try again in 20s\",\n",
    "                \"type\": \"rate_limit_error\",\n",
    "                \"code\": 429\n",
    "            }\n",
    "        }\n",
    "    elif error_type == \"server\":\n",
    "        return {\n",
    "            \"error\": {\n",
    "                \"message\": \"Server error: The service is currently unavailable\",\n",
    "                \"type\": \"server_error\",\n",
    "                \"code\": 503\n",
    "            }\n",
    "        }\n",
    "    elif error_type == \"timeout\":\n",
    "        # This is typically a client-side error\n",
    "        class TimeoutError(Exception):\n",
    "            pass\n",
    "        raise TimeoutError(\"Request timed out after 60 seconds\")\n",
    "    else:\n",
    "        return {\"error\": {\"message\": \"Unknown error\", \"type\": \"unknown\", \"code\": 400}}\n",
    "\n",
    "# Show example errors\n",
    "for error_type in [\"authentication\", \"rate_limit\", \"server\"]:\n",
    "    print(f\"Example {error_type} error response:\")\n",
    "    print(json.dumps(simulate_error(error_type), indent=2))\n",
    "    print()\n",
    "\n",
    "# Timeout errors are typically raised as exceptions\n",
    "try:\n",
    "    simulate_error(\"timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"Example timeout error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rate Limit Handling\n",
    "\n",
    "Rate limiting is one of the most common issues when working with LLM APIs. Most providers implement rate limits to ensure fair usage and system stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Exponential Backoff\n",
    "\n",
    "A key strategy for handling rate limits is exponential backoff, where each retry waits longer than the previous one. This helps prevent overwheling the API with retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_backoff_retry(func, max_retries=5, initial_delay=1):\n",
    "    \"\"\"Implements a simple exponential backoff retry mechanism.\n",
    "    \n",
    "    Args:\n",
    "        func: The function to retry\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        initial_delay: Initial delay in seconds\n",
    "        \n",
    "    Returns:\n",
    "        The function result or raises the last exception\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    delay = initial_delay\n",
    "    \n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            if retries >= max_retries:\n",
    "                logger.error(f\"Maximum retries ({max_retries}) exceeded. Last error: {e}\")\n",
    "                raise\n",
    "            \n",
    "            # Calculate delay with jitter for distributed load\n",
    "            jitter = random.uniform(0.8, 1.2)\n",
    "            wait_time = delay * jitter\n",
    "            \n",
    "            logger.info(f\"Retry {retries}/{max_retries} after error: {e}. Waiting {wait_time:.2f}s\")\n",
    "            time.sleep(wait_time)\n",
    "            \n",
    "            # Exponential increase for next retry\n",
    "            delay *= 2\n",
    "\n",
    "# Simulate a function that occasionally fails with rate limit errors\n",
    "def simulate_api_call(fail_probability=0.6):\n",
    "    \"\"\"Simulate an API call that might fail with rate limits.\"\"\"\n",
    "    if random.random() < fail_probability:\n",
    "        # Simulate a rate limit error\n",
    "        logger.warning(\"API call hit rate limit!\")\n",
    "        raise Exception(\"Rate limit exceeded\")\n",
    "    \n",
    "    logger.info(\"API call succeeded\")\n",
    "    return {\"success\": True, \"data\": \"This is a simulated response\"}\n",
    "\n",
    "# Test our backoff strategy\n",
    "try:\n",
    "    result = simple_backoff_retry(simulate_api_call, max_retries=5, initial_delay=0.5)\n",
    "    print(f\"Final result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"All retries failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using Tenacity for Robust Retries\n",
    "\n",
    "Instead of implementing our own retry logic, we can use the `tenacity` library which provides robust retry capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom retry strategy using tenacity\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=60),\n",
    "    retry=retry_if_exception_type((requests.exceptions.RequestException, ConnectionError, TimeoutError)),\n",
    "    reraise=True\n",
    ")\n",
    "def api_call_with_retry(prompt, model=\"openai/gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Make an API call with automatic retries.\"\"\"\n",
    "    # In a real application, we might want to inspect the exception type\n",
    "    # to determine if it's a retryable error\n",
    "    response = call_openrouter(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Check if the API returned an error instead of raising an exception\n",
    "    if not response.get(\"success\", False):\n",
    "        error = response.get(\"error\", \"Unknown error\")\n",
    "        # Simulate raising an exception for certain error types\n",
    "        if \"rate limit\" in str(error).lower():\n",
    "            raise requests.exceptions.RequestException(f\"Rate limit error: {error}\")\n",
    "        elif \"timeout\" in str(error).lower():\n",
    "            raise TimeoutError(f\"Timeout error: {error}\")\n",
    "        elif \"server\" in str(error).lower():\n",
    "            raise ConnectionError(f\"Server error: {error}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Let's test the retry-enabled API call\n",
    "# Note: This won't actually fail in normal use, as we're using a valid API key\n",
    "try:\n",
    "    response = api_call_with_retry(\"What is the capital of France?\")\n",
    "    if response.get(\"success\", False):\n",
    "        print(\"API call succeeded!\")\n",
    "        print(f\"Response: {extract_text_response(response)}\")\n",
    "    else:\n",
    "        print(f\"API call failed: {response.get('error', 'Unknown error')}\")\n",
    "except Exception as e:\n",
    "    print(f\"API call failed after all retries: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Request Rate Limiting\n",
    "\n",
    "In addition to handling rate limit errors, we can proactively prevent them by implementing client-side rate limiting. This ensures we don't exceed the API's rate limits in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    \"\"\"A simple rate limiter that enforces a maximum number of requests per minute.\"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_minute):\n",
    "        \"\"\"Initialize the rate limiter.\n",
    "        \n",
    "        Args:\n",
    "            requests_per_minute: Maximum requests per minute\n",
    "        \"\"\"\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.min_interval = 60.0 / requests_per_minute  # Minimum interval in seconds\n",
    "        self.last_request_time = 0\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        \"\"\"Wait if necessary to comply with the rate limit.\"\"\"\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.last_request_time\n",
    "        \n",
    "        if elapsed < self.min_interval:\n",
    "            wait_time = self.min_interval - elapsed\n",
    "            logger.info(f\"Rate limiting: Waiting {wait_time:.2f}s before next request\")\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "# Create a rate limiter that allows 6 requests per minute\n",
    "limiter = RateLimiter(requests_per_minute=6)\n",
    "\n",
    "def rate_limited_api_call(prompt, model=\"openai/gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Make an API call with rate limiting.\"\"\"\n",
    "    # Wait if needed to respect rate limits\n",
    "    limiter.wait_if_needed()\n",
    "    \n",
    "    # Make the API call\n",
    "    return call_openrouter(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "# Test the rate limiter with multiple requests\n",
    "test_prompts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain machine learning in simple terms\",\n",
    "    \"What is deep learning?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    start_time = time.time()\n",
    "    response = rate_limited_api_call(prompt)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if response.get(\"success\", False):\n",
    "        print(f\"Request for '{prompt}' took {end_time - start_time:.2f}s\")\n",
    "    else:\n",
    "        print(f\"Request for '{prompt}' failed: {response.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Timeout Management\n",
    "\n",
    "API calls can sometimes take longer than expected, especially for complex prompts or during high traffic periods. Let's implement proper timeout handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_with_timeout(prompt, model=\"openai/gpt-4o-mini-2024-07-18\", timeout=10):\n",
    "    \"\"\"Make an API call with a specified timeout.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        model: The model to use\n",
    "        timeout: Timeout in seconds\n",
    "        \n",
    "    Returns:\n",
    "        The API response or an error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Here we'd typically use requests.post() with a timeout parameter\n",
    "        # For our example, we'll use our utility function call_openrouter\n",
    "        # Since we don't have direct access to set the timeout, we'll simulate it\n",
    "        \n",
    "        # In a real implementation, you'd do something like this:\n",
    "        # response = requests.post(\n",
    "        #    \"https://openrouter.ai/api/v1/chat/completions\", \n",
    "        #    json=payload, \n",
    "        #    headers=headers, \n",
    "        #    timeout=timeout\n",
    "        # )\n",
    "        \n",
    "        # Simulate potential timeout\n",
    "        process_time = timeout * 0.8  # Simulate taking 80% of the timeout time\n",
    "        print(f\"API call will take {process_time:.2f}s (timeout is {timeout}s)\")\n",
    "        \n",
    "        if process_time > timeout:\n",
    "            raise requests.exceptions.Timeout(f\"Request timed out after {timeout}s\")\n",
    "        \n",
    "        return call_openrouter(\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            temperature=0.7,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "    except requests.exceptions.Timeout as e:\n",
    "        logger.error(f\"Request timed out: {e}\")\n",
    "        return {\"success\": False, \"error\": f\"Timeout error: {e}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during API call: {e}\")\n",
    "        return {\"success\": False, \"error\": f\"Error: {e}\"}\n",
    "\n",
    "# Test with different timeout values\n",
    "timeouts = [5, 15, 30]  # seconds\n",
    "for timeout in timeouts:\n",
    "    print(f\"\\nTrying with {timeout}s timeout:\")\n",
    "    response = call_with_timeout(\"Explain quantum computing briefly\", timeout=timeout)\n",
    "    \n",
    "    if response.get(\"success\", False):\n",
    "        print(f\"Success! Response: {extract_text_response(response)[:100]}...\")\n",
    "    else:\n",
    "        print(f\"Failed: {response.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Adaptive Timeouts\n",
    "\n",
    "Instead of using fixed timeouts, we can implement adaptive timeouts based on the complexity of the request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adaptive_timeout(prompt, model=\"openai/gpt-4o-mini-2024-07-18\", base_timeout=10):\n",
    "    \"\"\"Calculate an adaptive timeout based on prompt complexity.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        model: The model to use\n",
    "        base_timeout: Base timeout in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Calculated timeout in seconds\n",
    "    \"\"\"\n",
    "    # Start with the base timeout\n",
    "    timeout = base_timeout\n",
    "    \n",
    "    # Adjust based on prompt length (token count would be better)\n",
    "    prompt_length = len(prompt) if isinstance(prompt, str) else len(json.dumps(prompt))\n",
    "    \n",
    "    # Add 1 second for every 100 characters in the prompt\n",
    "    timeout += (prompt_length / 100)\n",
    "    \n",
    "    # Adjust based on model (more complex models might take longer)\n",
    "    if \"opus\" in model or \"gpt-4\" in model:\n",
    "        timeout *= 1.5  # 50% more time for complex models\n",
    "    elif \"mini\" in model or \"haiku\" in model:\n",
    "        timeout *= 0.8  # 20% less time for smaller models\n",
    "    \n",
    "    # Add a small random factor (±10%) to prevent thundering herd problem\n",
    "    jitter = random.uniform(0.9, 1.1)\n",
    "    timeout *= jitter\n",
    "    \n",
    "    # Ensure a minimum timeout\n",
    "    return max(5, timeout)\n",
    "\n",
    "# Test the adaptive timeout calculation\n",
    "test_cases = [\n",
    "    {\"prompt\": \"Hello\", \"model\": \"openai/gpt-4o-mini-2024-07-18\"},\n",
    "    {\"prompt\": \"Explain the theory of relativity in detail\", \"model\": \"openai/gpt-4o-mini-2024-07-18\"},\n",
    "    {\"prompt\": \"Write a 500 word essay on climate change\" * 5, \"model\": \"openai/gpt-4o-2024-08-06\"},\n",
    "    {\"prompt\": \"Summarize this book chapter\" + \"A\" * 2000, \"model\": \"anthropic/claude-3-opus-20240229\"}\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    timeout = calculate_adaptive_timeout(case[\"prompt\"], case[\"model\"])\n",
    "    print(f\"Prompt: '{case['prompt'][:30]}...' ({len(case['prompt'])} chars)\")\n",
    "    print(f\"Model: {case['model']}\")\n",
    "    print(f\"Calculated timeout: {timeout:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fallback Options\n",
    "\n",
    "When one model or provider fails, having fallback options can ensure your application remains operational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model Fallback\n",
    "\n",
    "Let's implement a fallback chain that tries multiple models in sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_with_model_fallback(prompt, models=None, max_attempts=3):\n",
    "    \"\"\"Try multiple models in sequence until one succeeds.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        models: List of models to try in order\n",
    "        max_attempts: Maximum attempts per model\n",
    "        \n",
    "    Returns:\n",
    "        The successful response or the last error\n",
    "    \"\"\"\n",
    "    if models is None:\n",
    "        # Default fallback chain from most capable to least capable\n",
    "        models = [\n",
    "            \"openai/gpt-4o-2024-08-06\",  # Primary model\n",
    "            \"openai/gpt-4o-mini-2024-07-18\",  # Fallback 1\n",
    "            \"anthropic/claude-3-haiku-20240307\",  # Fallback 2\n",
    "            \"google/gemini-2.5-flash\"  # Fallback 3\n",
    "        ]\n",
    "    \n",
    "    last_error = None\n",
    "    for model in models:\n",
    "        logger.info(f\"Trying model: {model}\")\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                response = call_openrouter(\n",
    "                    prompt=prompt,\n",
    "                    model=model,\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=100\n",
    "                )\n",
    "                \n",
    "                if response.get(\"success\", False):\n",
    "                    logger.info(f\"Successfully used model {model} on attempt {attempt + 1}\")\n",
    "                    return response\n",
    "                else:\n",
    "                    error = response.get(\"error\", \"Unknown error\")\n",
    "                    last_error = error\n",
    "                    logger.warning(f\"Model {model} failed (attempt {attempt + 1}): {error}\")\n",
    "                    \n",
    "                    # For certain errors, immediately try the next model\n",
    "                    if any(err in str(error).lower() for err in [\n",
    "                        \"model not found\", \"not available\", \"not supported\", \"invalid model\"\n",
    "                    ]):\n",
    "                        logger.info(f\"Model {model} is not available. Trying next model.\")\n",
    "                        break\n",
    "                    \n",
    "                    # For rate limits, wait before retrying\n",
    "                    if \"rate limit\" in str(error).lower():\n",
    "                        wait_time = (attempt + 1) * 2  # Progressive backoff\n",
    "                        logger.info(f\"Rate limited. Waiting {wait_time}s before retry.\")\n",
    "                        time.sleep(wait_time)\n",
    "            \n",
    "            except Exception as e:\n",
    "                last_error = str(e)\n",
    "                logger.error(f\"Error with model {model} (attempt {attempt + 1}): {e}\")\n",
    "    \n",
    "    # If we've exhausted all models, return the last error\n",
    "    logger.error(f\"All models failed. Last error: {last_error}\")\n",
    "    return {\"success\": False, \"error\": f\"All models failed. Last error: {last_error}\"}\n",
    "\n",
    "# Test the fallback chain\n",
    "# For this test, let's include a non-existent model at the start to force fallback\n",
    "test_fallback_chain = [\n",
    "    \"non-existent-model/gpt-99\",  # This will fail, forcing fallback\n",
    "    \"openai/gpt-4o-mini-2024-07-18\",  # This should work if available\n",
    "    \"anthropic/claude-3-haiku-20240307\"\n",
    "]\n",
    "\n",
    "response = call_with_model_fallback(\n",
    "    \"What's the best way to learn programming?\", \n",
    "    models=test_fallback_chain\n",
    ")\n",
    "\n",
    "if response.get(\"success\", False):\n",
    "    print(f\"Successfully got response using fallback: {extract_text_response(response)[:100]}...\")\n",
    "    print(f\"Model used: {response.get('model', 'Unknown')}\")\n",
    "else:\n",
    "    print(f\"All fallbacks failed: {response.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Parameter Fallbacks\n",
    "\n",
    "In addition to model fallbacks, we can also implement parameter fallbacks for cases where a request fails due to parameter issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_with_parameter_fallback(prompt, model=\"openai/gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Try different parameter configurations if the initial call fails.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        model: The model to use\n",
    "        \n",
    "    Returns:\n",
    "        The successful response or the last error\n",
    "    \"\"\"\n",
    "    # Initial parameters\n",
    "    initial_params = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 500,\n",
    "        \"top_p\": 1.0\n",
    "    }\n",
    "    \n",
    "    # Fallback parameter sets to try if the initial one fails\n",
    "    fallback_params = [\n",
    "        # Conservative fallback - reduce output size, lower temperature\n",
    "        {\"temperature\": 0.3, \"max_tokens\": 200, \"top_p\": 0.9},\n",
    "        # Minimal fallback - highly constrained\n",
    "        {\"temperature\": 0.0, \"max_tokens\": 100, \"top_p\": 0.5}\n",
    "    ]\n",
    "    \n",
    "    # Try the initial parameters\n",
    "    logger.info(f\"Trying initial parameters: {initial_params}\")\n",
    "    response = call_openrouter(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        **initial_params\n",
    "    )\n",
    "    \n",
    "    if response.get(\"success\", False):\n",
    "        return response\n",
    "    \n",
    "    # If initial call failed, try fallbacks\n",
    "    last_error = response.get(\"error\", \"Unknown error\")\n",
    "    for i, params in enumerate(fallback_params):\n",
    "        logger.info(f\"Trying fallback parameters {i+1}: {params}\")\n",
    "        \n",
    "        response = call_openrouter(\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        if response.get(\"success\", False):\n",
    "            logger.info(f\"Succeeded with fallback parameters {i+1}\")\n",
    "            return response\n",
    "        \n",
    "        last_error = response.get(\"error\", \"Unknown error\")\n",
    "    \n",
    "    # If all parameter sets failed, return the last error\n",
    "    logger.error(f\"All parameter sets failed. Last error: {last_error}\")\n",
    "    return {\"success\": False, \"error\": f\"All parameter sets failed. Last error: {last_error}\"}\n",
    "\n",
    "# Test the parameter fallback\n",
    "# For a real test, we'd need to simulate parameter failures,\n",
    "# but we'll just demonstrate the concept\n",
    "response = call_with_parameter_fallback(\n",
    "    \"Explain the concept of neural networks in simple terms\"\n",
    ")\n",
    "\n",
    "if response.get(\"success\", False):\n",
    "    print(f\"Successfully got response: {extract_text_response(response)[:100]}...\")\n",
    "else:\n",
    "    print(f\"All parameter sets failed: {response.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Logging and Monitoring\n",
    "\n",
    "Proper logging and monitoring are critical for tracking API usage, errors, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Comprehensive Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APILogger:\n",
    "    \"\"\"A class for comprehensive API call logging.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_file=None):\n",
    "        \"\"\"Initialize the logger.\n",
    "        \n",
    "        Args:\n",
    "            log_file: Path to the log file. If None, logs to console only.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger('api_logger')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Add a console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        console_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        console_handler.setFormatter(console_format)\n",
    "        self.logger.addHandler(console_handler)\n",
    "        \n",
    "        # Add a file handler if a log file is specified\n",
    "        if log_file:\n",
    "            file_handler = logging.FileHandler(log_file)\n",
    "            file_handler.setLevel(logging.DEBUG)  # Log everything to file\n",
    "            file_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            file_handler.setFormatter(file_format)\n",
    "            self.logger.addHandler(file_handler)\n",
    "        \n",
    "        # Keep track of API call statistics\n",
    "        self.stats = {\n",
    "            \"total_calls\": 0,\n",
    "            \"successful_calls\": 0,\n",
    "            \"failed_calls\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"call_durations\": [],\n",
    "            \"errors\": {}\n",
    "        }\n",
    "    \n",
    "    def log_api_call(self, model, prompt, response, start_time, end_time):\n",
    "        \"\"\"Log an API call with detailed information.\n",
    "        \n",
    "        Args:\n",
    "            model: The model used\n",
    "            prompt: The prompt sent\n",
    "            response: The API response\n",
    "            start_time: Start time of the call\n",
    "            end_time: End time of the call\n",
    "        \"\"\"\n",
    "        duration = end_time - start_time\n",
    "        success = response.get(\"success\", False)\n",
    "        \n",
    "        # Basic log entry\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": model,\n",
    "            \"success\": success,\n",
    "            \"duration\": duration,\n",
    "            \"prompt_length\": len(str(prompt))\n",
    "        }\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats[\"total_calls\"] += 1\n",
    "        self.stats[\"call_durations\"].append(duration)\n",
    "        \n",
    "        if success:\n",
    "            self.stats[\"successful_calls\"] += 1\n",
    "            response_text = extract_text_response(response)\n",
    "            log_entry[\"response_length\"] = len(response_text)\n",
    "            \n",
    "            # Use abbreviated response to avoid huge log entries\n",
    "            abbreviated_response = response_text[:100] + \"...\" if len(response_text) > 100 else response_text\n",
    "            self.logger.info(\n",
    "                f\"API call successful | Model: {model} | Duration: {duration:.2f}s | \"\n",
    "                f\"Response: {abbreviated_response}\"\n",
    "            )\n",
    "        else:\n",
    "            self.stats[\"failed_calls\"] += 1\n",
    "            error = response.get(\"error\", \"Unknown error\")\n",
    "            log_entry[\"error\"] = error\n",
    "            \n",
    "            # Track error types\n",
    "            error_type = \"unknown\"\n",
    "            if \"rate limit\" in str(error).lower():\n",
    "                error_type = \"rate_limit\"\n",
    "            elif \"timeout\" in str(error).lower():\n",
    "                error_type = \"timeout\"\n",
    "            elif \"auth\" in str(error).lower():\n",
    "                error_type = \"authentication\"\n",
    "            \n",
    "            self.stats[\"errors\"][error_type] = self.stats[\"errors\"].get(error_type, 0) + 1\n",
    "            self.logger.warning(\n",
    "                f\"API call failed | Model: {model} | Duration: {duration:.2f}s | \"\n",
    "                f\"Error: {error}\"\n",
    "            )\n",
    "        \n",
    "        return log_entry\n",
    "    \n",
    "    def get_stats_summary(self):\n",
    "        \"\"\"Get a summary of API call statistics.\"\"\"\n",
    "        if not self.stats[\"total_calls\"]:\n",
    "            return \"No API calls logged yet.\"\n",
    "        \n",
    "        success_rate = (self.stats[\"successful_calls\"] / self.stats[\"total_calls\"]) * 100\n",
    "        avg_duration = sum(self.stats[\"call_durations\"]) / len(self.stats[\"call_durations\"])\n",
    "        \n",
    "        summary = f\"\"\"API Call Statistics:  \n",
    "        Total Calls: {self.stats['total_calls']}  \n",
    "        Success Rate: {success_rate:.1f}%  \n",
    "        Average Duration: {avg_duration:.2f}s  \n",
    "        Failed Calls: {self.stats['failed_calls']}  \n",
    "        Error Types: {self.stats['errors']}  \n",
    "        \"\"\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create a logger\n",
    "api_logger = APILogger(log_file=\"api_calls.log\")\n",
    "\n",
    "def logged_api_call(prompt, model=\"openai/gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Make an API call with comprehensive logging.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = call_openrouter(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Log the API call\n",
    "    api_logger.log_api_call(model, prompt, response, start_time, end_time)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the logged API call\n",
    "for _ in range(3):\n",
    "    logged_api_call(\"What's your favorite color?\")\n",
    "\n",
    "# Print the stats summary\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "print(api_logger.get_stats_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Cost and Usage Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsageTracker:\n",
    "    \"\"\"Track API usage, costs, and quotas.\"\"\"\n",
    "    \n",
    "    def __init__(self, daily_budget=10.0):\n",
    "        \"\"\"Initialize the usage tracker.\n",
    "        \n",
    "        Args:\n",
    "            daily_budget: Daily budget in USD\n",
    "        \"\"\"\n",
    "        self.daily_budget = daily_budget\n",
    "        self.usage = {\n",
    "            \"total_cost\": 0.0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"calls_by_model\": {},\n",
    "            \"costs_by_model\": {},\n",
    "            \"start_time\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def track_usage(self, model, input_tokens, output_tokens, success=True):\n",
    "        \"\"\"Track usage for an API call.\n",
    "        \n",
    "        Args:\n",
    "            model: The model used\n",
    "            input_tokens: Number of input tokens\n",
    "            output_tokens: Number of output tokens\n",
    "            success: Whether the call was successful\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary with usage information\n",
    "        \"\"\"\n",
    "        # Initialize model tracking if not exists\n",
    "        if model not in self.usage[\"calls_by_model\"]:\n",
    "            self.usage[\"calls_by_model\"][model] = 0\n",
    "            self.usage[\"costs_by_model\"][model] = 0.0\n",
    "        \n",
    "        # Update call count\n",
    "        self.usage[\"calls_by_model\"][model] += 1\n",
    "        \n",
    "        # Only track tokens and costs for successful calls\n",
    "        if success:\n",
    "            # Import the cost estimation function\n",
    "            from api_utils import estimate_cost\n",
    "            \n",
    "            # Calculate cost\n",
    "            cost = estimate_cost(model, input_tokens, output_tokens)\n",
    "            \n",
    "            # Update cost tracking\n",
    "            self.usage[\"total_cost\"] += cost\n",
    "            self.usage[\"costs_by_model\"][model] += cost\n",
    "            \n",
    "            # Update token tracking\n",
    "            self.usage[\"total_tokens\"] += input_tokens + output_tokens\n",
    "        \n",
    "        # Check if we're approaching the budget\n",
    "        budget_percent = (self.usage[\"total_cost\"] / self.daily_budget) * 100\n",
    "        if budget_percent >= 80:\n",
    "            logger.warning(f\"WARNING: {budget_percent:.1f}% of daily budget used (${self.usage['total_cost']:.2f}/{self.daily_budget:.2f})\")\n",
    "        \n",
    "        return {\n",
    "            \"cost\": cost if success else 0.0,\n",
    "            \"total_cost\": self.usage[\"total_cost\"],\n",
    "            \"budget_percent\": budget_percent,\n",
    "            \"model\": model,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens\n",
    "        }\n",
    "    \n",
    "    def get_usage_report(self):\n",
    "        \"\"\"Get a detailed usage report.\"\"\"\n",
    "        # Calculate time period\n",
    "        start_time = datetime.fromisoformat(self.usage[\"start_time\"])\n",
    "        current_time = datetime.now()\n",
    "        duration = current_time - start_time\n",
    "        \n",
    "        # Create the report\n",
    "        report = f\"\"\"\\nUsage Report ({start_time.strftime('%Y-%m-%d %H:%M')} to {current_time.strftime('%Y-%m-%d %H:%M')}, {duration.total_seconds()/3600:.1f} hours):\n",
    "        \n",
    "        Total Cost: ${self.usage['total_cost']:.4f}\n",
    "        Budget Used: {(self.usage['total_cost'] / self.daily_budget) * 100:.1f}% of ${self.daily_budget:.2f}\n",
    "        Total Tokens: {self.usage['total_tokens']:,}\n",
    "        \n",
    "        Usage by Model:\n",
    "        \"\"\"\n",
    "        \n",
    "        for model in self.usage[\"calls_by_model\"]:\n",
    "            calls = self.usage[\"calls_by_model\"][model]\n",
    "            cost = self.usage[\"costs_by_model\"][model]\n",
    "            report += f\"        {model}: {calls} calls, ${cost:.4f}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Create a usage tracker\n",
    "usage_tracker = UsageTracker(daily_budget=5.0)\n",
    "\n",
    "def tracked_api_call(prompt, model=\"openai/gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Make an API call with usage tracking.\"\"\"\n",
    "    # Count input tokens\n",
    "    from token_counter import count_tokens\n",
    "    input_tokens = count_tokens(prompt if isinstance(prompt, str) else json.dumps(prompt))\n",
    "    \n",
    "    # Make the API call\n",
    "    response = call_openrouter(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Track usage\n",
    "    if response.get(\"success\", False):\n",
    "        output_text = extract_text_response(response)\n",
    "        output_tokens = count_tokens(output_text)\n",
    "        \n",
    "        usage_info = usage_tracker.track_usage(\n",
    "            model=model,\n",
    "            input_tokens=input_tokens,\n",
    "            output_tokens=output_tokens,\n",
    "            success=True\n",
    "        )\n",
    "        \n",
    "        # Add usage info to the response\n",
    "        response[\"usage_info\"] = usage_info\n",
    "    else:\n",
    "        # Track failed call\n",
    "        usage_tracker.track_usage(\n",
    "            model=model,\n",
    "            input_tokens=input_tokens,\n",
    "            output_tokens=0,\n",
    "            success=False\n",
    "        )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test with multiple models to see usage tracking\n",
    "test_models = [\n",
    "    \"openai/gpt-4o-mini-2024-07-18\",\n",
    "    \"openai/gpt-4o-mini-2024-07-18\",\n",
    "    \"anthropic/claude-3-haiku-20240307\"\n",
    "]\n",
    "\n",
    "for model in test_models:\n",
    "    response = tracked_api_call(f\"Tell me a fun fact about {random.choice(['cats', 'dogs', 'birds', 'space'])}\", model=model)\n",
    "    \n",
    "    if response.get(\"success\", False):\n",
    "        usage_info = response.get(\"usage_info\", {})\n",
    "        print(f\"API call to {model} cost: ${usage_info.get('cost', 0):.6f}\")\n",
    "        print(f\"Response: {extract_text_response(response)[:100]}...\\n\")\n",
    "\n",
    "# Print the usage report\n",
    "print(usage_tracker.get_usage_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating a Resilient API Client\n",
    "\n",
    "Now, let's combine everything we've learned to create a comprehensive, resilient API client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResilientLLMClient:\n",
    "    \"\"\"A comprehensive, resilient client for LLM API calls.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 primary_model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "                 fallback_models=None,\n",
    "                 base_timeout=15,\n",
    "                 max_retries=3,\n",
    "                 log_file=\"llm_api.log\",\n",
    "                 daily_budget=10.0,\n",
    "                 requests_per_minute=10):\n",
    "        \"\"\"Initialize the resilient client.\n",
    "        \n",
    "        Args:\n",
    "            primary_model: The primary model to use\n",
    "            fallback_models: List of fallback models\n",
    "            base_timeout: Base timeout in seconds\n",
    "            max_retries: Maximum retry attempts\n",
    "            log_file: Path to the log file\n",
    "            daily_budget: Daily budget in USD\n",
    "            requests_per_minute: Maximum requests per minute\n",
    "        \"\"\"\n",
    "        # Model configuration\n",
    "        self.primary_model = primary_model\n",
    "        self.fallback_models = fallback_models or [\n",
    "            \"openai/gpt-4o-mini-2024-07-18\",\n",
    "            \"anthropic/claude-3-haiku-20240307\",\n",
    "            \"google/gemini-2.5-flash\"\n",
    "        ]\n",
    "        \n",
    "        # Timeouts and retries\n",
    "        self.base_timeout = base_timeout\n",
    "        self.max_retries = max_retries\n",
    "        \n",
    "        # Set up logging\n",
    "        self.logger = APILogger(log_file=log_file)\n",
    "        \n",
    "        # Set up usage tracking\n",
    "        self.usage_tracker = UsageTracker(daily_budget=daily_budget)\n",
    "        \n",
    "        # Set up rate limiting\n",
    "        self.rate_limiter = RateLimiter(requests_per_minute=requests_per_minute)\n",
    "        \n",
    "        # Cache for responses (simple in-memory cache)\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Call statistics\n",
    "        self.call_count = 0\n",
    "        self.error_count = 0\n",
    "        self.cache_hits = 0\n",
    "        \n",
    "        logging.info(f\"Initialized ResilientLLMClient with primary model: {primary_model}\")\n",
    "    \n",
    "    def generate_cache_key(self, prompt, model, temperature, max_tokens):\n",
    "        \"\"\"Generate a cache key for a request.\"\"\"\n",
    "        # Convert prompt to string if it's not already\n",
    "        prompt_str = json.dumps(prompt) if not isinstance(prompt, str) else prompt\n",
    "        \n",
    "        # Create a unique key based on the request parameters\n",
    "        key_parts = [\n",
    "            prompt_str,\n",
    "            model,\n",
    "            str(temperature),\n",
    "            str(max_tokens)\n",
    "        ]\n",
    "        \n",
    "        # Use a hash for the key\n",
    "        import hashlib\n",
    "        return hashlib.md5(\"|\".join(key_parts).encode()).hexdigest()\n",
    "    \n",
    "    def call(self, prompt, model=None, temperature=0.7, max_tokens=300, \n",
    "             system_prompt=None, use_cache=True, timeout=None):\n",
    "        \"\"\"Make a resilient API call with comprehensive error handling.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to send\n",
    "            model: The model to use (defaults to primary_model)\n",
    "            temperature: Temperature parameter\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            system_prompt: Optional system prompt\n",
    "            use_cache: Whether to use caching\n",
    "            timeout: Custom timeout in seconds\n",
    "            \n",
    "        Returns:\n",
    "            The API response\n",
    "        \"\"\"\n",
    "        self.call_count += 1\n",
    "        model = model or self.primary_model\n",
    "        \n",
    "        # Calculate adaptive timeout if not specified\n",
    "        if timeout is None:\n",
    "            timeout = calculate_adaptive_timeout(prompt, model, self.base_timeout)\n",
    "        \n",
    "        # Check if we have a cached response\n",
    "        if use_cache:\n",
    "            cache_key = self.generate_cache_key(prompt, model, temperature, max_tokens)\n",
    "            if cache_key in self.cache:\n",
    "                self.cache_hits += 1\n",
    "                logging.info(f\"Cache hit for prompt: {prompt[:30]}...\")\n",
    "                return self.cache[cache_key]\n",
    "        \n",
    "        # Apply rate limiting\n",
    "        self.rate_limiter.wait_if_needed()\n",
    "        \n",
    "        # Count input tokens for tracking\n",
    "        from token_counter import count_tokens, count_message_tokens\n",
    "        if isinstance(prompt, list):\n",
    "            # It's a message list\n",
    "            input_tokens = count_message_tokens(prompt)\n",
    "        else:\n",
    "            # It's a string prompt\n",
    "            if system_prompt:\n",
    "                # Account for system prompt\n",
    "                input_tokens = count_tokens(prompt) + count_tokens(system_prompt) + 10  # 10 extra for message format\n",
    "            else:\n",
    "                input_tokens = count_tokens(prompt)\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Try the primary model first\n",
    "        try:\n",
    "            response = call_openrouter(\n",
    "                prompt=prompt,\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                system_prompt=system_prompt\n",
    "            )\n",
    "            \n",
    "            # Process and log the response\n",
    "            end_time = time.time()\n",
    "            self.logger.log_api_call(model, prompt, response, start_time, end_time)\n",
    "            \n",
    "            # If successful, track usage and cache the response\n",
    "            if response.get(\"success\", False):\n",
    "                output_text = extract_text_response(response)\n",
    "                output_tokens = count_tokens(output_text)\n",
    "                \n",
    "                # Track usage\n",
    "                usage_info = self.usage_tracker.track_usage(\n",
    "                    model=model,\n",
    "                    input_tokens=input_tokens,\n",
    "                    output_tokens=output_tokens,\n",
    "                    success=True\n",
    "                )\n",
    "                \n",
    "                response[\"usage_info\"] = usage_info\n",
    "                \n",
    "                # Cache the response if requested\n",
    "                if use_cache:\n",
    "                    cache_key = self.generate_cache_key(prompt, model, temperature, max_tokens)\n",
    "                    self.cache[cache_key] = response\n",
    "                \n",
    "                return response\n",
    "            else:\n",
    "                # If the primary model failed, try fallbacks\n",
    "                self.error_count += 1\n",
    "                return self._try_fallbacks(\n",
    "                    prompt=prompt, \n",
    "                    temperature=temperature, \n",
    "                    max_tokens=max_tokens,\n",
    "                    system_prompt=system_prompt,\n",
    "                    input_tokens=input_tokens\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle exceptions and try fallbacks\n",
    "            self.error_count += 1\n",
    "            logging.error(f\"Error with primary model {model}: {e}\")\n",
    "            \n",
    "            return self._try_fallbacks(\n",
    "                prompt=prompt, \n",
    "                temperature=temperature, \n",
    "                max_tokens=max_tokens,\n",
    "                system_prompt=system_prompt,\n",
    "                input_tokens=input_tokens\n",
    "            )\n",
    "    \n",
    "    def _try_fallbacks(self, prompt, temperature, max_tokens, system_prompt, input_tokens):\n",
    "        \"\"\"Try fallback models if the primary model fails.\"\"\"\n",
    "        for model in self.fallback_models:\n",
    "            logging.info(f\"Trying fallback model: {model}\")\n",
    "            \n",
    "            # Apply rate limiting\n",
    "            self.rate_limiter.wait_if_needed()\n",
    "            \n",
    "            # Start timing\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                response = call_openrouter(\n",
    "                    prompt=prompt,\n",
    "                    model=model,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    system_prompt=system_prompt\n",
    "                )\n",
    "                \n",
    "                # Process and log the response\n",
    "                end_time = time.time()\n",
    "                self.logger.log_api_call(model, prompt, response, start_time, end_time)\n",
    "                \n",
    "                if response.get(\"success\", False):\n",
    "                    logging.info(f\"Fallback to {model} successful\")\n",
    "                    \n",
    "                    # Track usage\n",
    "                    output_text = extract_text_response(response)\n",
    "                    from token_counter import count_tokens\n",
    "                    output_tokens = count_tokens(output_text)\n",
    "                    \n",
    "                    usage_info = self.usage_tracker.track_usage(\n",
    "                        model=model,\n",
    "                        input_tokens=input_tokens,\n",
    "                        output_tokens=output_tokens,\n",
    "                        success=True\n",
    "                    )\n",
    "                    \n",
    "                    response[\"usage_info\"] = usage_info\n",
    "                    response[\"used_fallback\"] = True\n",
    "                    \n",
    "                    return response\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error with fallback model {model}: {e}\")\n",
    "        \n",
    "        # If all fallbacks failed, return an error\n",
    "        logging.error(\"All models failed\")\n",
    "        return {\n",
    "            \"success\": False, \n",
    "            \"error\": \"All models failed\", \n",
    "            \"model\": \"none\"\n",
    "        }\n",
    "    \n",
    "    def get_client_stats(self):\n",
    "        \"\"\"Get statistics about client usage.\"\"\"\n",
    "        success_rate = 0 if self.call_count == 0 else ((self.call_count - self.error_count) / self.call_count) * 100\n",
    "        cache_hit_rate = 0 if self.call_count == 0 else (self.cache_hits / self.call_count) * 100\n",
    "        \n",
    "        stats = f\"\"\"Client Statistics:  \n",
    "        Total Calls: {self.call_count}  \n",
    "        Success Rate: {success_rate:.1f}%  \n",
    "        Error Count: {self.error_count}  \n",
    "        Cache Hit Rate: {cache_hit_rate:.1f}%  \n",
    "        \"\"\"\n",
    "        \n",
    "        # Add the usage report\n",
    "        stats += \"\\n\" + self.usage_tracker.get_usage_report()\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Create a resilient client\n",
    "resilient_client = ResilientLLMClient(\n",
    "    primary_model=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "    daily_budget=5.0,\n",
    "    requests_per_minute=10\n",
    ")\n",
    "\n",
    "# Test the client with a few requests\n",
    "test_prompts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain machine learning in simple terms\",\n",
    "    \"What is artificial intelligence?\"  # Repeat to test caching\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nSending prompt: '{prompt}'\")\n",
    "    response = resilient_client.call(prompt, max_tokens=100)\n",
    "    \n",
    "    if response.get(\"success\", False):\n",
    "        print(f\"Got response using model: {response.get('model', 'Unknown')}\")\n",
    "        print(f\"Response: {extract_text_response(response)[:100]}...\")\n",
    "        \n",
    "        # Show if it was a fallback\n",
    "        if response.get(\"used_fallback\", False):\n",
    "            print(\"(Used fallback model)\")\n",
    "        \n",
    "        # Show if it was from cache\n",
    "        if resilient_client.cache_hits > 0 and resilient_client.cache_hits == test_prompts.index(prompt) + 1:\n",
    "            print(\"(From cache)\")\n",
    "    else:\n",
    "        print(f\"Failed: {response.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Show client statistics\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "print(resilient_client.get_client_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Production Systems\n",
    "\n",
    "For production systems, there are additional considerations for error handling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Error Handling Best Practices\n",
    "\n",
    "1. **Health Checks and Circuit Breakers**\n",
    "   - Implement health checks to proactively detect API issues\n",
    "   - Use circuit breakers to prevent cascading failures\n",
    "\n",
    "2. **Graceful Degradation**\n",
    "   - Design your application to function with reduced capabilities when APIs are unavailable\n",
    "   - Provide clear error messages to users\n",
    "\n",
    "3. **Comprehensive Monitoring**\n",
    "   - Set up alerting for error rates and latency spikes\n",
    "   - Monitor quotas and resource usage\n",
    "\n",
    "4. **Structured Error Handling**\n",
    "   - Create specific exception types for different error categories\n",
    "   - Standardize error reporting across your application\n",
    "\n",
    "5. **Request Idempotency**\n",
    "   - Add request IDs to ensure retries don't result in duplicate operations\n",
    "   - Implement idempotency keys for critical operations\n",
    "\n",
    "6. **Dedicated Reliability Service**\n",
    "   - Consider building a dedicated service for handling API reliability\n",
    "   - Centralize error handling logic\n",
    "\n",
    "7. **Documentation and Runbooks**\n",
    "   - Document common failure modes and their solutions\n",
    "   - Create runbooks for handling different types of API outages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises\n",
    "\n",
    "Here are some exercises to practice error handling techniques:\n",
    "\n",
    "1. **Enhanced Rate Limiter**: Implement a more sophisticated rate limiter that adjusts based on observed error rates\n",
    "\n",
    "2. **Circuit Breaker**: Implement a circuit breaker pattern for the API client\n",
    "\n",
    "3. **Error Categorization**: Create a system that categorizes errors into actionable categories with specific handling strategies\n",
    "\n",
    "4. **Cost Optimization**: Enhance the usage tracker to automatically select the most cost-effective model for different types of prompts\n",
    "\n",
    "5. **Distributed Rate Limiting**: Implement a distributed rate limiter that works across multiple application instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "In this notebook, we've explored comprehensive error handling strategies for LLM APIs, including:\n",
    "\n",
    "- Understanding different types of API errors\n",
    "- Implementing exponential backoff and retry logic\n",
    "- Managing rate limits through client-side throttling\n",
    "- Setting up appropriate timeouts and timeout handling\n",
    "- Creating fallback chains across models and parameters\n",
    "- Logging API calls and monitoring usage\n",
    "- Building a resilient API client that incorporates all these techniques\n",
    "\n",
    "These strategies will help you build more reliable and robust applications that use LLM APIs, ensuring your systems remain operational even when facing API issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
